{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed42f10-f71b-4a43-bc09-ca1354f88d20",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    " I developed a RAG solution to answer questions about a repository of research papers. I parsed the paper PDF files, chunked and indexed the data, and then designed and executed an evaluation of the retriever results. In Naïve RAG, the query is compared to documents in the vector database for retrieval of the top N documents that match the query. The language model is then used to summarize the retrieved documents into an answer to the user query. Research papers are highly structured documents with technically deep content, in contrast to blogs, which contain more general and introductory content. This means that queries may be unlikely to match relevant chunks of the paper without additional processing, such as information extraction or summarization.\n",
    "\n",
    "One approach to address this problem is to use the language model to generate answerable questions from chunks of each paper. The generated questions can then be indexed as \"documents\" in a vector database, and the user query can be matched against the most similar questions. By maintaining a mapping between the indexed, generated question and the paper chunk, the retrieval process can then produce the most relevant chunks for use in summarizing an answer to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12a04-3acf-43f0-b984-d99790d4afff",
   "metadata": {},
   "source": [
    "## Setup the functions for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f04c17cf-85f5-4920-bea1-db9e480a4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc24019-6bef-4580-a823-aff13d959e49",
   "metadata": {},
   "source": [
    "## Parse data from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bf02b8-9880-450c-9505-4968d59dc61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 53 files:\n",
      "  File: 2020.coling-main.207.pdf\n",
      "  File: 2021.findings-emnlp.320.pdf\n",
      "  File: 2022.naacl-main.191.pdf\n",
      "  File: 2023.acl-long.557.pdf\n",
      "  File: 2023.emnlp-main.495.pdf\n",
      "  File: 2023.findings-emnlp.620.pdf\n",
      "  File: 2024.acl-long.642.pdf\n",
      "  File: 2024.eacl-demo.16.pdf\n",
      "  File: 2024.emnlp-industry.66.pdf\n",
      "  File: 2202.01110v2.pdf\n",
      "  File: 2212.14024v2.pdf\n",
      "  File: 23-0037.pdf\n",
      "  File: 2312.10997v5.pdf\n",
      "  File: 2402.19473v6.pdf\n",
      "  File: 29728-Article Text-33782-1-2-20240324-3.pdf\n",
      "  File: 3626772.3657834.pdf\n",
      "  File: 3626772.3657957.pdf\n",
      "  File: 3637870.pdf\n",
      "  File: 8917_Retrieval_meets_Long_Cont.pdf\n",
      "  File: 947_Augmented_Language_Models_.pdf\n",
      "  File: 967_generate_rather_than_retrieve_.pdf\n",
      "  File: NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf\n",
      "  File: NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf\n",
      "  File: NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf\n",
      "  File: tacl_a_00605.pdf\n"
     ]
    }
   ],
   "source": [
    "import os, bibtexparser, pypdf, logging\n",
    "\n",
    "# silence non-critical errors while parsing PDF files\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.CRITICAL)\n",
    "\n",
    "data_path = 'data/'\n",
    "data = {}\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "print('Reading %i files:' % len(files))\n",
    "for f in files:\n",
    "    path = os.path.join(data_path, f)\n",
    "\n",
    "    # each datum will have at least these attributes\n",
    "    d = {'filepath': None, 'title': None, 'text': None}\n",
    "\n",
    "    # parse bibtex file, if exists\n",
    "    if path.endswith('.bib'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        bib = bibtexparser.load(open(path, 'r'))\n",
    "        if 'title' in bib.entries[0]:\n",
    "            d['title'] = bib.entries[0]['title']\n",
    "            data[path[:-4]] = d\n",
    "\n",
    "    # parse pdf text, if exists\n",
    "    if path.endswith('.pdf'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        print('  File: %s' % f)\n",
    "        text = ''\n",
    "        reader = pypdf.PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        d['filepath'] = path\n",
    "        d['text'] = text\n",
    "        data[path[:-4]] = d\n",
    "\n",
    "data = [d for d in data.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063fc8f-0d55-499d-a112-2d9752b77821",
   "metadata": {},
   "source": [
    "## Chunk data and generate indices\n",
    "\n",
    "User queries will be matched to indexes that best approximate the text chunks used to summarize an answer. I chunked the text and then prompted the model to generate questions that are answerable by the text. The generated questions are then used as the \"documents\" stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ed1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few chunks of the first document:\n",
      "Chunk 1:\n",
      "Proceedings of the 28th International Conference on Computational Linguistics, pages 2284–2295 Barcelona, Spain (Online), December 8-13, 2020 2284 Retrieval-Augmented Controllable Review Generation Jihyeok Kim Yonsei University zizi1532@yonsei.ac.kr Seungtaek Choi Yonsei University hist0613@yonsei.ac.kr Reinald Kim Amplayo University of Edinburgh reinald.kim@ed.ac.uk Seung-won Hwang∗ Yonsei University seungwonh@yonsei.ac.kr Abstract In this paper, we study review generation given a set of attribute identiﬁers which are user ID, product ID and rating. This is a difﬁcult subtask of natural language generation since models are limited to the given identiﬁers, without any speciﬁc descriptive information regarding the inputs, when generating the text. The capacity of these models is thus conﬁned and dependent to how well the models can capture vector representations of attributes. We thus propose to additionally leverage references, which are selected from a large pool of texts labeled with one of the attributes, as textual information that enriches inductive biases of given attributes. With these references, we can now pose the problem as an instance of text-to-text generation, which makes the task easier since texts that are syntactically, semantically similar to the output text are provided as inputs. Using this framework, we address issues such as selecting references from a large candidate set without textual context and improving the model complexity for generation. Our experiments show that our models improve over previous approaches on both automatic and human evaluation metrics. 1 Introduction The ultimate goal of opinion mining and sentiment analysis (Pang and Lee, 2008) is to automatically digest opinions of users towards a certain product to accommodate decision making. While some of these opinions are explicitly articulated in product reviews that users write, most of them are unknown since users have not bought most of the products. Alternative solutions such as aspect-based sentiment analysis (Mukherjee and Liu, 2012; Pontiki et al., 2016) and recommendation systems (Resnick and Varian, 1997; Bobadilla et al., 2013) exist, however these only offer superﬁcial outputs that are not as expressive as textual reviews. Thus, the task of automatically generating reviews given their attributes such as user and product, or review generation (Dong et al., 2017), is necessary to achieve this goal. Most of the previous approaches (Dong et al., 2017; Sharma et al., 2018) have framed review gener- ation as A2T (Attribute-to-Text problem), where the given input is a non-linguistic data ( i.e., attribute identiﬁers for user, product, and rating) and the output is the review text. In this problem setup, the key challenge is to learn rich representations of the attributes, which are then used to produce the text us- ing either template-based surface realization methods (Kukich, 1983; McKeown, 1992) or neural-based decoders (Mei et al., 2016; Wiseman et al., 2017), as shown in the red box in Figure 1. However, it is difﬁcult to learn these representations merely from the given attribute identiﬁers since they do not convey any semantics regarding the attributes. Our key contribution isAT2T(Attribute-matched-Text-to-Text), of augmenting inductive biases of at- tributes with their matching reference reviews, as illustrated as the blue box in Figure\n",
      "\n",
      "Chunk 2:\n",
      "Figure 1. However, it is difﬁcult to learn these representations merely from the given attribute identiﬁers since they do not convey any semantics regarding the attributes. Our key contribution isAT2T(Attribute-matched-Text-to-Text), of augmenting inductive biases of at- tributes with their matching reference reviews, as illustrated as the blue box in Figure 1. For example, as shown in Figure 1, multiple references together contain inductive biases such as frequently reviewed as- pects of the product (e.g., talking about plot and character aspects) or habitual user phrases (e.g., “looking forward to the next book”). These references greatly help text generation since not only do they reinforce ∗Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/.2285 Attributes 𝐗=References ReferenceRetrieval𝐗∗⊆𝐗 TextGeneration Output Text ⋯ ⋯ (ref #2) I enjoyedprevious book.…but disappointed at this one, …too boring.(ref #3) it was a great read. Nice plotand charming characters. I am looking forward to the next! I reallyenjoyedthis book ! I loved the charactersand the plot. I am looking forward to the next book in the series! Selected References(ref #1) characters wereannoying and plot was boring. Output Text UserProduct #p3 Rating 5ID#u1 #u5 #u1 #u1 #u1 #p3 #p8 #p4 #p3 5 5 2 3 Figure 1: High-level diagram of frameworks of previous models (in red box) and our proposed model (in blue box), where we additionally make use of references to generate the output text. the representations learned from the attributes, they also allow the use of techniques used in sequence- to-sequence learning such as attention (Bahdanau et al., 2015) and copy (See et al., 2017) mechanisms. In related problem domains of generating abstractive summaries or dialogue utterances, such bias is in- troduced by a T2T (Text-to-Text) approach, of generating an extractive summary ﬁrst (Gehrmann et al., 2018) or retrieving informative prior turns (Cai et al., 2018), then generating ﬁnal outputs using these as references. Central to the framework is reference retrieval, since relevant references provide valuable context, but, in contrast, noisy references rather hinder generation. For reference retrieval in T2T, lexical features, e.g., TF-IDF, have been used, assigning relevance based on the degree of word overlap between two texts. In AT2T, however, lexical features are not directly applicable or effective. First, unlike T2T where input and output are both texts, our input is a list of identiﬁers, i.e., (user ID, product ID, rating). As a result, we cannot expedite the process of ﬁnding matching references, as in T2T solutions using lexical features for fast retrieval,e.g., using inverted index. Second, lexical similarity cannot fully capture rating, as sentiment lexicons appear in a small portion of text (Li et al., 2018). For example, ﬂipping a single lexicon (from ‘good’ to ‘bad’) from lexically identical sentences can completely invert the rating. One alternative solution, complementing lexical similarity, is to assign additional credits to references labeled with the input rating. However, references – labeled with a different rating, but having useful rating-related context – are forced to be penalized. For example, in\n",
      "\n",
      "Chunk 3:\n",
      "‘bad’) from lexically identical sentences can completely invert the rating. One alternative solution, complementing lexical similarity, is to assign additional credits to references labeled with the input rating. However, references – labeled with a different rating, but having useful rating-related context – are forced to be penalized. For example, in Figure 1, no additional credits are assigned to ref#2 labeled with the nearly opposite rating, although it contains useful context for the given rating, e.g., “enjoyed”. We later empirically show that neither lexical similarity nor rating accuracy of references guarantee rating semantics of generated reviews. To address these limitations, we propose two approaches: pseudo-supervised and reinforcement learning framework, denoted as SL and RL respectively. First, we expedite matching in SL using iden- tiﬁers. For efﬁcient retrieval without lexical features, we propose a parametric coarse-ﬁltering approach using attribute identiﬁers, having constant time complexity for each instance in a candidate pool. Sec- ond, to generate reviews which are compatible with input rating, we retrieve references which maximize the rating accuracy of generated reviews - rather than references labeled with the input rating. RL en- ables such retrieval: a retrieval model is trained to maximize rewards including rating accuracy as well as lexical similarity of generated reviews. To validate the effectiveness of AT2T, we perform experiments on a dataset consisting of product reviews from Amazon Books, aligned with their corresponding attributes: user, product and rating (Dong et al., 2017). Our experiments using automatic evaluations show that utilizing relevant references hugely helps generation in terms of content similarity, and rating accuracy. Moreover, our human evaluations show that our model generates more informative and grammatical texts compared to previous models.2286 2 Related Work Data-to-Text Generation Our task is generally related to a suite of tasks on data-to-text (D2T) gen- eration, where database tables (Wiseman et al., 2017), RDF graphs (Belz et al., 2011), and knowledge base relations (Perez-Beltrachini et al., 2016) are explored as inputs. A variety of neural-based models have been used on these tasks, including vanilla sequence-to-sequence models (Mei et al., 2016), ex- tended by explicitly incorporating context selection and planning (Puduppully et al., 2019a), by employ- ing graph-based neural networks (Marcheggiani and Perez-Beltrachini, 2018), and by modeling entities (Puduppully et al., 2019b). While review generation is essentially a subtask of D2T, it is relatively under- studied than other D2T tasks. Previous models include an encoder-decoder model with attention (Dong et al., 2017), improved by including an objective function for rating accuracy (Sharma et al., 2018; Li and Tuzhilin, 2019), by introducing a hierarchical decoder (Zang and Wan, 2017), by decomposing the de- coding stage as coarse-to-ﬁne manner (Li et al., 2019), and by using additional inputs such as user-given summary (Ni and McAuley, 2018) or product description (Li and Tuzhilin, 2019). In this paper, we make performance improvements by proposing a concept of leveraging references, and extensions proposed in the aforementioned literature are orthogonal and thus applicable to improve our models further. Augmenting context using references While data-hungry neural models for some\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits text into chunks of `chunk_size` words with `overlap` words overlapping between chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to chunk.\n",
    "        chunk_size (int): The number of words per chunk.\n",
    "        overlap (int): The number of words to overlap between chunks.\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    total_words = len(words)\n",
    "    while start < total_words:\n",
    "        end = start + chunk_size\n",
    "        chunk = words[start:end]\n",
    "        chunk_text = \" \".join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking to each document in your the list\n",
    "for d in data:\n",
    "    if d.get('text'):\n",
    "        d['chunks'] = chunk_text(d['text'], chunk_size=500, overlap=50)\n",
    "\n",
    "# Print the first few chunks from the first document for verification:\n",
    "if data and data[0].get('chunks'):\n",
    "    print(\"First few chunks of the first document:\")\n",
    "    for i, chunk in enumerate(data[0]['chunks'][:3]):\n",
    "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55891dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in all documents: 313190\n"
     ]
    }
   ],
   "source": [
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "\n",
    "total_words = sum(count_words(d['text']) for d in data if d.get('text'))\n",
    "print(\"Total words in all documents:\", total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8154b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First generated question for the first document:\n",
      " What is the main contribution of the paper \"Retrieval-Augmented Controllable Review Generation\" presented at the 28th International Conference on Computational Linguistics?\n"
     ]
    }
   ],
   "source": [
    "# Function to generate a question for a given text chunk\n",
    "def generate_question(chunk):\n",
    "    # Construct the prompt for the language model\n",
    "    prompt = f\"Generate a question that can be answered using the following text:\\n\\n{chunk}\\n\\nQuestion:\"\n",
    "    # Use LLM prompt_model function to get the question\n",
    "    question = prompt_model(prompt)\n",
    "    return question.strip()\n",
    "\n",
    "# Loop through each document in the data and generate questions for each chunk\n",
    "for d in data:\n",
    "    if d.get('chunks'):\n",
    "        # Initialize a new list to store questions corresponding to the chunks\n",
    "        d['chunk_questions'] = []\n",
    "        for chunk in d['chunks']:\n",
    "            question = generate_question(chunk)\n",
    "            d['chunk_questions'].append(question)\n",
    "\n",
    "# Verify by printing the first question from the first document \n",
    "if data and data[0].get('chunk_questions'):\n",
    "    print(\"First generated question for the first document:\\n\", data[0]['chunk_questions'][0])\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"chunk_questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91801-86ab-43ae-9ff1-1cd0891e82e5",
   "metadata": {},
   "source": [
    "## Build the vector database\n",
    "\n",
    "When building the vector database, I maintained a mapping between the generated questions and the chunks that can be used later to retrieve the chunks from the most similar indices to the user query provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c3d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anorm\\anaconda3\\envs\\crystal\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_25\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_26\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_27\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_28\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_29\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_30\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_31\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_32\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_33\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_34\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_35\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_36\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_37\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_38\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_39\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_40\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_41\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_32\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_33\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_34\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_35\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_32\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_33\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_34\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_35\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_36\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_37\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_38\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_39\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_40\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_41\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_42\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_43\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_44\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_45\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_0\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_1\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_2\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_3\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_4\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_5\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_6\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_7\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_8\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_9\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_10\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_11\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_12\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_13\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_14\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_15\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_16\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_17\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_13\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_14\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_15\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_16\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_17\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_18\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_19\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_20\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_21\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_22\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_13\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_14\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_15\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_16\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_17\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_18\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_19\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_20\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_21\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_22\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_23\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_24\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_25\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_26\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_27\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_28\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_29\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_30\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_31\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_32\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_33\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_34\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_35\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_36\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_37\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_38\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_39\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_40\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_41\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_42\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_43\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_44\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_45\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_46\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_47\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_48\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_49\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_50\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_51\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_52\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_53\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_54\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_55\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_56\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_57\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_58\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_59\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_60\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_61\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_62\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_63\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_64\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_65\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_66\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_67\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_0\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_1\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_2\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_3\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_4\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_5\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_6\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_7\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_8\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_9\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_10\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_11\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_12\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_13\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_14\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_15\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_16\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_17\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_18\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_0\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_1\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_2\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_3\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_4\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_5\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_6\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_7\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_8\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_9\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_10\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_11\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_12\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_13\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_14\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_15\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_16\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_17\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_18\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_19\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_20\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_21\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_22\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_23\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_24\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_25\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_26\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_27\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_28\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_29\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_30\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_31\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_32\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_33\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_34\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_35\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_36\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_37\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_38\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_39\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_40\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_41\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_42\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_43\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_44\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_45\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_46\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_47\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_0\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_1\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_2\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_3\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_4\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_5\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_6\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_7\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_8\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_9\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_10\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_11\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_12\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_13\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_14\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_15\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_16\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_17\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_18\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_19\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_20\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_21\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_22\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_23\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_24\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_25\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_26\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_27\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_28\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_29\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_30\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_31\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_32\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_33\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_19\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_20\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_21\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_22\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_23\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_24\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_25\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_26\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_27\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_28\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_29\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_30\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_31\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_32\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_33\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_34\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_35\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_36\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_37\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_38\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_39\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_19\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_0\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_1\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_2\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_3\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_4\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_5\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_6\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_7\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_8\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_9\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_10\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_11\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_12\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_13\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_14\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_15\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_16\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_17\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2020.coling-main.207.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2020.coling-main.207.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2021.findings-emnlp.320.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2022.naacl-main.191.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/2023.acl-long.557.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2023.acl-long.557.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2023.emnlp-main.495.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2023.findings-emnlp.620.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2024.acl-long.642.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2024.acl-long.642.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2024.eacl-demo.16.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2024.emnlp-industry.66.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2202.01110v2.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2202.01110v2.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2212.14024v2.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2212.14024v2.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_0\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_1\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_2\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_3\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_4\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_5\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_6\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_7\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_8\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_9\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_10\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_11\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_12\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_13\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_14\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_15\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_16\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_17\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_18\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_19\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_20\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_21\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_22\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_23\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_24\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_25\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_26\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_27\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_28\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_29\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_30\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_31\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_32\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_33\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_34\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_35\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_36\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_36\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_37\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_37\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_38\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_38\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_39\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_39\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_40\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_40\n",
      "Insert of existing embedding ID: data/23-0037.pdf_chunk_41\n",
      "Add of existing embedding ID: data/23-0037.pdf_chunk_41\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_32\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_33\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_34\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/2312.10997v5.pdf_chunk_35\n",
      "Add of existing embedding ID: data/2312.10997v5.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_0\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_1\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_2\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_3\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_4\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_5\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_6\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_7\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_8\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_9\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_10\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_11\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_12\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_13\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_14\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_15\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_16\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_17\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_18\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_19\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_20\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_21\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_22\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_23\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_24\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_25\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_26\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_27\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_28\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_29\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_30\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_31\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_32\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_33\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_34\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_35\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_36\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_36\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_37\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_37\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_38\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_38\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_39\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_39\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_40\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_40\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_41\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_41\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_42\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_42\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_43\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_43\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_44\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_44\n",
      "Insert of existing embedding ID: data/2402.19473v6.pdf_chunk_45\n",
      "Add of existing embedding ID: data/2402.19473v6.pdf_chunk_45\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_0\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_1\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_2\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_3\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_4\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_5\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_6\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_7\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_8\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_9\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_10\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_11\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_12\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_13\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_14\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_15\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_16\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_17\n",
      "Add of existing embedding ID: data/29728-Article Text-33782-1-2-20240324-3.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_13\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_14\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_15\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_16\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_17\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_18\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_19\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_20\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_21\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/3626772.3657834.pdf_chunk_22\n",
      "Add of existing embedding ID: data/3626772.3657834.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/3626772.3657957.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3626772.3657957.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_0\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_1\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_2\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_3\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_4\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_5\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_6\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_7\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_8\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_9\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_10\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_11\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_12\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_13\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_14\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_15\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_16\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_17\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_18\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_19\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_20\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_21\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_22\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_23\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_24\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_25\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_26\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_27\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_28\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_29\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_30\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_31\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_32\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_33\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_34\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_35\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_36\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_36\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_37\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_37\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_38\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_38\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_39\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_39\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_40\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_40\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_41\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_41\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_42\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_42\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_43\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_43\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_44\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_44\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_45\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_45\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_46\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_46\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_47\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_47\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_48\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_48\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_49\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_49\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_50\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_50\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_51\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_51\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_52\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_52\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_53\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_53\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_54\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_54\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_55\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_55\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_56\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_56\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_57\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_57\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_58\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_58\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_59\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_59\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_60\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_60\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_61\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_61\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_62\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_62\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_63\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_63\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_64\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_64\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_65\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_65\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_66\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_66\n",
      "Insert of existing embedding ID: data/3637870.pdf_chunk_67\n",
      "Add of existing embedding ID: data/3637870.pdf_chunk_67\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_0\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_1\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_2\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_3\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_4\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_5\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_6\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_7\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_8\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_9\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_10\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_11\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_12\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_13\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_14\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_15\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_16\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_17\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_18\n",
      "Add of existing embedding ID: data/8917_Retrieval_meets_Long_Cont.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_0\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_1\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_2\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_3\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_4\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_5\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_6\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_7\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_8\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_9\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_10\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_11\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_12\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_13\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_14\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_15\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_16\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_17\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_18\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_19\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_20\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_21\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_22\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_23\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_24\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_25\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_26\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_27\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_28\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_29\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_30\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_31\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_32\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_33\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_34\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_35\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_36\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_36\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_37\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_37\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_38\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_38\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_39\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_39\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_40\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_40\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_41\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_41\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_42\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_42\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_43\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_43\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_44\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_44\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_45\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_45\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_46\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_46\n",
      "Insert of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_47\n",
      "Add of existing embedding ID: data/947_Augmented_Language_Models_.pdf_chunk_47\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_0\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_1\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_2\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_3\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_4\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_5\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_6\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_7\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_8\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_9\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_10\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_11\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_12\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_13\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_14\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_15\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_16\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_17\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_18\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_19\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_20\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_21\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_22\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_23\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_24\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_25\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_26\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_27\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_28\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_29\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_30\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_31\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_32\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_33\n",
      "Add of existing embedding ID: data/967_generate_rather_than_retrieve_.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_19\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_20\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_20\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_21\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_21\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_22\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_22\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_23\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_23\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_24\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_24\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_25\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_25\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_26\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_26\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_27\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_27\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_28\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_28\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_29\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_29\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_30\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_30\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_31\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_31\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_32\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_32\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_33\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_33\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_34\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_34\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_35\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_35\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_36\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_36\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_37\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_37\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_38\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_38\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_39\n",
      "Add of existing embedding ID: data/NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf_chunk_39\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_0\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_1\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_2\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_3\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_4\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_5\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_6\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_7\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_8\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_9\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_10\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_11\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_12\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_13\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_14\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_15\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_16\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_17\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_18\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_18\n",
      "Insert of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_19\n",
      "Add of existing embedding ID: data/NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf_chunk_19\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_0\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_0\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_1\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_1\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_2\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_2\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_3\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_3\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_4\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_4\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_5\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_5\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_6\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_6\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_7\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_7\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_8\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_8\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_9\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_9\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_10\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_10\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_11\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_11\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_12\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_12\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_13\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_13\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_14\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_14\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_15\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_15\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_16\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_16\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_17\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_17\n",
      "Insert of existing embedding ID: data/tacl_a_00605.pdf_chunk_18\n",
      "Add of existing embedding ID: data/tacl_a_00605.pdf_chunk_18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database has been populated with questions and their corresponding chunks.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load from the JSON file\n",
    "with open(\"chunk_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"rag_questions\")\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store questions and their corresponding chunks\n",
    "for d in data:\n",
    "    if 'chunk_questions' in d and 'chunks' in d:\n",
    "        for i, question in enumerate(d['chunk_questions']):\n",
    "            chunk_text = d['chunks'][i]\n",
    "            embedding = embedding_model.encode(question).tolist()  # Convert to list for ChromaDB storage\n",
    "\n",
    "            # Use a unique ID for each entry\n",
    "            doc_id = f\"{d['filepath']}_chunk_{i}\"\n",
    "\n",
    "            # Store in ChromaDB\n",
    "            collection.add(\n",
    "                ids=[doc_id],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\"question\": question, \"chunk\": chunk_text}]\n",
    "            )\n",
    "\n",
    "print(\"Vector database has been populated with questions and their corresponding chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f708259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "(SMT) (Koehn et al., 2003) models (Simard and Isabelle, 2009; Koehn and Senellart, 2010) and in particular, we inten- sively highlight several popular methods to inte- grating translation memory to NMT models (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020; He et al., 2021). We also review the applications of retrieval-augmented generation in other genera- tion tasks such as abstractive summarization (Peng et al., 2019), code generation (Hashimoto et al., 2018), paraphrase (Kazemnejad et al., 2020; Su et al., 2021b), and knowledge-intensive generation (Lewis et al., 2020b). Finally, we also point out some promising directions on retrieval-augmented generation to push forward the future research. 2 Retrieval-Augmented Paradigm In this section, we ﬁrst give a general formulation of retrieval-augmented text generation. Then, we discuss three major components of the retrieval- augmented generation paradigm, including the re- arXiv:2202.01110v2 [cs.CL] 13 Feb 2022Input Sources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific Retrieval Retrieval MemoryGeneration Model Sec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & Templates Information RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4): Output Figure 1: The overview of this survey. trieval source, retrieval metric and integration meth- ods. 2.1 Formulation Most text generation tasks can be formulated as a mapping from input sequence x to output sequence y : y = f(x). For instance, x and y could be the dialogue history and the corresponding response for dialogue response generation, the text in the source language and the translation in the target language for machine translation, and so on. Recently, some researchers suggest to endow models the capability to access external memory via some information retrieval techniques, so that they can acquire more information in the generation process (Gu et al., 2018; Weston et al., 2018; Cai et al., 2019b). The retrieval-augmented generation can be further formulated as: y = f(x, z) (1) where z = {⟨xr, yr⟩}is a set of relevant instances retrieved from the original training set or external datasets. The main idea of this paradigm is that yr may beneﬁt the response generation, if xr (or yr) is similar (or relevant) to the input x. It is worth noting that xr = ∅when unsupervised retrieval sources are used. In general, the retrieval mem- ory can be retrieved from three kinds of sources: the training corpus, external datasets in the same format with the training corpus, and large-scale unsupervised corpus (§2.2). Metrics that evaluate the relevance between text are varied as well, in §2.3 we divided them into three categories: sparse- vector retrieval, dense-vector retrieval, and training- based retrieval. Finally, how to integrate the re- trieval memory to the generation model is also sig- niﬁcant, we also introduce some popular integra- tion approaches in §2.4. 2.2 Retrieval Sources Training Corpus Most previous studies search the external memory from itstraining corpus (Song et al., 2016; Gu et al., 2018; Weston et al., 2018). In the inference time, retrieved examples with high relevant scores could be regarded as extra refer- ences and reduce model’s uncertainty in generation.\n",
      "\n",
      "Chunk 2:\n",
      "A Survey on Retrieval-Augmented Text Generation Huayang Li♥,∗ Yixuan Su♠,∗ Deng Cai♦,∗ Yan Wang♣,∗ Lemao Liu♣,∗ ♥Nara Institute of Science and Technology ♠University of Cambridge ♦The Chinese University of Hong Kong ♣Tencent AI Lab li.huayang.lh6@is.naist.jp, ys484@cam.ac.uk thisisjcykcd@gmail.com, brandenwang@tencent.com lemaoliu@gmail.com Abstract Recently, retrieval-augmented text generation attracted increasing attention of the compu- tational linguistics community. Compared with conventional generation models, retrieval- augmented text generation has remarkable ad- vantages and particularly has achieved state-of- the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval- augmented text generation. It ﬁrstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable ap- proaches according to different tasks including dialogue response generation, machine trans- lation, and other generation tasks. Finally, it points out some promising directions on top of recent methods to facilitate future research. 1 Introduction Retrieval-augmented text generation, as a new text generation paradigm that fuses emerging deep learning technology and traditional retrieval tech- nology, has achieved state-of-the-art (SOTA) per- formance in many NLP tasks and attracted the at- tention of the computational linguistics community (Weston et al., 2018; Dinan et al., 2018; Cai et al., 2021). Compared with generation-based counter- part, this new paradigm has some remarkable ad- vantages: 1) The knowledge is not necessary to be implicitly stored in model parameters, but is explic- itly acquired in a plug-and-play manner, leading to great scalibility; 2) Instead of generating from scratch, the paradigm generating text from some re- trieved human-written reference, which potentially alleviates the difﬁculty of text generation. This paper aims to review many representative approaches for retrieval-augmented text generation tasks including dialogue response generation (We- ston et al., 2018), machine translation (Gu et al., 2018) and others (Hashimoto et al., 2018). We ∗All authors contributed equally. ﬁrstly present the generic paradigm of retrieval- augmented generation as well as three key com- ponents under this paradigm, which are retrieval sources, retrieval metrics and generation models. Then, we introduce notable methods about retrieval-augmented generation, which are orga- nized with respect to different tasks. Speciﬁcally, on the dialogue response generation task, exem- plar/template retrieval as an intermediate step has been shown beneﬁcial to informative response gen- eration (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a,b). In addition, there has been growing interest in knowledge-grounded generation explor- ing different forms of knowledge such as knowl- edge bases and external documents (Dinan et al., 2018; Zhou et al., 2018; Lian et al., 2019; Li et al., 2019; Qin et al., 2019; Wu et al., 2021; Zhang et al., 2021). On the machine translation task, we summa- rize the early work on how the retrieved sentences (called translation memory) are used to improve statistical machine translation (SMT) (Koehn et al., 2003) models (Simard and Isabelle, 2009; Koehn and Senellart, 2010) and in particular, we inten- sively highlight several popular methods to inte- grating translation memory to NMT models (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020; He et al., 2021). We also\n",
      "\n",
      "Chunk 3:\n",
      "Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky, “Bridging the preference gap between retrievers and llms,” arXiv preprint arXiv:2401.06954, 2024. [27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro- driguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval- augmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 , 2023. [28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or retrieval? comparing knowledge injection in llms,” arXiv preprint arXiv:2312.05934, 2023. [29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all you need,” in The Eleventh International Conference on Learning Representations, 2022. [30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and H. Zhang, “Dense x retrieval: What retrieval granularity should we use?” arXiv preprint arXiv:2312.06648 , 2023. [31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware multi-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023. [32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu, “Diversify question generation with retrieval-augmented style transfer,” arXiv preprint arXiv:2310.14503 , 2023. [33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re- trieval augmentation for non-knowledge-intensive tasks,”arXiv preprint arXiv:2305.17653, 2023. [34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning to filter context for retrieval-augmented generation,” arXiv preprint arXiv:2311.08377, 2023. [35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented data augmentation for low-resource domain tasks,” arXiv preprint arXiv:2402.13482, 2024. [36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not a good few-shot information extractor, but a good reranker for hard samples!” arXiv preprint arXiv:2303.08559 , 2023. [37] X. Du and H. Ji, “Retrieval-augmented generative question answering for event argument extraction,” arXiv preprint arXiv:2211.07067, 2022. [38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context examples for large language models,”arXiv preprint arXiv:2307.07164, 2023. [39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt, L. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems with generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023. [40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li, Y . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv preprint arXiv:2310.07815, 2023. [41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning for retrieval augmented generation,” arXiv preprint arXiv:2312.05708 , 2023. [42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot learning with retrieval augmented language models,” arXiv preprint arXiv:2208.03299, 2022. [43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan- zaro, “Raven: In-context learning with retrieval augmented encoder- decoder language models,” arXiv preprint arXiv:2308.07922 , 2023.18 [44] B. Wang, W. Ping, P. Xu, L. McAfee,\n",
      "\n",
      "Chunk 4:\n",
      "[196] S. Siriwardhana, R. Weerasekera, T. Kaluarachchi et al., “Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering,” TACL, vol. 11, pp. 1–17, 2023. [197] Y . Tang and Y . Yang, “Multihop-rag: Benchmarking retrieval- augmented generation for multi-hop queries,” arXiv:2401.15391, 2024. [198] K. Huang, C. Zhai, and H. Ji, “CONCRETE: improving cross-lingual fact-checking with cross-lingual retrieval,” in COLING, 2022. [199] L. Hagstr ¨om, D. Saynova, T. Norlund et al., “The effect of scaling, retrieval augmentation and form on the factual consistency of language models,” arXiv:2311.01307, 2023. [200] H. Zamani and M. Bendersky, “Stochastic rag: End-to-end retrieval- augmented generation through expected utility maximization,” arXiv:2405.02816, 2024. [201] Y . Liu, Y . Wanet al., “KG-BART: knowledge graph-augmented BART for generative commonsense reasoning,” in AAAI, 2021. [202] A. Wan, E. Wallace, and D. Klein, “What evidence do language models find convincing?” arXiv:2402.11782, 2024. [203] H. Zhang, Z. Liu et al., “Grounded conversation generation as guided traverses in commonsense knowledge graphs,” in ACL, 2020. [204] D. Cai, Y . Wang, W. Bi et al., “Skeleton-to-response: Dialogue gener- ation guided by retrieval memory,” in NAACL-HLT, 2019. [205] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue generation,” in ACL, 2022. [206] K. Shuster, J. Xu et al., “Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage,” arXiv:2208.03188, 2022. [207] S. Kim, J. Y . Jang, M. Jung, and S. Shin, “A model of cross-lingual knowledge-grounded response generation for open-domain dialogue systems,” in EMNLP Findings, 2021. [208] E. Nie, S. Liang, H. Schmid, and H. Sch ¨utze, “Cross-lingual retrieval augmented prompt for low-resource languages,” in ACL, 2023. [209] X. Li, E. Nie, and S. Liang, “From classification to generation: Insights into crosslingual retrieval augmented icl,” in NeurIPS, 2023. [210] W. Li, J. Li, W. Ma, and Y . Liu, “Citation-enhanced generation for llm-based chatbot,” arXiv:2402.16063, 2024. [211] D. Cai, Y . Wang, H. Li et al., “Neural machine translation with monolingual translation memory,” in ACL/IJCNLP, 2021. [212] U. Khandelwal, A. Fan, D. Jurafsky et al., “Nearest neighbor machine translation,” in ICLR, 2021. [213] X. Du and H. Ji, “Retrieval-augmented generative question answering for event argument extraction,” in EMNLP, 2022. [214] Y . Gao, Q. Yin, Z. Li et al., “Retrieval-augmented multilingual keyphrase generation with retriever-generator iterative training,” in NAACL Findings, 2022. [215] J. Zhang, E. J. Yu, Q. Chen et al., “Retrieval-based full-length wikipedia generation for emergent events,” arXiv:2402.18264, 2024. [216] R. Fan, Y . Fan, J. Chen et al., “RIGHT: retrieval-augmented generation for mainstream hashtag recommendation,” arxiv:2312.10466, 2023. [217] Z. Wang, S. X. Teo et al., “M-rag: Reinforcing large language model performance through retrieval-augmented generation with multiple par- titions,” arXiv:2405.16420, 2024. [218] Y . Wang, H. Le, A. D. Gotmare et al., “Codet5mix: A pretrained mixture of encoder-decoder transformers for code understanding and generation,” 2022. [219] A. Madaan, S. Zhou, U. Alon et al., “Language models of code are few-shot commonsense learners,” in EMNLP, 2022. [220] Y . Wang, H. Le, A. Gotmareet al., “Codet5+:\n",
      "\n",
      "Chunk 5:\n",
      "for limited scenarios. Most of the existing works focus on text-related RAG tasks that are facilitated by LLMs, without in-depth investigation in other modalities. The survey by Li et al. [57] offers a basic overview of RAG and discusses specific applications within the scope of text generation tasks. In a similar vein, the tutorial crafted by Asai et al. [58] centers on retrieval- based language models, detailing their structures and training strategies. Meanwhile, a recent survey by Gao et al. [59] explores RAG in the context of LLMs, with a particular emphasis on enhancement approaches for query-based RAG. Recognizing that RAG has extended beyond the text domain, our work broadens its reach to the entire AIGC landscape, facilitating a more comprehensive coverage of RAG research. In addition, another survey proposed by Zhao et al. [60] in- troduces RAG applications across multiple modalities, but ig- noring the discussion on RAG foundations. Another work [61] covers only part works of other modalities. While existing research has explored various aspects of RAG, there remains a need for a comprehensive overview that covers RAG foun- dations, enhancements, and its applicability across different domains. In this paper, we aim to address the gap by presenting a systematic survey of RAG. D. Roadmap The rest of the paper is organized as follows. Section II elab- orates on the preliminary of RAG, introducing retrievers and generators. Section III presents RAG foundations and further enhancements on RAG. Section IV reviews existing research on RAG across various applications. Section V investigates the benchmark frameworks for RAG. Section VI discusses current limitations of RAG and potential future directions. Finally, Section VII concludes this paper. II. PRELIMINARY In this section, we provide an overview of the general RAG architecture and explore the generators and the retrievers in today’s RAG-based AIGC. A. Overview As shown in Fig. 1, the entire RAG system consists of two core modules: the retriever and the generator, where the retriever searches for relevant information from the data store and the generator produces the required contents. The RAG process unfolds as follows: (i) the retriever initially receives the input query and searches for relevant information; (ii) then, the original query and the retrieval results are fed into the generator through a specific augmentation methodology; (iii) finally, the generator produces the desired outcomes. B. Generator The remarkable performance of generative AI across di- verse tasks has ushered in the era of AIGC. The generation module plays a crucial role within the RAG system. Different generative models are applied for different scenarios, such as transformer models for text-to-text tasks, VisualGPT [62] for image-to-text tasks, Stable Diffusion [10] for text-to- image tasks, Codex [2] for text-to-code tasks, etc. Here we introduce 4 typical generators that are frequently used in RAG: transformer model, LSTM, diffusion model, and GAN. 1) Transformer Model: Transformer models are one of the best performing models in the field of Natural Language Pro- cessing (NLP), consisting of self-attention mechanisms, feed- forward networks, layer normalization modules, and residual networks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to query vector database for similar questions and obtain chunks\n",
    "def query_vector_database(query, top_k=5):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_chunks = []\n",
    "    for result in results[\"metadatas\"][0]:\n",
    "        retrieved_chunks.append(result[\"chunk\"])\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "# Example Query\n",
    "user_query = \"What is retrieval-augmented generation?\"\n",
    "retrieved_chunks = query_vector_database(user_query)\n",
    "\n",
    "print(\"Retrieved Chunks:\")\n",
    "for i, chunk in enumerate(retrieved_chunks):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef44efd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "of data-to-text generation. To bridge the gap be- tween the structured data and natural language text, Su et al. (2021a) propose a novel retrieval- augmented framework. Speciﬁcally, given the source data, a set of candidate texts are ﬁrst re- trieved from a large unlabelled corpus. Then, a neural selector is applied to measure the similari- ties between the source data and candidate texts, and extract a set of more ﬁne-grained prototypes from the candidates. Lastly, a generation model takes the prototypes as input to produce the text that describes the given structured data. While retrieval-augmented generation has been widely explored in the NLP community, we sug- gest that future research could extend this approach to tasks that involve data from multiple modali- ties. For instance, with recent advancements in image-text retrieval (Jia et al., 2021; Radford et al., 2021), the structural gap between images and texts is largely bridged. Some early studies (Zhang et al., 2020) have shown that information retrieved from images could improve the performance of neural machine translation model. Naturally, such meth- ods could be extended to other multi-modal tasks, such as image captioning (Karpathy and Li, 2015). A similar idea could also be applied to tasks be- yond images, such as speech-to-text transcription (Gales and Young, 2007). 6 Future Directions Despite the current success of retrieval augmented text generation, there is still a long way to go as discussed in previous sections. We highlight some directions to facilitate the future research as fol- lows: Retrieval Sensitivity The performance of re- trieval augmented text generation is very sensitive to the retrieval quality, i.e., the similarity between the query and the retrieved examples. Currently, re- trieval augmented text generation models perform well when the retrieved examples are very simi- lar to the query. However, they are even worse than the generation models without retrieval when the retrieval examples are less similar. Therefore, it would be important to exploit new methods to address such an issue on similarity. Retrieval Efﬁciency Generally, if one enlarges the retrieval memory to some extent, it would be possible to retrieve an example which is very simi- lar to the query.Unfortunately, the downside is that the overall inference for the retrieval augmented generation models is less efﬁcient due the consid- erable retrieval overhead. In this sense, it is urgent to consider some methods to trade off the retrieval memory size and retrieval efﬁciency, for example, data compression for the retrieval memory. Local vs. Global Optimization Theoretically, it seems promising to jointly learn retrieval metrics and generation models. However, in practice, there is an essential gap about the retrieval metric be- tween the training and inference phrases. In the training phase, the loss is locally back-propagated to only a few retrieved examples while in the infer- ence phase the metric is globally conducted among all examples in the memory. It would be interesting to narrow such a gap when learning a better metric for generation tasks. Multi-Modalities With recent advancement in image-text retrieval, directly associating\n",
      "\n",
      "Chunk 2:\n",
      "SYNCHROMESH‡§ RESDSQL§ REFSQL‡§ CodeICL§ MURRE∥¶ StackSpotAI‡§ E&V Code4UIE§ De-fine‡∥ ImputBlaster¶ RAG for Knowledge Knowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others CBR-KBQA ‡§∥ TIARA †‡§ Keqing †‡§ RNG-KBQA ‡∥ ReTraCk § SKP †‡§ UniK-QA †‡ KG-FiD ‡ GRAPE ‡ SKURG †‡ KnowledGPT ‡ EFSUM § EfficientQA ‡ CORE § Convinse †‡ RINK ‡§ T-RAG ‡§ StructGPT ‡ GRetriever § SURGE § K-LaMP RHO ∥ RAG for 3D Text-to-3D ReMoDiffuse †‡ AMD † RAG for Image Image Generation Image Captioning Others RetrieveGAN‡ IC-GAN§ Re-imagen§ RDM Retrieve&Fuse§ KNN-Diffusion MA∥ REVEAL‡ SMALLCAP† CRSR† RA-Transformer PICa∥ Maira‡ KIF‡ RA-VQA‡ RAG for Video Video Captioning Video QA&Dialogue Others KaVD‡§ R-ConvED‡§ CARE§ EgoInstructor†‡§ MA-DRNN†‡ R2A‡ Tvqa+§ VGNMN‡ VidIL†‡ RAG-Driver‡ Animate-A-Story†§ RAG for Science Drug Discovery Biomedical Informatics Enhancement Math Applications RetMol†§ PromptDiff†‡ PoET‡ Chat-Orthopedist†§ BIOREADER† MedWriter‡ QARAG†‡ LeanDojo‡ RAG-for-math-QA†‡ RAG for Audio Audio Generation Audio Captioning Re-AudioLDM§ Make-An-Audio†§ RECAP‡§ Query-based Latent-based Logit-based Speculative Query+Latent Latent+Logit † Input ‡ Retriever § Generator ∥ Output ¶ Pipeline generated code to better utilize dispersed information and improve outcomes. ITER-RETGEN [187] iteratively enhances content quality by using the generator’s output to pinpoint knowledge gaps, retrieving necessary information, and in- forming future generation cycles. SelfMemory [188] utilizes a retrieval-augmented generator iteratively to form an expansive memory pool, from which a memory selector picks an output to inform the next generation cycle. RAT [189] initially generates content by an LLM with a zero-shot CoT prompt, then revises each thought step by retrieving knowledge from external knowledge base. IV. APPLICATIONS In this section, we focus on RAG applications spanning various modalities. To echo with the taxonomy of RAG foundations and enhancements, we also demonstrate their utilization across different tasks in Table I. A. RAG for Text To begin with, text generation is among the most important and widely deployed applications for RAG. Here we introduce popular works for seven tasks, respectively. 1) Question Answering : Question answering involves the process of providing responses to posed questions by drawing from a vast and comprehensive collection of textual sources. FiD [35] and REALM [33] identify the top-k most pertinent article snippets based on the query and forward each snippet along with the question to LLMs to generate k responses. These responses are then synthesized into a final answer. Toutanova et al. [190] substituted the text corpus in REALM with subgraphs from a knowledge graph, yielding impressive results. As shown in Fig. 5, RETRO [36] employs attention mechanisms to integrate the question with relevant retrieved documents within the model to produce the final answer. SKR [183] observes that using RAG does not invariably benefit question answering and thus explored guiding the model to evaluate its grasp of pertinent knowledge, subse- quently adapting its use of external resources for retrieval enhancement. TOG [191] introduces an innovative knowledge graph-augmented LLM framework, which excels by fostering interactions between LLMs and the knowledge graph and by expanding the inference path space with beam search. NPM [119] pioneers the use of nonparametric data distribu- tions in lieu of the\n",
      "\n",
      "Chunk 3:\n",
      "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992 December 6-10, 2023 ©2023 Association for Computational Linguistics Active Retrieval Augmented Generation Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2 Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1 1Language Technologies Institute, Carnegie Mellon University 2Sea AI Lab 3FAIR, Meta {zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu Abstract Despite the remarkable ability of large lan- guage models (LMs) to comprehend and gen- erate language, they have a tendency to hal- lucinate and create factually inaccurate out- put. Augmenting LMs by retrieving informa- tion from external knowledge resources is one promising solution. Most existing retrieval aug- mented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering in- formation throughout generation is essential. In this work, we provide a generalized view of ac- tive retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant doc- uments to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long- form knowledge-intensive generation tasks/- datasets. FLARE achieves superior or compet- itive performance on all tasks, demonstrating the effectiveness of our method.1 1 Introduction Generative language models (LMs) (Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023; Chowd- hery et al., 2022; Zhang et al., 2022; Touvron et al., 2023; Zhao et al., 2023) have become a founda- tional component in natural language processing (NLP) systems with their remarkable abilities. Al- though LMs have memorized some world knowl- edge during training (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020), they still tend to ∗Lead contributors. 1Code and datasets are available athttps://github.com/ jzbjyb/FLARE. hallucinate and create imaginary content (Maynez et al., 2020; Zhou et al., 2021). Augmenting LMs with retrieval components that look up relevant in- formation from external knowledge resources is a promising direction to address hallucination (Khan- delwal et al., 2020; Izacard et al., 2022). Retrieval augmented LMs commonly use a retrieve-and-generate setup where they retrieve doc- uments based on the user’s input, and then generate a complete answer conditioning on the retrieved documents (Chen et al., 2017; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021; Sachan et al., 2021; Lee et al., 2021; Jiang et al., 2022; Izacard et al., 2022; Nakano et al., 2021; Qian et al., 2023; Lazaridou et al., 2022; Shi et al., 2023). These single-time retrieval augmented LMs outper- form purely parametric LMs, particularly for short- form knowledge-intensive generation tasks such as factoid question answering (QA) (Kwiatkowski et al., 2019; Joshi et al., 2017), where the informa- tion needs are clear in the user’s input, and it is sufficient to\n",
      "\n",
      "Chunk 4:\n",
      "T. Cai, E. Rutherford, D. d. L. Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. v. d. Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training compute-optimal large language models, 2022. URL https: //arxiv.org/abs/2203.15556. S. Hofstätter, J. Chen, K. Raman, and H. Zamani. Multi-task retrieval-augmented text generation with relevance sampling, 2022. URLhttps://arxiv.org/abs/2207.03030. P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. InCIKM, 2013. G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain question answering. InEACL, 2021a. URLhttps://aclanthology.org/2021. eacl-main.74. G. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering. In ICLR, 2021b. URLhttps://openreview.net/forum?id=NTEz-6wysdb. G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S. Riedel, and E. Grave. A memory eﬃcient baseline for open domain question answering, 2020. URLhttps://arxiv.org/abs/2012. 15156. G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised dense information retrieval with contrastive learning.TMLR, 2022. URL https://openreview.net/forum?id=jKN1pXi7b0. H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search.IEEE TPAMI, 2010. Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. How can we know what language models know? TACL, 2020. URLhttps://aclanthology.org/2020.tacl-1.28. K. S. Jones. A statistical interpretation of term speciﬁcity and its application in retrieval. Journal of documentation, 1972. M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. InACL, 2017. URLhttps://aclanthology. org/P17-1147. J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models, 2020. URLhttps: //arxiv.org/abs/2001.08361. 38Atlas: Few-shot Learning with Retrieval Augmented Language Models V. Karpukhin, B. Oğuz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense passage retrieval for open-domain question answering.arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906. U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. In ICLR, 2020. URL https://openreview.net/forum?id=HklBjCEKvH. D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of EMNLP, 2020. URLhttps://aclanthology.org/2020.findings-emnlp.171. A. Krishna, S. Riedel, and A. Vlachos. ProoFVer: Natural logic theorem proving for fact veriﬁcation. TACL, 2022. URLhttps://aclanthology.org/2022.tacl-1.59. T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: A benchmark for question answering research.TACL, 2019. URLhttps://aclanthology.org/Q19-1026. G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehen- sion dataset from examinations. InEMNLP, 2017. URL https://aclanthology.org/ D17-1082. A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering, 2022. URL https://arxiv.org/abs/2203.05115. T. Le Scao and A. Rush. How many\n",
      "\n",
      "Chunk 5:\n",
      "1 Retrieval-Augmented Generation for AI-Generated Content: A Survey Penghao Zhao∗, Hailin Zhang ∗, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu†, Ling Yang, Wentao Zhang †, Jie Jiang, Bin Cui † Abstract—Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Con- tent (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and infer- ence costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In partic- ular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG techniques into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstrac- tions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey. Index Terms —Retrieval-augmented generation, AI-generated content, generative models, information retrieval. I. INTRODUCTION A. Background Recent years have witnessed the surge in interests surround- ing Artificial Intelligence Generated Content (AIGC). Various content generation tools have been meticulously crafted to produce diverse outputs across various modalities, such as Large Language Models (LLMs) including the GPT series [1]– [3] and the LLAMA series [4]–[6] for texts and codes, DALL- E [7]–[9] and Stable Diffusion [10] for images, and Sora [11] for videos. The word “AIGC” emphasizes that the contents are produced by advanced generative models other than human beings or rule-based approaches. These generative models have achieved remarkable performance due to the utilization of ∗ Both authors contributed equally to this research. † Corresponding authors. • Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with Peking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn, yuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn, ccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn, bin.cui@pku.edu.cn). • Jie Jiang is with Tencent Inc. (email: zeus@tencent.com) novel model algorithms, explosive scale of foundation models, and massive high-quality datasets. Specifically, sequence-to- sequence tasks have transitioned from utilizing Long Short- Term Memory (LSTM) networks [12] to Transformer-based models [13], and image-generation tasks have shifted from Generative Adversarial Networks (GANs) [14] to Latent Dif- fusion Models (LDMs) [10] as well. Notably, the architecture of foundation models, initially constituted by millions of parameters [15], [16], has now grown to billions or even trillions of parameters [1], [4], [17]. These advancements are further bolstered by the availability of rich, high-quality\n",
      "\n",
      "\n",
      "Generated Answer:\n",
      " IDK\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query_text):\n",
    "    # Retrieve relevant chunks from ChromaDB\n",
    "    retrieved_chunks = query_vector_database(query_text)\n",
    "\n",
    "    # Print the retrieved chunks\n",
    "    print(\"\\nRetrieved Chunks:\")\n",
    "    for i, chunk in enumerate(retrieved_chunks):\n",
    "        print(f\"Chunk {i+1}:\\n{chunk}\\n\")\n",
    "\n",
    "    # Construct a prompt for the LLM\n",
    "    context = \"\\n\\n\".join(retrieved_chunks)\n",
    "    prompt = f\"\"\"Use the following context to answer the query concisely:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    If the answer is not found in the context, respond only with \"IDK\".\n",
    "    \n",
    "    Query: {query_text}\n",
    "    \n",
    "    Answer:\"\"\"\n",
    "    \n",
    "    # Use the LLM to generate an answer\n",
    "    answer = prompt_model(prompt)\n",
    "    \n",
    "    return answer.strip()\n",
    "\n",
    "# Example user query\n",
    "user_query = \"What are the challenges of attribute-to-text generation?\"\n",
    "answer = generate_answer(user_query)\n",
    "\n",
    "print(\"\\nGenerated Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30541-75ca-4647-86b3-4af98d395c31",
   "metadata": {},
   "source": [
    "## Conduct experiments to evaluate user queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e9537d-6fae-4b48-a94e-6d7f57d728c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-queries.json has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the test queries\n",
    "test_queries = [\n",
    "    {\n",
    "        \"query\": \"For ALMs, what is a tool?\",\n",
    "        \"answer\": \"A tool is an external module that is typically called using a rule or special token and whose output is included in the ALM’s context.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a major feature of retrieve-then-read?\",\n",
    "        \"answer\": \"The ability to swap in new documents when new information is learned, such as temporally more recent documents, or adding in documents from a new domain to quickly adapt to a new downstream task.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The proposed retriever method, IRCoT, can be instantiated from what three ingredients?\",\n",
    "        \"answer\": \"(i) a base retriever that can take a query and return a given number of paragraphs from a corpus or knowledge source; (ii) a language model with zero/few-shot Chain-of-Thought (CoT) generation capabilities; and (iii) a small number of annotated questions with reasoning steps explaining how to arrive at the answer in natural language (chain of thoughts) and a set of paragraphs from the knowledge source that collectively support the reasoning chain and the answer.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What problem does augmenting LMs try to address and when is this method limiting?\",\n",
    "        \"answer\": \"Augmenting LMs tries to address the problem of LMs hallucinating and producing factually incorrect output. This method is limited in more general scenarios that involve the generation of long texts.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are symmetric tasks and what are two applicable scenarios?\",\n",
    "        \"answer\": \"Symmetric tasks have to do with queries and documents that have similar semantic meanings but different surface forms. Two applicable scenarios are monolingual semantic textual similarity and bitext retrieval.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the advantage of using LC instead of RAG, and what is the advantage of using RAG instead of LC?\",\n",
    "        \"answer\": \"The advantage of using LC instead of RAG is that it consistently outperforms RAG in terms of average performance. The advantage of using RAG instead of LC is that it has a much lower cost.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"For ALMs, what is a hammer?\",\n",
    "        \"answer\": \"IDK\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is a major feature of fetch-then-receive?\",\n",
    "        \"answer\": \"IDK\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Does Self-Route help the computer to have a longer life?\",\n",
    "        \"answer\": \"IDK\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"test-queries.json\", \"w\") as f:\n",
    "    json.dump(test_queries, f, indent=4)\n",
    "\n",
    "print(\"test-queries.json has been created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "596d7d2a-d863-4678-a285-21065993b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Gen- eration” and “Augmentation”, respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on post- retrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG’s downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII. II. O VERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector\n",
      "\n",
      "Chunk 2:\n",
      "date and long-tail knowledge [353]. We expect RAG to benefit from long context generation, rather than being replaced by it. VII. C ONCLUSION In this paper, we conducted a thorough and comprehensive survey on RAG within the context of AIGC, with a particular focus on augmentation foundations, enhancements, and ap- plications. We first systematically organized and summarized the foundation paradigms in RAG, providing insights into the interaction between retrievers and generators. Then, we reviewed the enhancements that further improve the effective- ness of RAG, including the enhancements on each component or the entire pipeline. To facilitate researchers across diverse domains, we showcased practical applications of RAG in a range of modalities and tasks. Finally, we also presented existing benchmarks for RAG, discussed current limitations of RAG, and shed light on promising future directions. REFERENCES [1] T. B. Brown, B. Mann et al., “Language models are few-shot learners,” in NeurIPS, 2020. [2] M. Chen, J. Tworek et al., “Evaluating large language models trained on code,” arXiv:2107.03374, 2021. [3] OpenAI, “GPT-4 technical report,” arXiv:2303.08774, 2023. [4] H. Touvron, T. Lavril et al., “Llama: Open and efficient foundation language models,” arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv:2307.09288, 2023. [6] B. Rozi `ere, J. Gehring et al., “Code llama: Open foundation models for code,” arXiv:2308.12950, 2023. [7] A. Ramesh, M. Pavlov, G. Goh et al., “Zero-shot text-to-image gener- ation,” in ICML, 2021. [8] A. Ramesh, P. Dhariwal, A. Nichol et al., “Hierarchical text-conditional image generation with CLIP latents,” arXiv:2204.06125, 2022. [9] J. Betker, G. Goh, L. Jing et al., “Improving image generation with better captions,” Computer Science, vol. 2, no. 3, p. 8, 2023. [10] R. Rombach, A. Blattmann, D. Lorenz et al., “High-resolution image synthesis with latent diffusion models,” in IEEE/CVF, 2022. [11] OpenAI, “Video generation models as world simulators,” https://openai. com/research/video-generation-models-as-world-simulators, 2024. [12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997. [13] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,” in NeurIPS, 2017. [14] I. Goodfellow, J. Pouget-Abadie, M. Mirza et al., “Generative adver- sarial networks,” CACM, vol. 63, no. 11, pp. 139–144, 2020. [15] J. Devlin, M. Chang et al., “BERT: pre-training of deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019. [16] C. Raffel, N. Shazeer, A. Roberts et al., “Exploring the limits of transfer learning with a unified text-to-text transformer,” JMLR, vol. 21, pp. 140:1–140:67, 2020. [17] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” JMLR, vol. 23, no. 120, pp. 1–39, 2022. [18] J. Kaplan, S. McCandlish, T. Henighan et al., “Scaling laws for neural language models,” 2020. [19] S. E. Robertson and H. Zaragoza, “The probabilistic relevance frame- work: BM25 and beyond,” FTIR, vol. 3, no. 4, pp. 333–389, 2009. [20] V . Karpukhin, B. Oguz, S. Min et al., “Dense passage retrieval for open-domain question answering,” in EMNLP, 2020. [21]\n",
      "\n",
      "Chunk 3:\n",
      "is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt. Generation. The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information con- tained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively. However, Naive RAG encounters notable drawbacks: Retrieval Challenges . The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information. Generation Difficulties. In generating responses, the model may face the issue of hallucination, where it produces con- tent not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses. Augmentation Hurdles . Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Deter- mining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information. Moreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information. B. Advanced RAG Advanced RAG introduces specific improvements to over- come the limitations of Naive RAG. Focusing on enhancing re- trieval quality, it employs pre-retrieval and post-retrieval strate- gies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process [8].4 Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not\n",
      "\n",
      "Chunk 4:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "Chunk 5:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "Chunk 2:\n",
      "with those presented in the local neighborhood of the query entity in knowledge graph. 5) RAG Pipeline Enhancement : RAG pipeline enhance- ment refers to optimizing the overall process of RAG in order to achieve better performance results. Adaptive Retrieval: Some studies on RAG suggest that re- trieval doesn’t always enhance the final results. Over-retrieval can lead to resource wastage and potential confusion when the model’s inherent parameterized knowledge suffices for answering relevant questions. Consequently, this chapter will delve into two methods for determining retrieval necessity: rule-based and model-based approaches. Rule-based: FLARE [178] actively decides whether and when to search through the probability in the generation pro- cess. Efficient-KNNLM [38] combines the generation proba- bility of KNN-LM [37] and NPM [119] with a hyperparameter λ to determine the proportion of generation and retrieval. Mallen et al. [179] used statistical analysis on questions to enable direct answers for high-frequency ones and applied RAG for low-frequency ones. Jiang et al. [180] evaluated model confidence based on Model Uncertainty, Input Uncer- tainty, and Input Statistics to guide retrieval decisions. Kandpal et al. [181] studied the correlation between the number of relevant documents and the model’s knowledge mastery to assess the need for retrieval. Model-based: Self-RAG [85] uses a trained generator to determine whether to perform a retrieval based on the retrieve token under different user querys. Ren et al. [182] used “Judg- ment Prompting” to determine whether LLMs can answer relevant questions and whether their answers are correct or not, thereby assisting in determining the necessity of a retrieval. SKR [183] uses the ability of LLMs themselves to judge in advance whether they can answer the question, and if they can answer, no retrieval is performed. Rowen [184] translates a question into multiple languages and checks for answer consistency across these languages, using the results to deter- mine the need for information retrieval. AdaptiveRAG [185] dynamically decides whether to retrieve based on the query complexity by a classifier, which is a smaller LM. Iterative RAG: Iterative RAG progressively refines results by repeatedly cycling through retrieval and generation phases, rather than a single round. RepoCoder [186] uses an iterative retrieval-generation ap- proach for code completion, refining queries with previously9 TABLE I: Taxonomy of RAG applications across various modalities. RAG for Text Question Answering Human-Machine ConversationNeural Machine Translation Summarization Others REALM‡§ TKEGEN§ RIAG‡ Fid‡§ RETRO§ NPM‡§ SKR§¶ Self-RAG§¶ TOG‡ ConceptFlow‡§ Skeleton-to-Response‡§ CREA-ICL†‡ Internet-Augmented-DG‡§ BlenderBot3‡§ CEG‡∥ NMT-with-Monolingual-TM†‡§ KNN-MT‡§ COG‡ TRIME‡§ RAMKG‡§ Unlimiformer§ RPRR‡ RIGHT‡§ CONCRETE‡§ Atlas‡§ KG-BART‡§ R-GQA‡§ RAG for Code Code GenerationCode Summary Code CompletionAutomatic Program RepairText-to-SQL andCode-based Semantic ParsingOthers SKCODER§ RRGCode‡ ARKS†¶ KNN-TRANX∥ RECODEToolcoder§∥ RACE† BASHEXPLAINER‡ READSUM∥ Rencos‡ CoRec‡ Tram§ EDITSUM‡ ReACC†‡ RepoCoder†§¶ De-Hallucinator¶ REPOFUSE§ RepoFusion§ EDITAS§ RING∥ CEDAR§ RAP-Gen‡§ InferFix§ SARGAM§ RTLFixer‡§ XRICL‡§ SYNCHROMESH‡§ RESDSQL§ REFSQL‡§ CodeICL§ MURRE∥¶ StackSpotAI‡§ E&V Code4UIE§ De-fine‡∥ ImputBlaster¶ RAG for Knowledge Knowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others CBR-KBQA ‡§∥ TIARA †‡§ Keqing †‡§ RNG-KBQA ‡∥ ReTraCk § SKP †‡§ UniK-QA †‡ KG-FiD ‡ GRAPE ‡ SKURG †‡ KnowledGPT ‡ EFSUM § EfficientQA ‡ CORE\n",
      "\n",
      "Chunk 3:\n",
      "SUMMARY OF RAG METHODS Method Retrieval Source Retrieval Data Type Retrieval Granularity Augmentation Stage Retrieval process CoG [29] Wikipedia Text Phrase Pre-training Iterative DenseX [30] FactoidWiki Text Proposition Inference Once EAR [31] Dataset-base Text Sentence Tuning Once UPRISE [20] Dataset-base Text Sentence Tuning Once RAST [32] Dataset-base Text Sentence Tuning Once Self-Mem [17] Dataset-base Text Sentence Tuning Iterative FLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive PGRA [33] Wikipedia Text Sentence Inference Once FILCO [34] Wikipedia Text Sentence Inference Once RADA [35] Dataset-base Text Sentence Inference Once Filter-rerank [36] Synthesized dataset Text Sentence Inference Once R-GQA [37] Dataset-base Text Sentence Pair Tuning Once LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative TIGER [39] Dataset-base Text Item-base Pre-training Once LM-Indexer [40] Dataset-base Text Item-base Tuning Once BEQUE [9] Dataset-base Text Item-base Tuning Once CT-RAG [41] Synthesized dataset Text Item-base Tuning Once Atlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative RA VEN [43] Wikipedia Text Chunk Pre-training Once RETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative INSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative RRR [7] Search Engine Text Chunk Tuning Once RA-e2e [46] Dataset-base Text Chunk Tuning Once PROMPTAGATOR [21] BEIR Text Chunk Tuning Once AAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once RA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once RAG-Robust [48] Wikipedia Text Chunk Tuning Once RA-Long-Form [49] Dataset-base Text Chunk Tuning Once CoN [50] Wikipedia Text Chunk Tuning Once Self-RAG [25] Wikipedia Text Chunk Tuning Adaptive BGM [26] Wikipedia Text Chunk Inference Once CoQ [51] Wikipedia Text Chunk Inference Iterative Token-Elimination [52] Wikipedia Text Chunk Inference Once PaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative NoiseRAG [54] FactoidWiki Text Chunk Inference Once IAG [55] Search Engine,Wikipedia Text Chunk Inference Once NoMIRACL [56] Wikipedia Text Chunk Inference Once ToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive SKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive ITRG [59] Wikipedia Text Chunk Inference Iterative RAG-LongContext [60] Dataset-base Text Chunk Inference Once ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative IRCoT [61] Wikipedia Text Chunk Inference Recursive LLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once RAPTOR [63] Dataset-base Text Chunk Inference Recursive RECITE [22] LLMs Text Chunk Inference Once ICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative Retrieve-and-Sample [65] Dataset-base Text Doc Tuning Once Zemi [66] C4 Text Doc Tuning Once CRAG [67] Arxiv Text Doc Inference Once 1-PAGER [68] Wikipedia Text Doc Inference Iterative PRCA [69] Dataset-base Text Doc Inference Once QLM-Doc-ranking [70] Dataset-base Text Doc Inference Once Recomp [71] Wikipedia Text Doc Inference Once DSP [23] Wikipedia Text Doc Inference Iterative RePLUG [72] Pile Text Doc Inference Once ARM-RAG [73] Dataset-base Text Doc Inference Iterative GenRead [13] LLMs Text Doc Inference Iterative UniMS-RAG [74] Dataset-base Text Multi Tuning Once CREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once PKG [75] LLM Tabular,Text Chunk Inference Once SANTA [76] Dataset-base Code,Text Item Pre-training Once SURGE [77] Freebase KG Sub-Graph Tuning Once MK-ToD [78] Dataset-base KG Entity Tuning Once Dual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once KnowledGPT [15] Dataset-base KG Triplet Inference Muti-time FABULA [80] Dataset-base,Graph KG Entity Inference\n",
      "\n",
      "Chunk 4:\n",
      "and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Gen- eration” and “Augmentation”, respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on post- retrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG’s downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII. II. O VERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector\n",
      "\n",
      "Chunk 5:\n",
      "the original query to generate a pseudo document, which is later used as the query for retrieval. The pseudo document contains richer relevant information, which helps to retrieve more accurate results. TOC [127] leverages retrieved contents to decompose the ambiguous query into multiple clear sub-queries, which are sent to the generator and aggregated to produce the final result. For complex or ambiguous queries, RQ-RAG [128] breaks them down into clear subqueries for fine-grained retrieval and synthesizes the responses to deliver a cohesive answer to the original query. Tayal et al. [129] refined the initial query using dynamic few-shot examples and context retrieval, enhancing the generator’s grasp of user intent. Data Augmentation: Data augmentation improves data before retrieval, including techniques such as removing irrelevant information, eliminating ambiguity, updating outdated docu- ments, synthesize new data, etc. Make-An-Audio [44] uses captioning and audio-text re- trieval to generate captions for language-free audio to mitigate data sparsity, and adds random concept audio to improve the original audio. LESS [130] optimizes dataset selection for downstream tasks by analyzing gradient information, aiming to enhance model performance in response to instructional prompts. ReACC [91] employs data augmentation (including renaming and dead code insertion) to pre-train the code re- trieval model. Telco-RAG [131] enhances the retrieve accuracy by appling a “V ocabulary for 3GPP Specifications”, and match them to user queries with a router module. 2) Retriever Enhancement : In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we intro- duce efficient ways to enhance retrieval effectiveness. Recursive Retrieval: Recursive retrieval is to perform multi- ple searches to retrieve richer and higher-quality contents. ReACT [132] uses Chain-of-Thought (CoT) [133] to break queries down for recursive retrieval and provide richer infor- mation. RATP [134] uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output. Chunk Optimization: Chunk optimization refers to adjusting chunk size for improved retrieval results. LlamaIndex [135] incorporates a series of chunk optimiza- tion methods, one of which operates on a ‘small to big’ princi- ple. The core concept here is to pinpoint finer-grained content but return richer information. For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment. In auto- merge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first. To address the lack of contextual information, RAPTOR [136] employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure. Prompt- RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results.7 Fig. 4: Taxonomy of\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "Chunk 2:\n",
      "automatic evaluation methods [29], [161], [163]. C. Evaluation Aspects Contemporary evaluation practices of RAG models empha- size three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and generation. 1) Quality Scores: Quality scores include context rele- vance, answer faithfulness, and answer relevance. These qual- ity scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation [164]–[166]. Context Relevance evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content. Answer Faithfulness ensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions. Answer Relevance requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry. 2) Required Abilities: RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [167], [168]. These abilities are critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores. Noise Robustness appraises the model’s capability to man- age noise documents that are question-related but lack sub- stantive information. Negative Rejection assesses the model’s discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question. Information Integration evaluates the model’s proficiency in synthesizing information from multiple documents to address complex questions. Counterfactual Robustness tests the model’s ability to rec- ognize and disregard known inaccuracies within documents, even when instructed about potential misinformation. Context relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.13 TABLE II DOWNSTREAM TASKS AND DATASETS OF RAG Task Sub Task Dataset Method QA Single-hop Natural Qustion(NQ) [111] [26], [30], [34], [42], [45], [50], [52], [59], [64], [82] [3], [4], [22], [27], [40], [43], [54], [62], [71], [112] [20], [44], [72] TriviaQA(TQA) [113] [13], [30], [34], [45], [50], [64] [4], [27], [59], [62], [112] [22], [25], [43], [44], [71], [72] SQuAD [114] [20], [23], [30], [32], [45], [69], [112] Web Questions(WebQ) [115] [3], [4], [13], [30], [50], [68] PopQA [116] [7], [25], [67] MS MARCO [117] [4], [40], [52] Multi-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82] [7], [14], [22], [27], [59], [62], [69], [71], [91] 2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91] MuSiQue [120] [14], [51], [61], [91] Long-form QA ELI5 [121] [27], [34], [43], [49], [51] NarrativeQA(NQA) [122] [45], [60], [63], [123] ASQA [124] [24], [57] QMSum(QM) [125] [60], [123] Domain QA Qasper [126] [60], [63] COVID-QA [127] [35], [46] CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78],\n",
      "\n",
      "Chunk 3:\n",
      "CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78], [79] Recommendation Amazon(Toys,Sport,Beauty) [138] [39], [40] IE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42] RAMS [140] [36], [37] Relation Extraction T-REx [141],ZsRE [142] [27], [51] Reasoning Commonsense Reasoning HellaSwag [143] [20], [66] CoT Reasoning CoT Reasoning [144] [27] Complex Reasoning CSQA [145] [55] Others Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72] Language Modeling WikiText-103 [147] [5], [29], [64], [71] StrategyQA [148] [14], [24], [48], [51], [55], [58] Fact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50] PubHealth [150] [25], [67] Text Generation Biography [151] [67] Text Summarization WikiASP [152] [24] XSum [153] [17] Text Classification VioLens [154] [19] TREC [155] [33] Sentiment SST-2 [156] [20], [33], [38] Code Search CodeSearchNet [157] [76] Robustness Evaluation NoMIRACL [56] [56] Math GSM8K [158] [73] Machine Translation JRC-Acquis [159] [17]14 TABLE III SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG Context Relevance Faithfulness Answer Relevance Noise Robustness Negative Rejection Information Integration Counterfactual Robustness Accuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓ EM ✓ Recall ✓ Precision ✓ ✓ R-Rate ✓ Cosine Similarity ✓ Hit Rate ✓ MRR ✓ NDCG ✓ BLEU ✓ ✓ ✓ ROUGE/ROUGE-L ✓ ✓ ✓ The specific metrics for each evaluation aspect are sum- marized in Table III. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies. D. Evaluation Benchmarks and Tools A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model perfor- mance but also enhance comprehension of the model’s capabil- ities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD [167]–[169] focus on appraising the essential abilities of RAG models. Concur- rently, state-of-the-art automated tools like RAGAS [164], ARES [165], and TruLens 8 employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table IV. VII. D ISCUSSION AND FUTURE PROSPECTS Despite the considerable progress in RAG technology, sev- eral challenges persist that warrant in-depth research.This chapter will mainly introduce the current challenges and future research directions faced by RAG. A. RAG vs Long Context With the deepening of related research, the context of LLMs is continuously expanding [170]–[172]. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens 9. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked\n",
      "\n",
      "Chunk 4:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "Chunk 5:\n",
      "the original query to generate a pseudo document, which is later used as the query for retrieval. The pseudo document contains richer relevant information, which helps to retrieve more accurate results. TOC [127] leverages retrieved contents to decompose the ambiguous query into multiple clear sub-queries, which are sent to the generator and aggregated to produce the final result. For complex or ambiguous queries, RQ-RAG [128] breaks them down into clear subqueries for fine-grained retrieval and synthesizes the responses to deliver a cohesive answer to the original query. Tayal et al. [129] refined the initial query using dynamic few-shot examples and context retrieval, enhancing the generator’s grasp of user intent. Data Augmentation: Data augmentation improves data before retrieval, including techniques such as removing irrelevant information, eliminating ambiguity, updating outdated docu- ments, synthesize new data, etc. Make-An-Audio [44] uses captioning and audio-text re- trieval to generate captions for language-free audio to mitigate data sparsity, and adds random concept audio to improve the original audio. LESS [130] optimizes dataset selection for downstream tasks by analyzing gradient information, aiming to enhance model performance in response to instructional prompts. ReACC [91] employs data augmentation (including renaming and dead code insertion) to pre-train the code re- trieval model. Telco-RAG [131] enhances the retrieve accuracy by appling a “V ocabulary for 3GPP Specifications”, and match them to user queries with a router module. 2) Retriever Enhancement : In RAG systems, the quality of retrieved content determines the information fed into the generators. Lower content quality increases the risk of model hallucinations or other degradation. In this section, we intro- duce efficient ways to enhance retrieval effectiveness. Recursive Retrieval: Recursive retrieval is to perform multi- ple searches to retrieve richer and higher-quality contents. ReACT [132] uses Chain-of-Thought (CoT) [133] to break queries down for recursive retrieval and provide richer infor- mation. RATP [134] uses the Monte-Carlo Tree Search for simulations to select optimal retrieval content, which is then templated and forwarded to the generator for output. Chunk Optimization: Chunk optimization refers to adjusting chunk size for improved retrieval results. LlamaIndex [135] incorporates a series of chunk optimiza- tion methods, one of which operates on a ‘small to big’ princi- ple. The core concept here is to pinpoint finer-grained content but return richer information. For instance, Sentence-window retrieval fetches small text chunks and returns a window of relevant sentences surrounding the retrieved segment. In auto- merge retrieval, documents are arranged in a tree structure. The process retrieves the parent node, which encapsulates the content of its child nodes, by fetching the child node first. To address the lack of contextual information, RAPTOR [136] employs recursive embedding, clustering, and summarization of text chunks until further clustering becomes infeasible, thereby constructing a multi-level tree structure. Prompt- RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results.7 Fig. 4: Taxonomy of\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Gen- eration” and “Augmentation”, respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on post- retrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG’s downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII. II. O VERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector\n",
      "\n",
      "Chunk 2:\n",
      "date and long-tail knowledge [353]. We expect RAG to benefit from long context generation, rather than being replaced by it. VII. C ONCLUSION In this paper, we conducted a thorough and comprehensive survey on RAG within the context of AIGC, with a particular focus on augmentation foundations, enhancements, and ap- plications. We first systematically organized and summarized the foundation paradigms in RAG, providing insights into the interaction between retrievers and generators. Then, we reviewed the enhancements that further improve the effective- ness of RAG, including the enhancements on each component or the entire pipeline. To facilitate researchers across diverse domains, we showcased practical applications of RAG in a range of modalities and tasks. Finally, we also presented existing benchmarks for RAG, discussed current limitations of RAG, and shed light on promising future directions. REFERENCES [1] T. B. Brown, B. Mann et al., “Language models are few-shot learners,” in NeurIPS, 2020. [2] M. Chen, J. Tworek et al., “Evaluating large language models trained on code,” arXiv:2107.03374, 2021. [3] OpenAI, “GPT-4 technical report,” arXiv:2303.08774, 2023. [4] H. Touvron, T. Lavril et al., “Llama: Open and efficient foundation language models,” arXiv:2302.13971, 2023. [5] H. Touvron, L. Martin et al., “Llama 2: Open foundation and fine-tuned chat models,” arXiv:2307.09288, 2023. [6] B. Rozi `ere, J. Gehring et al., “Code llama: Open foundation models for code,” arXiv:2308.12950, 2023. [7] A. Ramesh, M. Pavlov, G. Goh et al., “Zero-shot text-to-image gener- ation,” in ICML, 2021. [8] A. Ramesh, P. Dhariwal, A. Nichol et al., “Hierarchical text-conditional image generation with CLIP latents,” arXiv:2204.06125, 2022. [9] J. Betker, G. Goh, L. Jing et al., “Improving image generation with better captions,” Computer Science, vol. 2, no. 3, p. 8, 2023. [10] R. Rombach, A. Blattmann, D. Lorenz et al., “High-resolution image synthesis with latent diffusion models,” in IEEE/CVF, 2022. [11] OpenAI, “Video generation models as world simulators,” https://openai. com/research/video-generation-models-as-world-simulators, 2024. [12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Comput., vol. 9, no. 8, pp. 1735–1780, 1997. [13] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,” in NeurIPS, 2017. [14] I. Goodfellow, J. Pouget-Abadie, M. Mirza et al., “Generative adver- sarial networks,” CACM, vol. 63, no. 11, pp. 139–144, 2020. [15] J. Devlin, M. Chang et al., “BERT: pre-training of deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019. [16] C. Raffel, N. Shazeer, A. Roberts et al., “Exploring the limits of transfer learning with a unified text-to-text transformer,” JMLR, vol. 21, pp. 140:1–140:67, 2020. [17] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,” JMLR, vol. 23, no. 120, pp. 1–39, 2022. [18] J. Kaplan, S. McCandlish, T. Henighan et al., “Scaling laws for neural language models,” 2020. [19] S. E. Robertson and H. Zaragoza, “The probabilistic relevance frame- work: BM25 and beyond,” FTIR, vol. 3, no. 4, pp. 333–389, 2009. [20] V . Karpukhin, B. Oguz, S. Min et al., “Dense passage retrieval for open-domain question answering,” in EMNLP, 2020. [21]\n",
      "\n",
      "Chunk 3:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "Chunk 4:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "Chunk 5:\n",
      "is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt. Generation. The posed query and selected documents are synthesized into a coherent prompt to which a large language model is tasked with formulating a response. The model’s approach to answering may vary depending on task-specific criteria, allowing it to either draw upon its inherent parametric knowledge or restrict its responses to the information con- tained within the provided documents. In cases of ongoing dialogues, any existing conversational history can be integrated into the prompt, enabling the model to engage in multi-turn dialogue interactions effectively. However, Naive RAG encounters notable drawbacks: Retrieval Challenges . The retrieval phase often struggles with precision and recall, leading to the selection of misaligned or irrelevant chunks, and the missing of crucial information. Generation Difficulties. In generating responses, the model may face the issue of hallucination, where it produces con- tent not supported by the retrieved context. This phase can also suffer from irrelevance, toxicity, or bias in the outputs, detracting from the quality and reliability of the responses. Augmentation Hurdles . Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Deter- mining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information. Moreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information. B. Advanced RAG Advanced RAG introduces specific improvements to over- come the limitations of Naive RAG. Focusing on enhancing re- trieval quality, it employs pre-retrieval and post-retrieval strate- gies. To tackle the indexing issues, Advanced RAG refines its indexing techniques through the use of a sliding window approach, fine-grained segmentation, and the incorporation of metadata. Additionally, it incorporates several optimization methods to streamline the retrieval process [8].4 Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle) Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78], [79] Recommendation Amazon(Toys,Sport,Beauty) [138] [39], [40] IE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42] RAMS [140] [36], [37] Relation Extraction T-REx [141],ZsRE [142] [27], [51] Reasoning Commonsense Reasoning HellaSwag [143] [20], [66] CoT Reasoning CoT Reasoning [144] [27] Complex Reasoning CSQA [145] [55] Others Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72] Language Modeling WikiText-103 [147] [5], [29], [64], [71] StrategyQA [148] [14], [24], [48], [51], [55], [58] Fact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50] PubHealth [150] [25], [67] Text Generation Biography [151] [67] Text Summarization WikiASP [152] [24] XSum [153] [17] Text Classification VioLens [154] [19] TREC [155] [33] Sentiment SST-2 [156] [20], [33], [38] Code Search CodeSearchNet [157] [76] Robustness Evaluation NoMIRACL [56] [56] Math GSM8K [158] [73] Machine Translation JRC-Acquis [159] [17]14 TABLE III SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG Context Relevance Faithfulness Answer Relevance Noise Robustness Negative Rejection Information Integration Counterfactual Robustness Accuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓ EM ✓ Recall ✓ Precision ✓ ✓ R-Rate ✓ Cosine Similarity ✓ Hit Rate ✓ MRR ✓ NDCG ✓ BLEU ✓ ✓ ✓ ROUGE/ROUGE-L ✓ ✓ ✓ The specific metrics for each evaluation aspect are sum- marized in Table III. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies. D. Evaluation Benchmarks and Tools A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model perfor- mance but also enhance comprehension of the model’s capabil- ities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD [167]–[169] focus on appraising the essential abilities of RAG models. Concur- rently, state-of-the-art automated tools like RAGAS [164], ARES [165], and TruLens 8 employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table IV. VII. D ISCUSSION AND FUTURE PROSPECTS Despite the considerable progress in RAG technology, sev- eral challenges persist that warrant in-depth research.This chapter will mainly introduce the current challenges and future research directions faced by RAG. A. RAG vs Long Context With the deepening of related research, the context of LLMs is continuously expanding [170]–[172]. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens 9. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked\n",
      "\n",
      "Chunk 2:\n",
      "automatic evaluation methods [29], [161], [163]. C. Evaluation Aspects Contemporary evaluation practices of RAG models empha- size three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and generation. 1) Quality Scores: Quality scores include context rele- vance, answer faithfulness, and answer relevance. These qual- ity scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation [164]–[166]. Context Relevance evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content. Answer Faithfulness ensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions. Answer Relevance requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry. 2) Required Abilities: RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [167], [168]. These abilities are critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores. Noise Robustness appraises the model’s capability to man- age noise documents that are question-related but lack sub- stantive information. Negative Rejection assesses the model’s discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question. Information Integration evaluates the model’s proficiency in synthesizing information from multiple documents to address complex questions. Counterfactual Robustness tests the model’s ability to rec- ognize and disregard known inaccuracies within documents, even when instructed about potential misinformation. Context relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.13 TABLE II DOWNSTREAM TASKS AND DATASETS OF RAG Task Sub Task Dataset Method QA Single-hop Natural Qustion(NQ) [111] [26], [30], [34], [42], [45], [50], [52], [59], [64], [82] [3], [4], [22], [27], [40], [43], [54], [62], [71], [112] [20], [44], [72] TriviaQA(TQA) [113] [13], [30], [34], [45], [50], [64] [4], [27], [59], [62], [112] [22], [25], [43], [44], [71], [72] SQuAD [114] [20], [23], [30], [32], [45], [69], [112] Web Questions(WebQ) [115] [3], [4], [13], [30], [50], [68] PopQA [116] [7], [25], [67] MS MARCO [117] [4], [40], [52] Multi-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82] [7], [14], [22], [27], [59], [62], [69], [71], [91] 2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91] MuSiQue [120] [14], [51], [61], [91] Long-form QA ELI5 [121] [27], [34], [43], [49], [51] NarrativeQA(NQA) [122] [45], [60], [63], [123] ASQA [124] [24], [57] QMSum(QM) [125] [60], [123] Domain QA Qasper [126] [60], [63] COVID-QA [127] [35], [46] CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78],\n",
      "\n",
      "Chunk 3:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "Chunk 4:\n",
      "Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to be more speciﬁc by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example. When generating “Sun”, the posterior is high for document 2 which mentions “The Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens. This observation suggests that the generator can complete the titles without depending on speciﬁc documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding\"The Sun. BART completes the generation\"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART’s parameters. Similarly, BART will complete the partial decoding\"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memorieswork together—the non-parametric component helps to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory. 4.4 Fact Veriﬁcation Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6Document 1: his works are considered classics of Americanliterature ... His wartime experiences formed the basis for his novel”A Farewell to Arms”(1929) ... Document 2: ... artists of the 1920s ”Lost Generation” expatriatecommunity. His debut novel,”The Sun Also Rises”, was publishedin 1926. BOS”TheSunAlsoRises” is a novelbythisauthor of” AFarewelltoArms ” Doc 1 Doc 2 Doc 3 Doc 4 Doc 5 Figure 2: RAG-Token document posteriorp(zi|x, yi,y \u0000i) for each generated token for input “Hem- ingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating “A Farewell to Arms\" and for document 2 when\n",
      "\n",
      "Chunk 5:\n",
      "LLM’s memory to guide retrieval, creating an unbounded memory pool that5 aligns the text more closely with data distribution through iter- ative self-enhancement [17], [18]. Routing in the RAG system navigates through diverse data sources, selecting the optimal pathway for a query, whether it involves summarization, specific database searches, or merging different information streams [19]. The Predict module aims to reduce redundancy and noise by generating context directly through the LLM, ensuring relevance and accuracy [13]. Lastly, the Task Adapter module tailors RAG to various downstream tasks, automating prompt retrieval for zero-shot inputs and creating task-specific retrievers through few-shot query generation [20], [21] .This comprehensive approach not only streamlines the retrieval pro- cess but also significantly improves the quality and relevance of the information retrieved, catering to a wide array of tasks and queries with enhanced precision and flexibility. 2) New Patterns: Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks. Innovations such as the Rewrite-Retrieve-Read [7]model leverage the LLM’s capabilities to refine retrieval queries through a rewriting module and a LM-feedback mechanism to update rewriting model., improving task performance. Similarly, approaches like Generate-Read [13] replace tradi- tional retrieval with LLM-generated content, while Recite- Read [22] emphasizes retrieval from model weights, enhanc- ing the model’s ability to handle knowledge-intensive tasks. Hybrid retrieval strategies integrate keyword, semantic, and vector searches to cater to diverse queries. Additionally, em- ploying sub-queries and hypothetical document embeddings (HyDE) [11] seeks to improve retrieval relevance by focusing on embedding similarities between generated answers and real documents. Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER- RETGEN [14], showcase the dynamic use of module out- puts to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27]. D. RAG vs Fine-tuning The augmentation of LLMs has attracted considerable atten- tion due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct charac- teristics as illustrated in Figure 4. We used a quadrant chart to illustrate the differences among three methods in two dimen- sions: external knowledge requirements and model adaption requirements.\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "SYNCHROMESH‡§ RESDSQL§ REFSQL‡§ CodeICL§ MURRE∥¶ StackSpotAI‡§ E&V Code4UIE§ De-fine‡∥ ImputBlaster¶ RAG for Knowledge Knowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others CBR-KBQA ‡§∥ TIARA †‡§ Keqing †‡§ RNG-KBQA ‡∥ ReTraCk § SKP †‡§ UniK-QA †‡ KG-FiD ‡ GRAPE ‡ SKURG †‡ KnowledGPT ‡ EFSUM § EfficientQA ‡ CORE § Convinse †‡ RINK ‡§ T-RAG ‡§ StructGPT ‡ GRetriever § SURGE § K-LaMP RHO ∥ RAG for 3D Text-to-3D ReMoDiffuse †‡ AMD † RAG for Image Image Generation Image Captioning Others RetrieveGAN‡ IC-GAN§ Re-imagen§ RDM Retrieve&Fuse§ KNN-Diffusion MA∥ REVEAL‡ SMALLCAP† CRSR† RA-Transformer PICa∥ Maira‡ KIF‡ RA-VQA‡ RAG for Video Video Captioning Video QA&Dialogue Others KaVD‡§ R-ConvED‡§ CARE§ EgoInstructor†‡§ MA-DRNN†‡ R2A‡ Tvqa+§ VGNMN‡ VidIL†‡ RAG-Driver‡ Animate-A-Story†§ RAG for Science Drug Discovery Biomedical Informatics Enhancement Math Applications RetMol†§ PromptDiff†‡ PoET‡ Chat-Orthopedist†§ BIOREADER† MedWriter‡ QARAG†‡ LeanDojo‡ RAG-for-math-QA†‡ RAG for Audio Audio Generation Audio Captioning Re-AudioLDM§ Make-An-Audio†§ RECAP‡§ Query-based Latent-based Logit-based Speculative Query+Latent Latent+Logit † Input ‡ Retriever § Generator ∥ Output ¶ Pipeline generated code to better utilize dispersed information and improve outcomes. ITER-RETGEN [187] iteratively enhances content quality by using the generator’s output to pinpoint knowledge gaps, retrieving necessary information, and in- forming future generation cycles. SelfMemory [188] utilizes a retrieval-augmented generator iteratively to form an expansive memory pool, from which a memory selector picks an output to inform the next generation cycle. RAT [189] initially generates content by an LLM with a zero-shot CoT prompt, then revises each thought step by retrieving knowledge from external knowledge base. IV. APPLICATIONS In this section, we focus on RAG applications spanning various modalities. To echo with the taxonomy of RAG foundations and enhancements, we also demonstrate their utilization across different tasks in Table I. A. RAG for Text To begin with, text generation is among the most important and widely deployed applications for RAG. Here we introduce popular works for seven tasks, respectively. 1) Question Answering : Question answering involves the process of providing responses to posed questions by drawing from a vast and comprehensive collection of textual sources. FiD [35] and REALM [33] identify the top-k most pertinent article snippets based on the query and forward each snippet along with the question to LLMs to generate k responses. These responses are then synthesized into a final answer. Toutanova et al. [190] substituted the text corpus in REALM with subgraphs from a knowledge graph, yielding impressive results. As shown in Fig. 5, RETRO [36] employs attention mechanisms to integrate the question with relevant retrieved documents within the model to produce the final answer. SKR [183] observes that using RAG does not invariably benefit question answering and thus explored guiding the model to evaluate its grasp of pertinent knowledge, subse- quently adapting its use of external resources for retrieval enhancement. TOG [191] introduces an innovative knowledge graph-augmented LLM framework, which excels by fostering interactions between LLMs and the knowledge graph and by expanding the inference path space with beam search. NPM [119] pioneers the use of nonparametric data distribu- tions in lieu of the\n",
      "\n",
      "Chunk 2:\n",
      "the opti- mal k depends on the nature of the task, as well as the performance requirement. We encourage future researchers to look for different ks when applying our method to various applications. 5.2 Why does RAG fail? To gain a better understanding of why RAG lags behind LC, we analyze the failure reasons for the examples that cannot be answered by RAG. We ﬁrst manually check some examples for which our RAG-and-Route step predicts “unanswerable” and summarize four typical failure reasons, then prompt LLM to classify all the examples. The four reasons include: (A) The query requires multi-step reasoning so the results of previous steps are needed to retrieve information for later steps, e.g. “What nationality is the performer of song XXX”. (B) The query is general, e.g. “What does the group think about XXX” , which is challenging for the retriever to formulate a good query. (C) The query is long and complex, which is challenging for the retriever to understand. How- ever, answering this kind of questions is arguably, 885Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC 1 LC 49.70 32.76 47.83 52.33 61.85 62.96 40.22 20.73 43.08 85.57 Dragon 2 RAG 38.09 21.91 44.33 53.08 51.61 50.05 30.47 19.93 21.25 50.22 3 combine 46.81 28.50 43.82 54.62 56.58 60.62 40.66 20.07 37.79 78.60 4 RAG ratio 77.88 74.00 84.00 97.33 86.00 77.00 66.00 95.50 61.25 59.83 5 Token ratio 37.87 19.31 54.15 34.78 32.64 55.65 48.16 16.64 38.71 40.83 Table 2: Results for Gemini-1.5-Pro using Dragon retriever. an advantage of LLMs. (D) The query is implicit, demanding a thorough understanding of the en- tire context. For instance, in a lengthy conversa- tional narrative about a space voyage, a question like “What caused the shadow behind the spaceship?” requires readers to connect the dots and deduce the answer, as there is no explicit men- tion of the shadow when the cause is revealed. number of queries Figure 4: Distribution of typical RAG failure reasons. Using these reasons, we prompt Gemini-1.5-Pro with few-shot in-context examples that we man- ually annotated, to classify all the unanswerable examples into these four categories, plus an “other\" option. Fig. 4 shows the distribution of failure rea- sons on the seven datasets in LongBench. Each dataset may contain different number of RAG fail- ure cases, resulting in various bar heights. The distribution patterns are consistent with the nature of the datasets. For example, the three Wikipedia- based multi-hop reasoning datasets (HotpotQA, 2WikiMQA, MuSiQue) are challenging for RAG because of multi-step retrieval as shown in blue. For NarrativeQA, which are long stories containing a lot of dialogues, most failure cases are due to im- plicit queries that requires understanding the whole context (shown in green). For QMSum, which is a summarization dataset contains open-ended ques- tions, failures are mostly due to general queries (shown in red). We manually checked the exam- ples classiﬁed as “others” and ﬁnd that most of them are actually multi-step questions, often with ambiguities, which poses challenges for\n",
      "\n",
      "Chunk 3:\n",
      "performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen- eration. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst country to host this international sports competition twice.” As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options—quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veriﬁcation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks,\n",
      "\n",
      "Chunk 4:\n",
      "e.g., multi-hop rea- soning (Yang et al., 2018) and long-form question answering (Fan et al., 2019). To fulfill complex information needs, recent work proposes to gather required knowledge multi- ple times throughout the generation process, using partial generation (Trivedi et al., 2022a; Press et al., 2022)) or forward-looking sentence(s) (Jiang et al., 2023) as search queries. However, such structured workflows of interleaving retrieval with generation have the following limitations: (1) as intermediate generation is conditioned on knowledge retrieved before, with no awareness of knowledge retrieved afterwards, they fail to process all retrieved knowl- edge as a whole during the generation process; (2) they require multi-round retrieval to gather a com- prehensive set of knowledge, and may frequently change the prompts by updating newly retrieved knowledge, thus increasing the overheads of both retrieval and generation. In this paper, we find it simple but effective to enhance retrieval-augmented LLMs through itera- tive retrieval-generation synergy (ITER -RETGEN, Fig 1). ITER -RETGEN iterates retrieval-augmented generation and generation-augmented retrieval: Retrieval-augmented generation outputs a response to a task input based on all retrieved knowledge (initially using the task input as the query). This 9248output shows what might be needed to fulfill the task, and thus can serve as an informative context to retrieve more relevant knowledge, i.e., generation- augmented retrieval. The newly retrieved knowl- edge can benefit another iteration of retrieval- augmented generation. We can also leverage model generations to adapt retrieval, by distilling knowl- edge from a re-ranker with access to model genera- tions to a dense retriever with access to task inputs only, which may be beneficial in scenarios where user inputs can be easily collected, but relevant knowledge or desirable outputs are not annotated. We evaluate our method on three tasks, includ- ing multi-hop question answering, fact verification, and commonsense reasoning. Our method prompts an LLM to produce a chain of reasoning steps fol- lowed by the final answer under a few-shot set- ting. For in-context demonstrations, we focus on problem-solving and follow Wei et al. (2022) to annotate chains of thoughts, without explicitly con- sidering how generation-augmented retrieval might be affected, which makes it conceptually simple and easy to implement. Our method achieves up to 8.6% absolute gains over previous state-of-the- art retrieval-augmented methods on four out of six datasets while being competitive on the remaining two. According to our experiments, generation gen- erally benefits from more iterations, with two itera- tions giving the most performance gains. One may customize the performance-cost tradeoffs by choos- ing an appropriate number of iterations. We can further improve performance and also reduce itera- tions via the aforementioned generation-augmented retrieval adaptation. We summarize our findings as follows: • Automatic metrics such as exact match can significantly underestimate the performance of LLMs in question answering tasks. More- over, improvements in exact match do not always reflect improvements in generations. Evaluation using LLMs may be more reliable. • ITER -RETGEN is superior to or competi- tive with state-of-the-art retrieval-augmented methods, while being simpler and causing fewer overheads of\n",
      "\n",
      "Chunk 5:\n",
      "two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross- encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them. Docu- ments with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for Open- Domain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 - /50.1 37.4 - T5-11B+SSM[52] 36.6 - /60.5 44.7 - Open Book REALM [20] 40.4 - / - 40.7 46.8 DPR [26] 41.5 57.9/ - 41.1 50.6 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Seq. 44.5 56.8/68.0 45.2 52.2 Table 2: Generation and classiﬁcation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2 * BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speciﬁc information required to generate the reference answer, (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see §4.5). 4.3 Jeopardy Question Generation Table 2 shows\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "evaluated with prompts composed of the gold document ⋆and a varying number of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083). Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k,⋆, /u♀k, Q] Near - [I, /u♀k,⋆, Q] # /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon 0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602 2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111 4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736 6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656 8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424 10 0.2537 - - - 0.2180 - - - 0.3716 - - - 12 0.2688 - - - 0.2382 - - - 0.3991 - - - 14 0.2583 - - - 0.2280 - - - 0.4118 - - - 16 0.2413 - - - 0.2024 - - - 0.3889 - - - 18 0.2348 - - - 0.1795 - - - 0.3781 - - - Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083). Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q] # Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon 0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091 2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912 4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949 6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908 8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704 10 0.3390 - - - 0.3675 - - -\n",
      "\n",
      "Chunk 2:\n",
      "irrelevant and has zero-valued attention weights. Given the surge of interest in long context LLM research and much more required computation at inference 2, it is still unclear for practitioners whether extending the context window of LLM 1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity search library (Johnson et al., 2019). 2For example, the price of GPT-4 with 32k context length is twice the 8k context model. 1Published as a conference paper at ICLR 2024 provides higher accuracy than the retrieval-augmentation for downstream tasks with informative queries. Moreover, it would be compelling if we could combine the strength of both methods and achieve even higher accuracies. In this work, we attempt to answer the above questions through a comprehensive study. Specifically, we make the following contributions: 1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre- trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks, including single and multi document question answering (QA), query-based summarization, and in context few-shot learning tasks. 2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation. 3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K) can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6), outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score. It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while can be much faster at generation (e.g., 4× faster on NarrativeQA). We organize the rest of the paper as follows. We discuss related work in Section 2, and present the experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5. 2 R ELATED WORK In this section, we discuss the related work in long context LLM, efficient attention methods, and retrieval-augmented language models. 2.1 P ARALLEL WORK When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b), and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model, i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their study are relatively small, thus they have limited zero-shot capability of incorporating context through retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context model\n",
      "\n",
      "Chunk 3:\n",
      "established correct answer is “President Roosevelt”, the response would be deemed incorrect under our current evaluation schema. Recognizing this limitation, we acknowledge the necessity for a more advanced analysis of answer variations, which we leave to future research. 5 RESULTS Studying the characteristics of optimal prompts for RAG systems corresponds to answering our research question (RQ): \"What char- acteristics are desirable in a retriever to optimize prompt construction for RAG systems in order to increase the LLM effectiveness?\". More specifically, we focus on three essential elements of the configura- tion: type, number, and positioning of the documents, and for each, we test various prompt combinations. To facilitate the understand- ing of our experimental setup, we employ a streamlined schema for representing the composition of prompts via the following symbols: [I, ⋆, ὑ7, /u♀k, , Q]. The task instruction (I) and the query (Q) are consistently positioned at the beginning and end, respectively. The middle section varies and represents different contextual elements - in this instance, these are gold, relevant, distracting, and random, appearing in that specific sequence. Additionally, the number of contextual documents is a variable in its own right and will be reported in the results tables below. 5.1 Impact of Distracting Documents LLM Input - Distracting /u♀kand Gold ⋆ T ask Instruction... Documents: Document [1](Title: Han Solo) Before the events of the film, he and Chewbacca had lost the “Millennium Falcon” to thieves, but they reclaim the ship after it... Document [2](Title: Millennium Falcon) The “Falcon” has been depicted many times in the franchise, and ownership has changed several times... Document [3](Title: Millennium Falcon) Han Solo won the Millennium Falcon from Lando Calrissian in the card game sabacc... Question: who owned the millennium falcon be- fore han solo Answer: Han Solo Figure 2: Example LLM input with an erroneous output, high- lighted in red. The context of the prompt is composed of distracting documents and the gold near the query. The task instruction is as in Figure 1. In our first set of experiments, we use a selection of 10K queries from the training set of the NQ-open dataset and assume an oracle setup in which the gold document for the query is known. To this effect, we add to the gold document a set of distracting documents, i.e., documents with high retrieval scores but not containing the answer, in order to measure their impact on the system; schemat- ically [I, /u♀k, ⋆, Q]. Figure 2 shows an example of this setup’s visualization. Results of this experiment are shown in Table 1 (far, mid, and near relate to the distance between the gold document and the query; more details in the following sub-section). A crit- ical observation emerging from this analysis is a clear pattern of progressive accuracy degradation as the number of distracting doc- uments included in the context increases. This was observed across all LLMs, with accuracy deteriorating by more than 0.38 (−67%) in some cases. Even more importantly, adding just one distracting document causes a\n",
      "\n",
      "Chunk 4:\n",
      "modeling and precise concept comprehension. Results on Negative Rejection Testbed We evaluated the rejection rate when only noise documents were provided. The results are shown in Table 3. In addi- tion to evaluating the rejection rate through exact matching (Rej in Table 3), we also utilize ChatGPT to determine if the responses from the LLMs contain any rejection informa- tion (Rej∗ in Table 3). We can see that: Negative Rejection poses a challenge for RAG in LLMs.The highest rejection rates for LLMs in English and Chinese were only 45% and 43.33%, respectively. This suggests that LLMs can be easily misled by noisy documents, leading to incorrect answers. In addition, through comparing Rej and Rej ∗, we found that LLMs fail to strictly follow instructions, and they often generate unpredictable responses, which make it hard to use them as state triggers (such as for recognizing rejection). We conduct case studies in Table 4. The first error is because of Evidence uncertainty. Although the document only mentions contact with “Adam McKay” and does not explicitly state that he is the director of the movie, the model still concludes that he holds this role. The first er- ror is because of Concept confusion. The information pro- vided in the answer pertains to “the 2018 Winter Olympics” instead of “the 2022 Olympics” mentioned in the question. Retrieval-augmented generation poses a greater challenge of Question Answer Response who will direct Irredeemable film? Jeymes Samuel ... Adam McKayto mo vie adaptation of “Irredeemable” from Which country w- on the most medals at the 2022 Winter Olympics? Norway ... that won the most medals ... is German- y. It has won a total of 31 medals ... Table 4: Error cases of negative rejection generated by ChatGLM2-6B. The bold text highlights the error answers. negative rejection compared to answer directly as it presents relevant documents that could potentially mislead the LLMs and result in incorrect responses. In future developments, it will be crucial for LLMs to enhance their ability to accu- rately match questions with the appropriate documents. Results on Information Integration Testbed We evaluated the accuracy based on the different noise ratios in external documents, and the results are shown in Table 5. When comparing the model to Table 1, we observed that it has a weak information integration ability, which in turn affects its noise robustness. We can see that: (1) Information integration poses a challenge for RAG in LLMs.Even without noise, the highest accuracy of LLMs can only reach 60% and 67% for English and Chinese, respectively. After adding noise, the highest accuracy de- creases to 43% and 55%. These results suggest that LLMs struggle with integrating information effectively and are not well-suited for directly answering complex questions. (2) Complex questions are more challenging for RAG with noisy documents.Performance decline becomes sig- nificant when the noise ratio is 0.4, but for simple problems, a significant decline occurs only at a noise ratio of 0.8 at a significance level of 0.05. This indicates that complex\n",
      "\n",
      "Chunk 5:\n",
      "constructed by latest news articles. Additionally, we retrieve external documents from Internet through search engines. Finally, we expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. The overall procedure of our data construction is illustrated in Figure 2. QA instances generation.We first collect latest news ar- ticles and use prompts to make ChatGPT generate events, questions, and answers for each articles. For example, as shown in the Figure 2, for a report about “The 2022 Nobel Prize”, ChatGPT will generate corresponding event, ques- tion and provide key information for answering it. By gen- erating events, the model is able to preliminarily filter out news articles that do not contain any events. After genera- tion, we manually check the answer and filter out data that is difficult to retrieve through search engines. Retrieve using search engine.For each query, we use Google’s API to fetch 10 relevant web pages and extract cor- responding snippets of text from them. Simultaneously, we read these web pages and convert their textual content into text chunks with a maximum length of 300 tokens. Using an open-source dense retriever, we select the top-30 text chunks that match the query most effectively. These retrieved text chunks, along with the snippets provided by the search API, will serve as our external documents. These documents will be divided into positive documents and negative documents based on whether they contain the answer. Testbeds construction for each ability.We expand the corpus and divided it into 4 testbeds to evaluate the above basic abilities of LLMs. To evaluate the noise robustness, we sample varying numbers of negative documents ac- cording to the desired ratio of noises. For negative rejec- tion, all the external documents are sampled from negative documents. For the information integration ability, we fur- ther construct data based on the above generated questions. This involves expanding or rewriting these questions so that their answers encompass multiple aspects. For example, the question “Who won the MVP of Super Bowl 2023?” can be rewrite as “Who won the MVPs of Super Bowl 2022 and 2023?”. Consequently, answering such questions re- quires utilizing information from various documents. Dif- ferent from the first three abilities, the data of counterfactual robustness is constructed solely based on the internal knowl- edge of the model. Based on the aforementioned generated questions mentioned above, we adopt ChatGPT to automat- ically generate its known knowledge. Specifically, we use prompts to allow the model to generate both questions and answers that are already known. For example, based on the question “Who was awarded the 2022 Nobel Prize for Phys- iology and Medicine?”, the model will generate the known question “Who was awarded the 2021 Nobel Prize in Lit- erature?” and answer “Abdulrazak Gurnah”. We then man- ually verified the generated answers, and retrieve relevant documents as described above. In order to make documents contain factual errors, we manually modify the answers and replace the corresponding parts in the document. Finally, we collect totally\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78], [79] Recommendation Amazon(Toys,Sport,Beauty) [138] [39], [40] IE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42] RAMS [140] [36], [37] Relation Extraction T-REx [141],ZsRE [142] [27], [51] Reasoning Commonsense Reasoning HellaSwag [143] [20], [66] CoT Reasoning CoT Reasoning [144] [27] Complex Reasoning CSQA [145] [55] Others Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72] Language Modeling WikiText-103 [147] [5], [29], [64], [71] StrategyQA [148] [14], [24], [48], [51], [55], [58] Fact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50] PubHealth [150] [25], [67] Text Generation Biography [151] [67] Text Summarization WikiASP [152] [24] XSum [153] [17] Text Classification VioLens [154] [19] TREC [155] [33] Sentiment SST-2 [156] [20], [33], [38] Code Search CodeSearchNet [157] [76] Robustness Evaluation NoMIRACL [56] [56] Math GSM8K [158] [73] Machine Translation JRC-Acquis [159] [17]14 TABLE III SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG Context Relevance Faithfulness Answer Relevance Noise Robustness Negative Rejection Information Integration Counterfactual Robustness Accuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓ EM ✓ Recall ✓ Precision ✓ ✓ R-Rate ✓ Cosine Similarity ✓ Hit Rate ✓ MRR ✓ NDCG ✓ BLEU ✓ ✓ ✓ ROUGE/ROUGE-L ✓ ✓ ✓ The specific metrics for each evaluation aspect are sum- marized in Table III. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies. D. Evaluation Benchmarks and Tools A series of benchmark tests and tools have been proposed to facilitate the evaluation of RAG.These instruments furnish quantitative metrics that not only gauge RAG model perfor- mance but also enhance comprehension of the model’s capabil- ities across various evaluation aspects. Prominent benchmarks such as RGB, RECALL and CRUD [167]–[169] focus on appraising the essential abilities of RAG models. Concur- rently, state-of-the-art automated tools like RAGAS [164], ARES [165], and TruLens 8 employ LLMs to adjudicate the quality scores. These tools and benchmarks collectively form a robust framework for the systematic evaluation of RAG models, as summarized in Table IV. VII. D ISCUSSION AND FUTURE PROSPECTS Despite the considerable progress in RAG technology, sev- eral challenges persist that warrant in-depth research.This chapter will mainly introduce the current challenges and future research directions faced by RAG. A. RAG vs Long Context With the deepening of related research, the context of LLMs is continuously expanding [170]–[172]. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens 9. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked\n",
      "\n",
      "Chunk 2:\n",
      "automatic evaluation methods [29], [161], [163]. C. Evaluation Aspects Contemporary evaluation practices of RAG models empha- size three primary quality scores and four essential abilities, which collectively inform the evaluation of the two principal targets of the RAG model: retrieval and generation. 1) Quality Scores: Quality scores include context rele- vance, answer faithfulness, and answer relevance. These qual- ity scores evaluate the efficiency of the RAG model from different perspectives in the process of information retrieval and generation [164]–[166]. Context Relevance evaluates the precision and specificity of the retrieved context, ensuring relevance and minimizing processing costs associated with extraneous content. Answer Faithfulness ensures that the generated answers remain true to the retrieved context, maintaining consistency and avoiding contradictions. Answer Relevance requires that the generated answers are directly pertinent to the posed questions, effectively addressing the core inquiry. 2) Required Abilities: RAG evaluation also encompasses four abilities indicative of its adaptability and efficiency: noise robustness, negative rejection, information integration, and counterfactual robustness [167], [168]. These abilities are critical for the model’s performance under various challenges and complex scenarios, impacting the quality scores. Noise Robustness appraises the model’s capability to man- age noise documents that are question-related but lack sub- stantive information. Negative Rejection assesses the model’s discernment in refraining from responding when the retrieved documents do not contain the necessary knowledge to answer a question. Information Integration evaluates the model’s proficiency in synthesizing information from multiple documents to address complex questions. Counterfactual Robustness tests the model’s ability to rec- ognize and disregard known inaccuracies within documents, even when instructed about potential misinformation. Context relevance and noise robustness are important for evaluating the quality of retrieval, while answer faithfulness, answer relevance, negative rejection, information integration, and counterfactual robustness are important for evaluating the quality of generation.13 TABLE II DOWNSTREAM TASKS AND DATASETS OF RAG Task Sub Task Dataset Method QA Single-hop Natural Qustion(NQ) [111] [26], [30], [34], [42], [45], [50], [52], [59], [64], [82] [3], [4], [22], [27], [40], [43], [54], [62], [71], [112] [20], [44], [72] TriviaQA(TQA) [113] [13], [30], [34], [45], [50], [64] [4], [27], [59], [62], [112] [22], [25], [43], [44], [71], [72] SQuAD [114] [20], [23], [30], [32], [45], [69], [112] Web Questions(WebQ) [115] [3], [4], [13], [30], [50], [68] PopQA [116] [7], [25], [67] MS MARCO [117] [4], [40], [52] Multi-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82] [7], [14], [22], [27], [59], [62], [69], [71], [91] 2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91] MuSiQue [120] [14], [51], [61], [91] Long-form QA ELI5 [121] [27], [34], [43], [49], [51] NarrativeQA(NQA) [122] [45], [60], [63], [123] ASQA [124] [24], [57] QMSum(QM) [125] [60], [123] Domain QA Qasper [126] [60], [63] COVID-QA [127] [35], [46] CMB [128],MMCU Medical [129] [81] Multi-Choice QA QuALITY [130] [60], [63] ARC [131] [25], [67] CommonsenseQA [132] [58], [66] Graph QA GraphQA [84] [84] Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42] Personal Dialog KBP [134] [74], [135] DuleMon [136] [74] Task-oriented Dialog CamRest [137] [78],\n",
      "\n",
      "Chunk 3:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "Chunk 4:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "Chunk 5:\n",
      "a multi-level tree structure. Prompt- RAG [137] enhances retrieval accuracy by pre-generating a table of contents, enabling the model to autonomously select relevant chapters based on the query. Raina et al. [138] break text chunks into finer atomic statements to achieve higher recall and improved results.7 Fig. 4: Taxonomy of RAG Enhancements. Retriever Finetuning: The retriever, central to the RAG system, relies on a proficient embedding model [139]–[142] to represent related content and feed the generator, enhancing system performance. Additionally, embedding models with strong expressive power can be fine-tuned with domain-specific or task-related data to boost performance in targeted areas. REPLUG [86] treats LM as a black box and update the retriever model based on the final results. APICoder [88] finetunes the re- triever with python files and api names, signature, descrip- tion. EDITSUM [98] finetunes the retriever to decrease the jaccard distance between summaries after retrieval. SYN- CHROMESH [81] adds tree distance os ASTs in the loss and uses Target Similarity Tuning (TST) to finetune the retriever. R-ConvED [48] finetunes the retriever with the same data as generator. Kulkarni et al. [143] applied infoNCE loss to finetune the retriever. Hybrid Retrieval: Hybrid retrieve denotes the concurrent employment of a diverse array of retrieval methodologies or the extraction of information from multiple distinct sources. RAP-Gen [144], BlendedRAG [145]and ReACC [91] use both dense retriever and sparse retriever to improve the quality of retrieval. Rencos [80] uses sparse retriever to retrieve similar code snippets on syntactic-level and uses dense retriever to retrieve similar code snippets on semantic-level. BASHEX- PLAINER [99] first uses dense retriever to capture semantic information and then uses sparse retriever to acquire lexical information. RetDream [50] first retrieves with text and then retrieves with the image embedding. CRAG [146] features a retrieval evaluator that gauges document relevance to queries, prompting three retrieval responses based on confidence: direct use of results for Knowledge Refinement if accurate, Web Search if incorrect, and a hybrid approach for ambiguous cases. Huang et al. [147] improved question-answering by introducing DKS (Dense Knowledge Similarity) and RAC (Retriever as Answer Classifier) in the retrieval phase, evalu- ating answer relevance and knowledge applicability. UniMS- RAG [148] introduces a novel kind of token, termed as the “acting token”, which determines the source from which to retrieve information. Koley et al. [149] enhance image retrieval by integrating sketch and text for fine-grained retrieval, yield- ing improved results. Re-ranking: The Rerank technique refers to reordering the retrieved content in order to achieve greater diversity and better results. Re2G [150] applies a re-ranker [151] model after the tradi- tional retriever to reduce the impact of information loss caused by compressing text into vectors. AceCoder [152] reranks the retrieved programs with a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "incorrect predictions. We demonstrated case study in Table 15. Consideration in combination with recent approaches (Creswell & Shanahan, 2022) to boost generative faithfulness is a also direction worthy of future research. ACKNOWLEDGEMENTS This work was supported in part by NSF IIS-2119531, IIS-2137396, IIS-2142827, CCF-1901059, and ONR N00014-22-1-2507. Wenhao is supported in part by Bloomberg Data Science Ph.D Fellowship. We are grateful to the reviewers for their insightful suggestions that have improved the quality of our paper. Their dedication to ensuring the completeness of our research is greatly appreciated. 9Published as a conference paper at ICLR 2023 ETHICS STATEMENT Large language models have a wide range of beneﬁcial applications for society, but they also have potentially harmful applications. Previous work has shown various forms of bias, such as racial and gender bias, in large language models like GPT-3, even after explicit efforts to reduce toxic language (Chan, 2022). The importance of addressing these societal harms is acknowledged by OpenAI themselves in their 2020 paper introducing GPT-3 (Brown et al., 2020), which stated “we focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 ... and issues of bias, fairness, and representation within models like GPT-3.” on page 34. The goal of this paper is to utilize knowledge stored in the parameters of large language models to answer open-domain questions and solve knowledge-intensive tasks. Unlike retrieve-then-read where an external corpus can be curated to be trustworthy, the use of a model to generate contextual documents may further permeate existing biases in common models. First, our work shows that generated documents suffer from challenges of stale information from outdated documents used for training. Second, we show that generated documents tend to be less diverse, potentially biasing answers towards more common entities and terms from the training data. Finally, we conducted experiments on only three large language models. It is possible that some of our conclusions or observations may not necessarily hold for other models trained with different data or objectives. Regarding ethical solutions, future work includes (i) further exploring potential bias and intentional or unintentional harm that may result from using generated contextual documents; (ii) better aligning language models with user intent to generate less biased contents and fewer fabricated facts. REFERENCES Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault F ´evry, et al. Promptsource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pp. 93–104, 2022. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In EMNLP 2013, pp. 1533–1544, 2013. Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio Petroni. Autoregressive search engines: Generating substrings as document identiﬁers. arXiv preprint arXiv:2204.10628, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n",
      "\n",
      "Chunk 2:\n",
      "1 31.5% 63.0% 54.5% k = 5 31.9% 64.2% 54.3% k = 10 31.0% 64.1% 52.2% L = 50 31.9% 64.2% 54.3% L = 100 32.3% 63.7% 51.0% Table 8: In the LM-as-a-service setup, using GPT-Neo, we search for other values for L and k, and note that the choice of our hyperparameters is robust. Training details To train EPR, we use the Adam optimizer (Kingma and Ba, 2015) with batch size 120 and learning rate 1e-4 on eight RTX 3090. We run training for 30 epochs. We used the default DPR hyperparameters without tuning. We used the final epoch of the model to perform model selec- tion, and applied minimal learning rate tuning on the validation set of BREAK . Risk assessment Large language models have been shown to exhibit various kinds of bias (Bender et al., 2021), since EPR is trained on the signal obtained from such large LMs, it might also exhibit these biases. Additional examples Tables 9, 10, and 11 pro- vide more examples for cases where EPR is cor- rect while CBR is incorrect along with the top-3 prompts for each method. 266710 20 30 40 50 60 70 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 Break 20 40 60 80 100 0.000 0.005 0.010 0.015 0.020 0.025 0.030 0.035 MT op 20 40 60 80 100 0.000 0.005 0.010 0.015 0.020 0.025 0.030 SMCalFlow Figure 5: Distribution of the number of in-context examples per test instance for each of the datasets. We mark the distribution mean using a dashed line. EPR CBR Test Example Utterance Remind me to add 2 dozen eggs to my grocery list. Meaning Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO add 2 dozen eggs to my grocery list ] ] Top-1 Utterance Remind me to get two bottles of water. Please add a grocery list to my list of things to be reminded about doing today. Meaning Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO get two bottles of water ] ] [IN:CREATE_REMINDER [SL:TODO a grocery list ] [SL:PERSON_REMINDED my ] [SL:DATE_TIME today ] ] Top-2 Utterance Remind me to bring an extra pair of shoes to the river. Remind me to make a grocery list Meaning Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO bring an extra pair of shoes to the river ] ] [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO make a grocery list ] ] Top-3 Utterance Remind me to add bottled water to grocery list. I need to make a grocery list; will you remind me when I get off work at 5:00 p.m.? Meaning Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED me ] [SL:TODO add bottled water to grocery list ] ] [IN:CREATE_REMINDER [SL:TODO make a grocery list ] [SL:PERSON_REMINDED me ] [SL:DATE_TIME at 5 : 00 p.m . ] ] Table 9: An example from MTOP development set where EPR is correct and CBR is incorrect along with the top-3 training examples retrieved from each retriever. EPR CBR Test Example Utterance confirmed thanks Meaning Representation (PleasantryAnythingElseCombined) Top-1 Utterance it’s ok bye Yes, but\n",
      "\n",
      "Chunk 3:\n",
      "asserted the theorem was proved, while we knew it was not, by looking at LeanDojo’s response. This demonstrates the value of theorem proving as a rigorous benchmark for addressing LLMs’ hallucination problem. Another key limitation of ChatGPT was its inability to search systematically in a large space. We frequently found it stuck to an unpromising path when the correct solution could be found by backtracking and exploring alternative paths. This behavior is consistent with the general observation that LLMs are weak at search and planning. Addressing this weakness is an active area of research [102]. We emphasize a few caveats about our study of theorem proving with ChatGPT. First, data con- tamination is likely. Many theorems we evaluated have been publicly available on GitHub before ChatGPT’s data cutoff date. Therefore, ChatGPT may have seen them in training. Second, our study is exploratory. A more detailed and quantitative study is needed to characterize ChatGPT’s capability in theorem proving. Such a study with ChatGPT plugins is challenging, as plugins currently only support interaction through the browser. Also, OpenAI has taken measures to block automated access by bots. Using humans may be an option, but that is beyond the scope of this paper. Figure E: (ChatGPT-3.5, 1/8) After receiving the theorem to prove, ChatGPT first called “initialize”, which returned the initial state. Then it tried to interpret the theorem in natural language. Note that it made a mistake here. The theorem was about natural numbers (N), not complex numbers (C). 13https://www.youtube.com/watch?v=CEwRMT0GpKo 31Figure F: (ChatGPT-3.5, 2/8) ChatGPT tried to rewrite the goal using the lemma “b + c = c + b”. This was a reasonable but incorrect move. After receiving the error message from Lean, ChatGPT explained the error in natural language. Here the explanation is quite accurate, which is impressive given that the error message looks opaque to anyone not familiar with Lean. F Limitations and Future Work Our work is one step toward unlocking the potential of LLMs for generating verifiable formal proofs, and we see abundant space for future exploration. A learning-based prover is a complex system consisting of multiple components: data extraction, interaction with proof assistants, model training, and proof search. While navigating the design space spanned by various components, we err on the side of simplicity and efficiency, instead of pushing performance to the limit. This helps us deliver a reliable, open, and accessible system, laying the foundation for further research. There are many directions in which the system can be improved, and we discuss a few of them here.14 Stronger LLMs. Our backbone model, ByT5 [44], was published in 2021 and has 299M parameters, which is not very large by today’s standard. Recently, there have been a plethora of open-source LLMs demonstrating strong capabilities in writing code, e.g., CodeGen [103], StarCoder [94], and CodeGeeX [104]. We are excited to see how they might impact theorem proving and, more generally, how far we can go by pushing the limit of the model/data scale. ByT5’s tokenizer-free nature helps us\n",
      "\n",
      "Chunk 4:\n",
      "for BM25 (cf. Figure 6), whereℓ=3 2is optimal. Figure 9: An analysis of perplexity as a function ofthe number of tokens in the queryfor an off-the-shelf BERT retriever on the development set of WikiText-103. Figure 10: An analysis of perplexity as a function of the number of tokens in the queryfor Contriever on the development set of WikiText-103. Model Retrieval Wiki-103 RealNews word ppl token ppl GPT-Neo 1.3B – 17.5 12.3 BM25, §5 14.6 9.9 GPT-Neo 2.7B – 15.1 11.0 BM25, §5 12.8 9.0 GPT-J 6B – 11.6 9.2 BM25, §5 10.0 7.7 Table 5: The performance of models from the GPT-Neo family, measured by word-level per- plexity on the test set of WikiText-103 and token-level perplexity on the development set of RealNews. B GPT-Neo Results Table 5 gives the results of applying In-Context RALM to the models from the GPT-Neo model family on WikiText-103 and RealNews. C Open-Domain Question Answering Experiments: Further Details Closed-Book Setting For the closed-book set- ting, we adopt the prompt of Touvron et al. (2023): Answer these questions: Q: Who got the first nobel prize in physics? A: Open-Book Setting For the open-book setting, we extend the above prompt as follows: Nobel Prize A group including 42 Swedish writers, artists, 1330 Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025 and literary critics protested against this decision, having expected Leo Tolstoy to be awarded. Some, including Burton Feldman, have criticised this prize because they... Nobel Prize in Physiology or Medicine In the last half century there has been an increasing tendency for scientists to work as teams, resulting in controversial exclusions. Alfred Nobel was born on 21 October 1833 in Stockholm, Sweden, into a family of engineers... Based on these texts, answer these questions: Q: Who got the first nobel prize in physics? A: 1331 Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025\n",
      "\n",
      "Chunk 5:\n",
      "is: April 19, 1954. Q: Which of the schools Fiser is affiliated with was founded first? A: The schools that Fiser is affiliated with (1) Academy of Music, University of Zagreb (2) Mozarteum University of Salzburg (3) Croatian Music Institute orchestra. Academy of Music, University of Zagreb was founded in the year 1829. Mozarteum University of Salzburg was founded in the year 1841. Croatian Music Institute was founded in the year 1827. Thus, the school founded earliest of these is Croatian Music Institute. So the answer is: Croatian Music Institute. Q: How many casualties were there at the battle that Dearing fought at under Jubal Early? A: Under Jubal Early, Dearing fought the First Battle of Bull Run. First Battle of Bull Run has 460 union casualties and 387 confederate casualties. Thus, in total the First Battle of Bull Run had 460 + 387 = 847 casualties. So the answer is: 847. Q: Which of the two congregations which provided leadership to the Pilgrims was founded first? A: The congregations which provided leadership to the Pilgrims are Brownists and Separatist Puritans. Brownist was founded in 1581. The Separatist Puritans was founded in 1640. Thus, Brownist was founded first. So the answer is: Brownist. Q: How long had the Rock and Roll Hall of Fame been open when the band was inducted into it? A: The band was inducted into Rock and Roll Hall of Fame in the year 2017. Rock and Roll Hall of Fame was established in the year of 1983. Thus, Rock and Roll Hall of Fame been open for 2018 − 1983 = 34 years when the band was inducted into it. So the answer is: 34. Q: Did the Lord Sewer who was appointed at the 1509 coronation live longer than his king? A: Lord Sewer who was appointed at the 1509 coronation was Robert Radcliffe, 1st Earl of Sussex. Lord Sever's king in 1509 was Henry VIII of England. Robert Radcliffe, 1st Earl of Sussex was born in the year 1483, and died in the year 1542. So Robert lived for 1542 − 1483 = 59 years. Henry VIII of England was born in the year 1491 and died in the year 1547. So Henry VIII lived for 1547 − 1491 = 56 years. Thus, Robert Radcliffe lived longer than Henry VIII. So the answer is: yes. Q: When was the place near where Manuchar was defeated by Qvarqvare established? A: Manuchar was defeated by Qvarqvare near Erzurum. Erzurum was founded during the Urartian period. So the answer is: Urartian period. Q: What year was the man who implemented the 46 calendar reform born? A: The man who implemented the 46 calendar reform is Julius Caesar. Julius Caesar was born in the year 100 BC. So the answer is: 100 BC. Q: How many years after the first recorded Tommy John surgery did Scott Baker undergo his? A: The first recorded Tommy John surgery happened when it was invented in the year 1974. Scott Baker\n",
      "\n",
      "Generated answers for provided queries have been saved to generated-provided-answers.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load provided queries\n",
    "with open(\"provided-queries.json\", \"r\") as f:\n",
    "    provided_queries = json.load(f)\n",
    "\n",
    "# Generate answers for provided queries\n",
    "for q in provided_queries:\n",
    "    q[\"generated_answer\"] = generate_answer(q[\"query\"])\n",
    "\n",
    "# Save provided results\n",
    "with open(\"generated-provided-answers.json\", \"w\") as f:\n",
    "    json.dump(provided_queries, f, indent=4)\n",
    "\n",
    "print(\"Generated answers for provided queries have been saved to generated-provided-answers.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc706bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "can lead to the correct predictions even though the intermediate reasoning doesn’t make any sense, indicating clear challenges for researchers exploring this direction. • Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example, an ALM connected to a python interpreter can run code and experiments on a user’s behalf, which a vanilla LM cannot do. In addition, a feedback loop can emerge between reasoning and acting, where each ability further improves the other (Yao et al., 2022b). Interacting with external tools, entities, and environments can improve reasoning since it allows the ALM to collect additional information and ground itself in the real-world. Similarly, reasoning can improve the ALM’s decision making abilities such as when and how to use a certain tool. Cost of using tools. To the best of our knowledge, the cost of using tools has not yet been taken into account comprehensively. Overall, using tools seem to be beneficial in terms of energy use. Retrieval-augmented language models are a good example of a more efficient way to handle new information since updating an external data store can be 21Published in Transactions on Machine Learning Research (06/2023) orders of magnitude cheaper than re-training a LLM for scratch every time we get new data (as argued for example in Borgeaud et al. (2022)). This stays true for using a calculator or browsing the web for example. One can therefore assume that relying on tools rather than storing knowledge in parameters is generally more energy-efficient than training, frequently updating a LLM, and scaling it if some ability remains out of reach, if one assumes an optimal tool usage (i.e., not using each tool at each token). Then, how to assess the cost of using a tool versus another? We believe that the most natural way of estimating such cost is to consider two factors: (i) the price per tool request, and (ii) the time required to fulfill the request. These factors make sense from the perspective of the ALM, and are a reasonable way to take into account the cost of creating and maintaining the tool since this should typically be reflected in the pricing. Interestingly, including such cost at training and inference time under the form of a loss term for example could help ALMs to develop optimal tool use, and lead to the emergence of models that know how to balance between tool use or internal chain of thoughts. Ethical concerns. ALMs raise new ethical concerns. LM predictions based on tools may look more trustworthy and authoritative at a first glance, when in fact many of them will still be incorrect. Moreover, we can expect this phenomenon to be amplified as LMs reason in quite a similar manner to humans (Dasgupta et al., 2022), making it even harder to detect mistakes. Hence, ALMs may be leveraged to generate more convincing fake news and conspiracy theories. Conversely, conditioning ALMs on “safe” and trusted\n",
      "\n",
      "Chunk 2:\n",
      "throughout the survey. • Reasoning. In the context of ALMs, reasoning is decomposing a potentially complex task into simpler subtasks the LM can solve more easily by itself or using tools. There exist various ways to decompose into subtasks, such as recursion or iteration. In that sense, reasoning is akin to planning as defined for example in LeCun (2022). In this survey, reasoning will very often refer to the various strategies to improve reasoning skills in LMs, such as step-by-step reasoning using few-shot examples. It is not yet fully understood whether the LM is really reasoning, or simply producing a larger context that increases the likelihood of correctly predicting the missing tokens. We refer to Huang and Chang (2022) for a discussion on this topic: although reasoning may currently be an abuse of language given the current state of the art, the term is already in use within the community. A more pragmatic definition of reasoning in the context of ALMs is giving more computation steps to the model before yielding the answer to a prompt. 3https://github.com/google/BIG-bench 2Published in Transactions on Machine Learning Research (06/2023) • Tool. For ALMs, a tool is an external module that is typically called using a rule or a special token and whose output is included in the ALM’s context. The tool can gather external information, or have an effect on the virtual or physical world (generally perceived by the ALM). An example of a tool fetching external information is a document retriever, while a tool having an external effect could be a robotic arm. A tool can be called at training or at inference time. More generally, learning to interact with a tool may consist in learning to call its API. • Act. For ALMs, calling a tool that modifies a state in a virtual or physical object, and observing the result, typically by including it in the ALM’s current context. For example, some works from the survey discuss searching the web, or robotic arm manipulation via LMs. With a slight abuse of term, we will sometimes denote the call of a tool by an ALM as an action, even if it does not have an external effect. Why jointly discussing reasoning and tools?The combination of reasoning and tools within LMs should allow solving a broad range of complex tasks without heuristics, hence with better generalization capabilities. Typically, reasoning would foster the LM to decompose a given problem into potentially simpler subtasks while tools would help getting each step right, for example obtaining the result from a mathematical operation. Put it differently, reasoning is a way for LMs to combine different tools in order to solve complex tasks, and tools are a way to not fail a reasoning with valid decomposition. Both should benefit from the other. Moreover, reasoning and tools can be put under the same hood, as both augment the context of the LM so that it better predicts the missing tokens, albeit in a different way. Why jointly discussing tools and\n",
      "\n",
      "Chunk 3:\n",
      "within another tool call, the number of tool uses can become exponential in the number of tools. For example, within a tool call, an ALM could browse the web by reading the content of a web page, 19Published in Transactions on Machine Learning Research (06/2023) which is text, then apply another of the tools to the text it found to refine its query, etc. If for some reason we require a depth that is equal to the number of tools, we have an exponential complexity. For these reasons, we refer to Augmented Language Models (ALMs) in this survey, to distinguish from Language Modeling in the traditional sense. A tradeoff between memorizing and querying tools.Is it preferable to memorize information in the model weights, or to leverage external tools? Some situations arguably require external tools, for example computing 213443344. However, many information are well known facts such as “The Eiffel tower is located in Paris” or1 + 2 = 3, and should not be offloaded. And, when learning world representations, memorization is not only desirable, but also deeply connected to reasoning (Hayes et al., 2014). Can ALMs be calibrated enough to decide when and when not to use a tool? Could a computation budget for each tool be integrated into the loss to let the model learn to do so? Generalizing the non-parametric framework. A motivation behind information retrieval augmented LMs such asRETRO (Borgeaud et al., 2022) andAtlas (Izacard et al., 2022) is to develop a class of LM requiring less parameters through relying on an external non-parametric memory. The motivation for using other kind of tools such ascode interpreter or calculator has been slightly different so far: for instance, Cobbe et al. (2021) use a calculator to improve accuracy on tasks requiring arithmetic. Yet, the paradigm of tool-augmented LMs can be seen as a generalization of the non-parametric framework. Indeed, beyond information retrieval, LMs can delegate any kind of abilities such as calculus to the corresponding external tools. By avoiding to store rarely accessed knowledge in their weights, tool-augmented LMs may have better scaling laws and thus yield smaller models retaining the capabilities of their largest counterpart. Combined with the possibility to access recent information from the external world thus avoiding frequent updates, non-parametric generalization holds great benefits for ALMs. A path towards autonomous machine intelligence?A concept for an autonomous intelligent agent was proposed by LeCun (2022). We now discuss to what extent ALMs instantiate this idea. In LeCun (2022), the agent is composed of different modules starting from a world model and a short-term memory. Essentially, the agent takes actions via an actor module based on its world model, perception module, and short-term memory so as to minimize some cost. The agent is also equipped with a configurator module for modulating the world model, the perception, the actor and the cost given the task at hand. Translating into this framework, the ALM’s weights essentially contain the world model, perception and actor modules. The short-term memory can be identified with\n",
      "\n",
      "Chunk 4:\n",
      "Published in Transactions on Machine Learning Research (06/2023) Augmented Language Models: a Survey Grégoire Mialon∗ gmialon@meta.com Roberto Dessì∗† rdessi@meta.com Maria Lomeli∗ marialomeli@meta.com Christoforos Nalmpantis∗ christoforos@meta.com Ram Pasunuru∗ rpasunuru@meta.com Roberta Raileanu∗ raileanu@meta.com Baptiste Rozière∗ broz@meta.com Timo Schick∗ schick@meta.com Jane Dwivedi-Yu∗ janeyu@meta.com Asli Celikyilmaz∗ aslic@meta.com Edouard Grave∗ egrave@meta.com Yann LeCun∗ yann@meta.com Thomas Scialom∗ tscialom@meta.com ∗Meta AI †Universitat Pompeu Fabra Reviewed on OpenReview:https://openreview.net/forum?id=jh7wH2AzKK Abstract This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues. 1 Introduction: motivation for the survey and definitions 1.1 Motivation and Definitions Large Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022) have fueled dramatic progress in Natural Language Processing (NLP) and are already core in several products with millions of users, such as the coding assistant Copilot (Chen et al., 2021), Google search engine1 or more recently ChatGPT2. Memorization (Tirumala et al., 2022) combined with compositionality (Zhou et al., 2022) capabilities made LLMs able to execute various tasks such as language understanding or conditional and 1See e.g. https://blog.google/products/search/search-language-understanding-bert/ 2https://openai.com/blog/chatgpt/ 1Published in Transactions on Machine Learning Research (06/2023) unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards higher-bandwidth human-computer interactions. However, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide non-factual but seemingly plausible predictions, often referred to as hallucinations (Welleck et al., 2020). This leads to many avoidable mistakes, for example in the context of arithmetics (Qian et al., 2022) or within a reasoning chain (Wei et al., 2022c). Moreover, many LLMs groundbreaking capabilities seem to emerge with size, measured by the number of trainable parameters: for example, Wei et al. (2022b) demonstrate that LLMs become able to perform some BIG-bench tasks3 via few-shot prompting once a certain scale is attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest counterpart (Hoffmann et al., 2022), the size and need for data of LLMs can be impractical for training but also maintenance: continual learning for large models remains an open research question (Scialom et al., 2022).\n",
      "\n",
      "Chunk 5:\n",
      "in natural language and interact with external tools. Along these lines, Wang et al. (2023) uses a LM as a centralized planner to generate goal sequences for solving tasks in the game of Minecraft. Through a feedback loop and intermediate checks on subgoals execution, the LM can explain mistakes of the goal executor and refine its original plan. However, we note that a LM-based controller might not be the only viable approach for a generalist agent. Recent work on the game of Diplomacy (Bakhtin et al., 2022), a long-standing challenge for AI agents due to its complex planning and reasoning dynamics, employs an ad-hoc planning model trained via self-play and reinforcement learning. Here the LM is used to interact with other players, thus as an external communication module grounded in the current state of the game. This offers an alternative view of LMs as agents specialized to communicate with humans, albeit in the restricted setting of a Diplomacy game. We believe that (A)LMs will play a central role in the next generation of powerful interactive systems, whether as centralized controller of a modular system or as a language-only module that needs to interact with an orchestrator remains an open research question. Augmented Language Models benefits.Overall, ALMs offer many potential advantages over traditional LMs. • Truthfulness: As the current LM’s training objective is arguably responsible for inciting the generation of seemingly plausible but not factual information, grounding the predictions through some tools should lead to more trustworthy models. However, although this conclusion is straightforward when equipping a LM with a calculator, there is surprisingly little evidence of it for information retrieval augmented LMs (Krishna et al., 2021). One of the reasons is the presence of a lot of non-truthful information in the web. Investigating this direction will be critical for making LM reliable. • Estimating and reducing uncertainty: Extending the maximum-likelihood paradigm by letting the model reason and access additional information could help models to learn what they know and what they don’t. Some papers suggest that LMs are already well calibrated (Kadavath et al., 2022), i.e. there is a high correlation between the accuracy of their predictions and the corresponding likelihood. This uncertainty could be directly exploited by ALMs to know when to rely on their own weights, or when to query an external tool. • Interpretability: Deep learning models are often considered to be black boxes, and their predictions are difficult to interpret. Providing intermediate reasoning steps and relying on tools should help to make ALMs more interpretable. In particular, we can expect that being able to cite the sources used to compose the answer to be critical. However, some works Lewkowycz et al. (2022) pointed out that chain-of-thoughts can lead to the correct predictions even though the intermediate reasoning doesn’t make any sense, indicating clear challenges for researchers exploring this direction. • Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example,\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "the other three datasets, which originally come in a reading comprehension or mixed setting, we used the associated contexts to construct a 10017corpus for our open-domain setting (see App. A for details). For each dataset, we use 100 randomly sampled questions from the original development set for tuning hyperparameters, and 500 other randomly sampled questions as our test set. 4.1 Models Retriever. We use BM25 (Robertson et al., 2009) implemented in Elasticsearch6 as our base retriever. We compare two retriever systems: (i) One-step Retriever (OneR)uses the ques- tion as a query to retrieve K paragraphs. We select K ∈{5,7,9,11,13,15}that’s best on the dev set. (ii) IRCoT Retriever is our method de- scribed in §3. We use BM25 as its underly- ing retriever and experiment with OpenAI GPT3 (code-davinci-002) (Brown et al., 2020; Ouyang et al., 2022; Chen et al., 2021) and Flan-T5 (Chung et al., 2022) of different sizes as its CoT generator. For demonstrating in-context examples to these LMs, we wrote CoTs for 20 questions for all the datasets (see App. §G). We then create 3 demon- stration (“training”) sets by sampling 15 questions each for each dataset. For each experiment, we search for the best hyperparameters for the dev set using the first demonstration set and evaluate each demonstration set on the test set using the selected hyperparameters. We report the mean and standard deviation of these 3 results for each experiment. At test time, we pack as many demonstrations as possible within the model’s context length limit. The context limit for GPT3 (code-davinci-002) is 8K word pieces. Flan-T5-* doesn’t have any hard limit as it uses relative position embeddings. But we limit Flan-T5’s context to 6K word pieces, which is the maximum we could fit in the memory of our 80G A100 GPUs. IRCoT Retriever has one key hyperparameter: K ∈{2,4,6,8}, the number of paragraphs to re- trieve at each step. Additionally, when creating “training” demonstrations for IRCoT’s Reasoner module, we use gold paragraphs and a smaller num- ber M ∈{1,2,3}of distractor paragraphs (§3.1). Retrieval Metric:We allow a maximum of 15 paragraphs for all retriever systems and measure the recall of the gold paragraphs among the re- trieved set of paragraphs. We search for the hyper- parameter K (and M for IRCoT) that maximizes the recall on the dev set and use it on the test set. 6https://www.elastic.co/ The reported metric can thus be viewed as thefixed- budget optimal recall for each system considered.7 QA Reader. To implement the reader, we use the same LMs as used in the reason-step of IRCoT Retriever. We found that QA readers im- plemented with Flan-T5-* perform better with the Direct Prompting strategy and GPT3 performs bet- ter with CoT Prompting strategy (see App. E). Hence we use Direct prompting strategy for QA with Flan-T5-* and CoT with GPT3 for the experi- ments.8 The QA reader has one hyperparameter M: the number of distractor paragraphs in the in-context demonstrations. We search for M in {1,2,3}. When used in conjunction with IRCoT\n",
      "\n",
      "Chunk 2:\n",
      "encode all candidate documents and store representations for each document. These two operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022). § Unless otherwise speciﬁed, we use the text-davinci-002 version of InstructGPT in our experiments. * Work done during internship at Microsoft Cognitive Service Research group. 1Published as a conference paper at ICLR 2023 In this paper, we propose to leverage large language models, such as InstructGPT (Ouyang et al., 2022), to directly generate contextual documents for a given question, instead of retrieving relevant documents from an external corpus, such as Wikipedia. Our approach has two main advantages. First, we show that generated contextual documents contain the correct answer more often than the top retrieved documents. We believe this is because large language models generate contextual documents by performing deep token-level cross-attention between all the question and document contents, resulting in generated documents that are more speciﬁc to the question than retrieved documents. Second, we show that our approach signiﬁcantly outperforms directly generating answers from large language models despite not incorporating any new external information. This is mainly because the task of generating document-level contexts is close to the objective of causal language modeling pre-training, so the world knowledge stored in the model parameters can be better utilized. We show, on multiple datasets, that generated documents are more likely to contain correct answers than the top retrieved documents. Notably, in dense retrieval methods, as more documents are retrieved, the recall of documents containing the correct answer increases (Karpukhin et al., 2020). However, the recall performance does not scale as well with generated documents because even with sampling methods, generated documents tend to contain duplicate information. In order to improve the recall performance of generated documents, we propose a novel clustering-based prompt method. We synthesize a prompt with in-context demonstrations of question-document pairs sampled from diverse clusters. These prompts result in generated documents that cover different perspectives of the question and improve the scaling of performance as more documents are generated per question. In contrast to the retrieve-then-read pipeline, our method is essentially a generate-then-read pipeline. Speciﬁcally, it ﬁrst prompts a large language model to generate contextual documents based on a given question, and then reads the generated document to produce the ﬁnal answer. The reader can still be a large model (e.g., InstructGPT (Ouyang et al., 2022)) used under a zero-shot setting, or a small one (e.g., FiD (Izacard & Grave, 2021)) ﬁne-tuned with generated documents on the training split of the target dataset. We evaluate our proposed method on three different knowledge-intensive tasks and demonstrate its effectiveness on both zero-shot and supervised settings. Overall, our main contributions can be summarized as follows: 1. We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks, i.e., replacing the process of retrieving documents from Wikipedia or searching for related documents on Google, by prompting a large language model to generate\n",
      "\n",
      "Chunk 3:\n",
      "Published as a conference paper at ICLR 2023 GENERATE RATHER THAN RETRIEVE : L ARGE LANGU - AGE MODELS ARE STRONG CONTEXT GENERATORS Wenhao Yu1∗, Dan Iter2, Shuohang Wang2, Yichong Xu2, Mingxuan Ju1, Soumya Sanyal3∗, Chenguang Zhu2, Michael Zeng2, Meng Jiang1 1University of Notre Dame 2Microsoft Cognitive Service Research 3University of Southern California 1wyu1@nd.edu; 2iterdan@microsoft.com ABSTRACT Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that ﬁrst retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GENREAD ), which ﬁrst prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the ﬁnal answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, signiﬁcantly outperforming the state-of-the-art retrieve-then- read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead. 1 I NTRODUCTION Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require access to a large amount of world or domain knowledge (Petroni et al., 2021). These tasks are even challenging for humans without access to an external knowledge source such as Wikipedia. A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read pipeline that ﬁrst retrieves a handful of relevant contextual documents from Wikipedia and then conditions the prediction of the answer on these documents along with the question (Karpukhin et al., 2020; Lewis et al., 2020; Izacard & Grave, 2021). Nevertheless, these methods mainly suffer from three drawbacks. First, candidate documents for retrieval are chunked (e.g., 100 words) and ﬁxed, so the retrieved documents might contain noisy information that is irrelevant to the question. Second, the representations of questions and documents are typically obtained independently in modern two-tower dense retrieval models (Karpukhin et al., 2020), leading to only shallow interactions captured between them (Khattab et al., 2021). Third, document retrieval over a large corpus requires the retriever model to ﬁrst encode all candidate documents and store representations for each document. These two operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022). § Unless otherwise speciﬁed, we use the\n",
      "\n",
      "Chunk 4:\n",
      "large (e.g., 100 billion scale), it is generally not feasible to use completely in-memory ANNS index for dense retrieval, where we need to de- velop multi-level index using a hybrid of memory index and disk index, e.g., DiskANN [250]a n d SPANN [34]. ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:30 W. Xin et al. 6 INTEGRATION WITH RERANKING Inacompleteretrievalsystem,itusuallyconsistsoffirst-stageretrieversandsubsequentrerankers. Inthissection,wefirstbrieflyintroducetheretrievalpipeline,andthenpresentseveralapproaches to optimizing the retrieval pipeline. 6.1 Retrieval Pipeline This part introduces the retrieval pipeline and PLM-based rerankers. 6.1.1 General Retrieval Pipeline. To start our discussion, we consider a simplified retrieval pipeline,consistingoftwomajorstages,namely,thefirst-stageretrievalandthereranking.Toob- tain the final ranked list, a certain number of possibly relevant documents (e.g., several hundreds to thousands) to a given query are retrieved from a corpus by a retriever, such as dense retriever or BM25. Then, the candidate texts are scored and reranked by a more capable relevance model (called reranker). Finally, top-ranked texts (e.g., several or tens) will be returned as the search re- sults to downstream tasks, such as question answering and dialog systems. Generally, first-stage retrievalandrerankingstageshavedifferentfocusesinaretrievalsystem[ 9,137,188].First-stage retrieval aims to efficiently recall relevant candidates from a large text corpus. As a result, it is practically infeasible to employ time-consuming models in first-stage retrieval. In contrast, the goal of reranking is to reorder the candidate results from the proceeding stages, where the num- ber of candidate texts is generally smaller (e.g., hundreds or thousands) and more complicated models can be used to implement the rerankers. Therefore, bi-encoder is often used for imple- menting the retriever, and cross-encoder is often used as the architecture of the reranker. Note that a retrieval pipeline usually contains multiple reranking stages by successively refining a re- ducedcandidatelistforproducingthefinalresults.Besides,theremayalsoexistmultiplefirst-stage retrieversinapracticalretrievalsystem,wheretheresultsfrommultipleretrieversareaggregated as the input of the rerankers. Interested readers can refer to References [9, 57, 137] for detailed discussions. 6.1.2 PLM-based Rerankers and Multi-stage Ranking.In this part, we first discuss the typical rerankermodelsbasedonPLMs,andthenintroducemulti-stagerankingmechanismthatinvolves PLM-based rerankers. Reranker models. To implement the reranker, a typical approach is to employ the cross- encoder as the ranking model, showing substantial improvements over the traditional meth- ods [188, 204, 284, 293]. Specifically, BERT is widely used to implement the cross-encoder for estimating the relevance score [188, 204], e.g., monoBERT [191]. As the input, a query and a can- didate text are concatenated as a sequence proceeded by the “[CLS]” token. After encoding the query-text sequence, the “[CLS]” embedding is adopted as the match representation between the queryandtext,whichwillbesubsequentlyfedintoaneuralnetworkforcomputingtherelevance scoreofthetextbeingrelevant(Equation( 4)).Therelevancescoreiscomputedforeachtextinde- pendently,andthefinalrankingoftextsisobtainedbysortingthemaccordingtorelevancescores. For training the rerankers, it usually formulates the ranking problem as a binary classification task [188] using the BCE loss as shown in Equation (5). To optimize the BCE loss, it also needs to generate negatives for learning, which can be randomly selected or sampled from the top results of the retriever [68, 325]. Furthermore, duoBERT [191] implements the BERT-based text ranking inapairwiseway,whereittakesasinputaqueryandapairoftextstocompare(withaconcatena- tion pattern “[CLS]query [SEP]text1 [SEP]text2”). The training objective is to reserve the partial order of semantic relevance for a given text pair, such that it can predict the relevance\n",
      "\n",
      "Chunk 5:\n",
      "larger term overlap. Existing solutions for zero-shot retrieval.To improve the performance of dense retrievers in the zero-shot setting, there are generally three important approaches explored in the litera- ture [220], including training data augmentation for target domain, term-matching capacity en- hancement, and model size scaling. Next, we introduce each approach in detail. •Augmenting the target training data. Considering the lack of labeled data from the target do- main,amajorsolutionistogeneratelarge-scalesyntheticdataforimprovingthetrainingofdense retrievers. After being trained on large-scale synthetic training data, the zero-shot capabilities of denseretrieverscanbeimprovedtosomeextent.Typically,adatagenerationmodelisemployedfor generating large-scale query-text pairs in the target domain [135,163,214,277]. Recently, Promp- tagator[52]leveragesalargelanguagemodelconsistingof137billionparameters,i.e.,FLAN[ 285], to generate large-scale query-text pairs by using only a small number of labeled examples. As an alternativeapproach,knowledgedistillationiscommonlyadoptedfortacklingthescarcityoftrain- ingdata,whichutilizesamorecapableteachermodeltoimprovethestudentmodel[ 37,259,277]in zero-shotretrievalscenarios.Besides,severalstudies[ 101,290]conductunsupervisedpretraining by leveraging large-scale positive and negative pairs with different data augmentation methods, e.g., ICT [120] and SimCSE [71]. •Enhancingtheterm-matchingcapability .Unlikesparseretrievers(e.g.,BM25),denseretrievers no longer rely on exact term matching for text retrieval, but instead learn semantic matching for capturing the text relevance. While, empirical studies show that the capability of exact term matchingisusefultoimprovezero-shotretrievalperformance[ 36,260],sincetermoverlappingisa ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:34 W. Xin et al. strongindicatorfortextrelevance.Thus,sparseretrieversareeasiertoadapttozero-shotsettings without training data. Inspired by this, several studies propose to enhance the lexical matching capacity of dense retrievers by leveraging sparse retrievers, such as the fusion of rankings [36]o r relevancescores[ 290]forbothsparseanddenseretrievers.Besides,wecanalsoemployknowledge distillation for improving dense retrievers [37], taking sparse retrievers as the teacher model. •Scaling up the model capacity.Recently, scaling law for language models has been widely ex- plored for raising the performance bar of various tasks. It has been shown useful to improve the zero-shot performance by increasing the model size of dense retrievers. As shown in Reference [187], based on a T5-based dense retriever trained on large-scale question-answer pairs, scaling up the model size with multi-stage training can significantly improve the zero-shot retrieval per- formance on the BEIR benchmark. Similarly, performance improvement has been observed when the model size is increased from 0.1 billion to 2.4 billion in ERNIE-Search [160]. Besides,Zhanetal.[ 310]examinehowdenseretrieversgeneralizetoout-of-domaintestqueries, calledextrapolationcapacity intheirpaper.Theyfindthatcross-encodercanextrapolatewell,but notbi-encoderandsparseretriever. Thispartcanbeextendedtoamoregeneraltopic low-resourced dense retrieval, and readers can find a comprehensive discussion in a recent survey [241]. 7.2 Improving the Robustness to Query Variations Besidestheout-of-distributionissueinzero-shotretrieval,denseretrievalmodelsareshownmore sensitive toquery variationsthan traditional lexical matching methods [38, 196, 333]. Generally, queryvariationsexistwidelyinrealsearchbehaviors,e.g.,unexpectedquerytyposduetospelling errors and diverse query formulations for the same information need. We next discuss the effect of query variations on retrieval performance and introduce several enhanced training methods. Effect of query variations on retrieval.There are increasing concerns on the robustness of dense retrieval models to query variations. These studies typically create different types of query variations and examine the performance of dense retrieval models under different query varia- tions. Zhuang et al. [333] present a study on the impact of query typos based on character op- erations (insertion, deletion, substitution, and swap), showing a significant performance drop for BERT-basedretrieverandreranker.Penhaetal.[ 196]aimtoexaminehowdifferenttypesofquery variations negatively affect the robustness of the retrieval pipeline. To generate query variations, theyconsiderfoursyntax-changingtypes(misspelling,naturality,ordering,andparaphrasing)and two semantics-changing types (gen./specialization, aspect change) for query transformations. Ex- perimental results show that retrieval\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "with Chain-of-Thought Reasoning Our proposed retriever method, IRCoT, can be instantiated from the following three ingredients: (i) a base retriever that can take a query and re- turn a given number of paragraphs from a corpus or knowledge source; (ii) a language model with zero/few-shot Chain-of-Thought (CoT) generation capabilities; and (iii) a small number of annotated questions with reasoning steps explaining how to arrive at the answer in natural language (chain of thoughts) and a set of paragraphs from the knowl- edge source that collectively support the reasoning chain and the answer. The overview of IRCoT is given in Fig. 2. We first gather a base set of paragraphs by retrievingK paragraphs using the questionQ as the query. Then, we interleave two steps ( reason and retrieve) iteratively until the termination criterion is met. The retrieval-guided reasoning step (“Rea- son”) generates the next CoT sentence using the question, the paragraphs collected thus far, and the CoT sentences generated thus far. The prompt template for the task looks as follows: Wikipedia Title: <Page Title> <Paragraph Text> ... Wikipedia Title: <Page Title> <Paragraph Text> Q: <Question> A: <CoT-Sent-1> ... <CoT-Sent-n> For in-context demonstrations, we use the com- plete CoT in the above format. For a test instance, 10016Who wrote the 1970 international hit song that Murray Head is most recognized for? The 1970 international hit song that Murray Head is most recognized for is \"Super Star\" \"Super Star\" was written by Andrew Lloyd W ebber and T im Rice. So the answer is: Andrew Lloyd W ebber and T im Rice. Retrieve (Q) → Retrieve (T1) → Retrieve (T2) → T1 ← Reason (Q, , ) T2 ← Reason (Q, + , T1) T3 ← Reason (Q, + + , T1+T2) T1 Q T2 T3 Stop + + IRCoT Interleaved Retrieval guided by Chain-of-Thought Reasoning Retrieve( ) Wikipedia T itle: Mack Rides Mack Rides GmbH & Co KG, also ... Q: In what country was Lost Gravity manufactured? A: The Lost Gravity was manufactured by Mack Rides. Mack Rides is a company from Germany . The answer is Germany . ... Wikipedia T itle: Murray Head Murray Seafield St George Head .. ... Wikipedia T itle: Most Beautifullest Hits The Most Beautifullest Hits is ... Q: Who wrote the 1970 international hit .. A: The 1970 international hit song that Murray Head is most recognized for is \"Super Star\". \"Super Star\" was written by . Andrew Lloyd W ebber and T im Rice. q Reason( , , ) Q T1 Q T1 q Figure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and vice-versa. We start by retrieving K documents using the question as they query and repeat two steps alternatingly until termination. (i) reason-step generates next CoT sentence based on the question, so far retrieved paragraphs, and CoT sentences. (ii) retrieve-step retrieves K more paragraphs based on the last CoT sentence. The process terminates when the generated CoT has “answer is” or the number\n",
      "\n",
      "Chunk 2:\n",
      "one-step question- based retrieval is insufficient for such tasks, and introduced IRCoT, which uses interleaved CoT rea- soning and retrieval steps that guide each other step-by-step. On four datasets, IRCoT significantly improves both retrieval and QA performance when compared to one-step retrieval, for both large and relatively smaller-scale LMs. Additionally, CoTs generated by IRCoT contain fewer factual errors. Limitations IRCoT relies on the base LM to have a zero or few-shot CoT-generation ability. While this is com- monly available in large LMs (over 100B), it’s not as common for small LMs (under 20B), which to some extent limits IRCoT adoptability. Given the recent surge of interest (Tay et al., 2023; Magis- ter et al., 2022; Ho et al., 2022), however, smaller LMs will likely increasingly acquire such ability, making IRCoT compatible with many more LMs. IRCoT also relies on the base LM to support long inputs as multiple retrieved paragraphs need to fit in the LM’s input, in addition to at least a few demonstrations of QA or CoT with para- graphs. This was supported by the models we used as code-davinci-002 (GPT3) allows 8K tokens and Flan-T5-* uses relative position embeddings making it as extensible as the GPU memory con- straints allow. Future work can explore strategies to rerank and select the retrieved paragraphs instead of passing all of them to the LM to alleviate the need for the LM to support long input. The performance gain of IRCoT retriever and QA (over OneR and ZeroR baselines) come with an additional computational cost. This is because IRCoT makes a separate call to an (L)LM for each sentence of CoT. Future work can focus on, for instance, dynamically deciding when to retrieve more information and when to perform additional reasoning with the current information. 10022Lastly, a portion of our experiments was carried out using a commercial LLM API from OpenAI (code-davinci-002). This model was deprecated by OpenAI after our submission making the repro- duction of these experiments challenging despite our best efforts, just like any other work using such APIs. The trends discussed in the paper ( IRCoT > OneR > NoR), we believe, would still hold. Additionally, all our experiments using Flan-T5-*, which exhibit similar trends as that of GPT3, will remain reproducible, thanks to its publicly avail- able model weights. Ethical Considerations Language models are known to hallucinate incor- rect and potentially biased information. This is especially problematic when the questions asked to it are of a sensitive nature. While retrieval- augmented approaches such as ours are expected to alleviate this issue to some extent by grounding generation in external text, this by no means solves the problem of generating biased or offensive state- ments. Appropriate care should thus be taken if deploying such systems in user-facing applications. All the datasets and models used in this work are publicly available with permissible licenses. HotpotQA has CC BY-SA 4.0 license 15, 2Wiki- MultihopQA has Apache-2.0 license16, MuSiQUe and IIRC have CC BY 4.0 license17, and Flan-T5-* models have Apache-2.0\n",
      "\n",
      "Chunk 3:\n",
      "68.0± 1.5 36.5± 1.2 49.9± 1.1 w/o reader 61.0± 0.7 70.4± 1.5 31.5± 0.6 48.4± 1.0 Table 6: Answer F1 of IRCoT QA with and without a separate reader for Flan-T5-XXL (top two rows) and GPT3 (bottom two rows). When the reader is not used, the answer is extracted from the CoT generated by IRCoT while doing the retrieval. Ablating the reader usually hurts the performance. G Prompts Our manually written chain-of-thought annotations for HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC are given in Listing 1, 2, 3 and 4 respec- tively. Our prompts for GPT3 CoT Prompting are the same as these, except they have Wikipipedia paragraphs on the top of the questions as shown in § 3.120. Our prompts for GPT3 Direct Prompt- ing are the same as that of CoT prompting, except have the answer after \"A:\" directly. Our prompts for Flan-T5-* are slightly different from that of GPT3. For CoT Prompting, we prefix the question 20We are not showing the paragraphs in the paper for brevity but they can be obtained from the released code. 10028line: \"Q: Answer the following question by reason- ing step-by-step. <actual-question>\". For Direct Prompting, we prefix the question line: \"Q: Answer the following question. <actual-question>\". We did this to follow Flan-T5-*’s training format and found it to help its CoT generation. 10029Listing 1: Chain-of-Thought annotations for HotpotQA. Q: Jeremy Theobald and Christopher Nolan share what profession? A: Jeremy Theobald is an actor and producer. Christopher Nolan is a director, producer, and screenwriter. Therefore, they both share the profession of being a producer. So the answer is: producer. Q: What film directed by Brian Patrick Butler was inspired by a film directed by F.W. Murnau? A: Brian Patrick Butler directed the film The Phantom Hour. The Phantom Hour was inspired by the films such as Nosferatu and The Cabinet of Dr. Caligari. Of these Nosferatu was directed by F.W. Murnau. So the answer is: The Phantom Hour. Q: How many episodes were in the South Korean television series in which Ryu Hye−young played Bo−ra? A: The South Korean television series in which Ryu Hye−young played Bo−ra is Reply 1988. The number of episodes Reply 1988 has is 20. So the answer is: 20. Q: Vertical Limit stars which actor who also played astronaut Alan Shepard in \"The Right Stuff\"? A: The actor who played astronaut Alan Shepard in \"The Right Stuff\" is Scott Glenn. The movie Vertical Limit also starred Scott Glenn. So the answer is: Scott Glenn. Q: What was the 2014 population of the city where Lake Wales Medical Center is located? A: Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was 15,140. So the answer is: 15,140. Q: Who was born first? Jan de Bont or Raoul Walsh? A: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the first. So the answer is: Raoul Walsh.\n",
      "\n",
      "Chunk 4:\n",
      "datasets. For Flan-T5-XXL, IRCoT im- proves our recall metric relative to one-step re- trieval, on HotpotQA by 7.9, on 2WikiMultihopQA by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2 points. For GPT3, this improvement is by 11.3, 22.6, 12.5, and 21.2 points, respectively. IRCoT QA outperforms NoR and OneR QA. Fig. 4 compares ODQA performance using NoR, OneR and IRCoT retriever made from Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL, IRCoT QA outperforms OneR QA on HotpotQA by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue by 5.0 and IIRC by 2.5 F1 points. For GPT3, the corresponding numbers (except for IIRC) are 7.1, 13.2, and 7.1 F1 points. For GPT3, IRCoT doesn’t improve the QA score on IIRC, despite signifi- cantly improved retrieval (21 points as shown in Fig. 3). This is likely because IIRC relevant knowl- edge may already be present in GPT3, as also ev- idenced by its NoR QA score being similar. For other datasets and model combinations, NoR QA is much worse than IRCoT QA, indicating the limits of the models’ parametric knowledge. IRCoT is effective in OOD setting.Since CoT may not always be easy to write for new datasets, we evaluate NoR, OneR, and IRCoT on generaliza- tion to new datasets, i.e. OOD setting. To do so, we use prompt demonstrations from one dataset to evaluate on another dataset.9 For all pairs of the datasets10 and for both Flan-T5-XXL and GPT3, we find the same trend as in the IID setting: IRCoT re- trieval outperforms OneR (Fig. 5), and IRCoT QA outperforms both OneR QA and NoR QA (Fig. 6). IRCoT generates CoT with fewer factual errors. To assess whether our approach also improves the factuality of generated CoTs, we manually anno- tated CoTs generated by NoR QA, OneR QA, and IRCoT QA using GPT3 for 40 randomly sampled questions from each of the four datasets. We con- sidered CoT to have a factual error if at least one 9We use the evaluation dataset’s corpus for retrieval. 10We skip IIRC in this exploration as the task is structured a bit differently and requires special handling (see App. B). 10019Figure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X →Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y .IRCoT outperforms OneR in such an OOD setting. Figure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X→Y indicates prompt demonstrations are from dataset X and evaluation is on dataset Y . IRCoTQA outperforms OneR QA and NoR QA in such OOD setting. Figure 7: Number of questions, out of 40, where CoT generated by GPT3 using different methods has at least 1 factual error. Factual errors: IRCoT < OneR < NoR. of the facts11 is not true.12 As Fig. 7 shows, NoR makes\n",
      "\n",
      "Chunk 5:\n",
      "(Fer- guson et al., 2020). Our experiments using OpenAI GPT3 (code-davinci-002) (Brown et al., 2020; Ouyang et al., 2022; Chen et al., 2021) demon- 2Interleaved Retrieval guided by Chain-of-Thought. strate that retrieval using IRCoT is substantially more effective than the baseline, one-step, question- based retrieval by 11-21 recall points under a fixed- budget optimal recall setup.3 When IRCoT is used in conjunction with a prompting-based reader, it also leads to substantial improvement (up to 15 F1 points) in downstream few-shot QA performance and reduces factual errors in generated CoT by up to 50%. Our approach also works on much smaller Flan-T5 models (11B, 3B, and 0.7B) show- ing similar trends. In particular, we find QA using Flan-T5-XL (3B) with IRCoT even outperforms the 58X larger GPT3 with a one-step question- based retrieval. Furthermore, these improvements also hold up in an out-of-distribution (OOD) setting where the demonstrations from one dataset are used when testing on another dataset. Lastly, we note that our QA scores exceed those reported by recent works on few-shot prompting for open-domain QA (ODQA) (Khot et al., 2023; Press et al., 2022; Yao et al., 2022), although a fair apples-to-apples com- parison with them isn’t possible (cf. Appendix C). In summary, our maincontribution is a novel re- trieval method, IRCoT, that leverages LMs’ chain- of-thought generation capabilities to guide retrieval and uses retrieval in turn to improve CoT reasoning. We demonstrate that IRCoT: 1. improves both retrieval and few-shot QA per- formance on several multi-step open-domain QA datasets, in both IID and OOD settings; 2. reduces factual errors in generated CoTs; and 3. improves performance with both large-scale (175B models) as well as smaller-scale mod- els (Flan-T5-*, ≤11B) without any training. 2 Related Work Prompting for Open-Domain QA.LLMs can learn various tasks by simply using a few exam- ples as prompts (Brown et al., 2020). They’ve also been shown to answer complex questions by producing step-by-step reasoning (chain-of- thoughts, or CoT) when prompted with a few or zero demonstrations (Wei et al., 2022; Kojima et al., 2022). Prompting has been applied to open-domain QA (Lazaridou et al., 2022; Sun et al., 2022; Yu et al., 2023) but its value in improving retrieval and QA for multi-step open-domain questions remains relatively underexplored. 3We explain later (in the Metric section and Footnote 7) the appropriateness of this metric in our setting as opposed to more mainstream information recall metrics. 10015Recently three approaches have been proposed for multi-step open-domain QA. SelfAsk (Press et al., 2022) prompts LLMs to decompose a ques- tion into subquestions and answers subquestions by a call to Google Search API. DecomP (Khot et al., 2023) is a general framework that decomposes a task and delegates sub-tasks to appropriate sub- models. They also decompose questions but dele- gate retrieval to a BM25-based retriever. Both of these approaches are not developed for CoT reason- ing, do not focus on the retrieval problem, and re- quire a single-hop QA model to answer the decom- posed questions. Recently proposed ReAct (Yao\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "validate intermediate steps. More generally, the next section of the survey focuses on the various tools that can be queried by LMs to increase the chance of outputting a correct answer. 3 Using Tools and Act A recent line of LM research allows the model to access knowledge that is not necessarily stored in its weights, such as a given piece of factual knowledge. More precisely, tasks such as exact computation or information retrieval for example can be offloaded to external modules such as apython interpreter or a search engine that are queried by the model which, in that respect, use tools. Additionally, we can say the LM performs an action when the tool has an effect on the external world. The possibility to easily include tools and actions in the form of special tokens is a convenient feature of language modeling coupled with transformers. 3.1 Calling another model In many cases, the tool can simply be another neural network or the LM itself. 8Published in Transactions on Machine Learning Research (06/2023) Iteration 0 Text: Brittney Reese (born September 9, 1986 in Gulfport, Mississippi) is an American long jumper. <LM> Plan: Remove incorrect information Edit: Brittney Reese (born September 9, 1986in Gulfport, Mississippi) is an American long jumper. </LM> Iteration 1 Text: Brittney Reese (born September 9, 1986) is an American long jumper. <LM> Plan: Add information about her career Edit: Brittney Reese (born September 9, 1986) is an American long jumper , who competed at the 2008 Summer Olympics, and is a 4-time World Champion . </LM> Iteration 2 Text: Brittney Reese (born September 9, 1986) is an American long jumper, who competed at the 2008 Summer Olympics, and is a 4-time World Champion. <LM> Plan: Add her birthplace Edit: Brittney Reese (born September 9, 1986 in Inglewood, California) is an American long jumper, who competed at the 2008 Summer Olympics, and is a 4-time World Champion. </LM> Figure 5: Iterative prompting example using PEER (Schick et al., 2022), a LM trained to produce a plan of action and edit to the input text at each step. This process can be repeated until the generated text requires no further updates.<LM> denotes the start of the LM’s output to the prompt, while</LM> denotes the end. Iterative LM calling. As an alternative to improving the LM’s context for better outputs after a single inference pass, an alternative and intuitive way to get better results from LMs consists of repeatedly calling the model to iteratively refine its output.Re3 (Yang et al., 2022c) exploits this idea to automatically generate stories of over two thousand words. More precisely,Re3 first generates a plan, setting, and characters by prompting GPT3 (Brown et al., 2020) with a premise. Then,Re3 iteratively injects information from both the plan and current story state into a newGPT3 prompt to generate new story passages. This work is improved upon in Yang et al. (2022b) with the use of a learned detailed outliner that iteratively expands the brief initial outline to any desired\n",
      "\n",
      "Chunk 2:\n",
      "throughout the survey. • Reasoning. In the context of ALMs, reasoning is decomposing a potentially complex task into simpler subtasks the LM can solve more easily by itself or using tools. There exist various ways to decompose into subtasks, such as recursion or iteration. In that sense, reasoning is akin to planning as defined for example in LeCun (2022). In this survey, reasoning will very often refer to the various strategies to improve reasoning skills in LMs, such as step-by-step reasoning using few-shot examples. It is not yet fully understood whether the LM is really reasoning, or simply producing a larger context that increases the likelihood of correctly predicting the missing tokens. We refer to Huang and Chang (2022) for a discussion on this topic: although reasoning may currently be an abuse of language given the current state of the art, the term is already in use within the community. A more pragmatic definition of reasoning in the context of ALMs is giving more computation steps to the model before yielding the answer to a prompt. 3https://github.com/google/BIG-bench 2Published in Transactions on Machine Learning Research (06/2023) • Tool. For ALMs, a tool is an external module that is typically called using a rule or a special token and whose output is included in the ALM’s context. The tool can gather external information, or have an effect on the virtual or physical world (generally perceived by the ALM). An example of a tool fetching external information is a document retriever, while a tool having an external effect could be a robotic arm. A tool can be called at training or at inference time. More generally, learning to interact with a tool may consist in learning to call its API. • Act. For ALMs, calling a tool that modifies a state in a virtual or physical object, and observing the result, typically by including it in the ALM’s current context. For example, some works from the survey discuss searching the web, or robotic arm manipulation via LMs. With a slight abuse of term, we will sometimes denote the call of a tool by an ALM as an action, even if it does not have an external effect. Why jointly discussing reasoning and tools?The combination of reasoning and tools within LMs should allow solving a broad range of complex tasks without heuristics, hence with better generalization capabilities. Typically, reasoning would foster the LM to decompose a given problem into potentially simpler subtasks while tools would help getting each step right, for example obtaining the result from a mathematical operation. Put it differently, reasoning is a way for LMs to combine different tools in order to solve complex tasks, and tools are a way to not fail a reasoning with valid decomposition. Both should benefit from the other. Moreover, reasoning and tools can be put under the same hood, as both augment the context of the LM so that it better predicts the missing tokens, albeit in a different way. Why jointly discussing tools and\n",
      "\n",
      "Chunk 3:\n",
      "irrelevant and has zero-valued attention weights. Given the surge of interest in long context LLM research and much more required computation at inference 2, it is still unclear for practitioners whether extending the context window of LLM 1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity search library (Johnson et al., 2019). 2For example, the price of GPT-4 with 32k context length is twice the 8k context model. 1Published as a conference paper at ICLR 2024 provides higher accuracy than the retrieval-augmentation for downstream tasks with informative queries. Moreover, it would be compelling if we could combine the strength of both methods and achieve even higher accuracies. In this work, we attempt to answer the above questions through a comprehensive study. Specifically, we make the following contributions: 1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre- trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks, including single and multi document question answering (QA), query-based summarization, and in context few-shot learning tasks. 2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation. 3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K) can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6), outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score. It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while can be much faster at generation (e.g., 4× faster on NarrativeQA). We organize the rest of the paper as follows. We discuss related work in Section 2, and present the experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5. 2 R ELATED WORK In this section, we discuss the related work in long context LLM, efficient attention methods, and retrieval-augmented language models. 2.1 P ARALLEL WORK When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b), and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model, i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their study are relatively small, thus they have limited zero-shot capability of incorporating context through retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context model\n",
      "\n",
      "Chunk 4:\n",
      "in natural language and interact with external tools. Along these lines, Wang et al. (2023) uses a LM as a centralized planner to generate goal sequences for solving tasks in the game of Minecraft. Through a feedback loop and intermediate checks on subgoals execution, the LM can explain mistakes of the goal executor and refine its original plan. However, we note that a LM-based controller might not be the only viable approach for a generalist agent. Recent work on the game of Diplomacy (Bakhtin et al., 2022), a long-standing challenge for AI agents due to its complex planning and reasoning dynamics, employs an ad-hoc planning model trained via self-play and reinforcement learning. Here the LM is used to interact with other players, thus as an external communication module grounded in the current state of the game. This offers an alternative view of LMs as agents specialized to communicate with humans, albeit in the restricted setting of a Diplomacy game. We believe that (A)LMs will play a central role in the next generation of powerful interactive systems, whether as centralized controller of a modular system or as a language-only module that needs to interact with an orchestrator remains an open research question. Augmented Language Models benefits.Overall, ALMs offer many potential advantages over traditional LMs. • Truthfulness: As the current LM’s training objective is arguably responsible for inciting the generation of seemingly plausible but not factual information, grounding the predictions through some tools should lead to more trustworthy models. However, although this conclusion is straightforward when equipping a LM with a calculator, there is surprisingly little evidence of it for information retrieval augmented LMs (Krishna et al., 2021). One of the reasons is the presence of a lot of non-truthful information in the web. Investigating this direction will be critical for making LM reliable. • Estimating and reducing uncertainty: Extending the maximum-likelihood paradigm by letting the model reason and access additional information could help models to learn what they know and what they don’t. Some papers suggest that LMs are already well calibrated (Kadavath et al., 2022), i.e. there is a high correlation between the accuracy of their predictions and the corresponding likelihood. This uncertainty could be directly exploited by ALMs to know when to rely on their own weights, or when to query an external tool. • Interpretability: Deep learning models are often considered to be black boxes, and their predictions are difficult to interpret. Providing intermediate reasoning steps and relying on tools should help to make ALMs more interpretable. In particular, we can expect that being able to cite the sources used to compose the answer to be critical. However, some works Lewkowycz et al. (2022) pointed out that chain-of-thoughts can lead to the correct predictions even though the intermediate reasoning doesn’t make any sense, indicating clear challenges for researchers exploring this direction. • Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example,\n",
      "\n",
      "Chunk 5:\n",
      "Published in Transactions on Machine Learning Research (06/2023) Augmented Language Models: a Survey Grégoire Mialon∗ gmialon@meta.com Roberto Dessì∗† rdessi@meta.com Maria Lomeli∗ marialomeli@meta.com Christoforos Nalmpantis∗ christoforos@meta.com Ram Pasunuru∗ rpasunuru@meta.com Roberta Raileanu∗ raileanu@meta.com Baptiste Rozière∗ broz@meta.com Timo Schick∗ schick@meta.com Jane Dwivedi-Yu∗ janeyu@meta.com Asli Celikyilmaz∗ aslic@meta.com Edouard Grave∗ egrave@meta.com Yann LeCun∗ yann@meta.com Thomas Scialom∗ tscialom@meta.com ∗Meta AI †Universitat Pompeu Fabra Reviewed on OpenReview:https://openreview.net/forum?id=jh7wH2AzKK Abstract This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues. 1 Introduction: motivation for the survey and definitions 1.1 Motivation and Definitions Large Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022) have fueled dramatic progress in Natural Language Processing (NLP) and are already core in several products with millions of users, such as the coding assistant Copilot (Chen et al., 2021), Google search engine1 or more recently ChatGPT2. Memorization (Tirumala et al., 2022) combined with compositionality (Zhou et al., 2022) capabilities made LLMs able to execute various tasks such as language understanding or conditional and 1See e.g. https://blog.google/products/search/search-language-understanding-bert/ 2https://openai.com/blog/chatgpt/ 1Published in Transactions on Machine Learning Research (06/2023) unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards higher-bandwidth human-computer interactions. However, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide non-factual but seemingly plausible predictions, often referred to as hallucinations (Welleck et al., 2020). This leads to many avoidable mistakes, for example in the context of arithmetics (Qian et al., 2022) or within a reasoning chain (Wei et al., 2022c). Moreover, many LLMs groundbreaking capabilities seem to emerge with size, measured by the number of trainable parameters: for example, Wei et al. (2022b) demonstrate that LLMs become able to perform some BIG-bench tasks3 via few-shot prompting once a certain scale is attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest counterpart (Hoffmann et al., 2022), the size and need for data of LLMs can be impractical for training but also maintenance: continual learning for large models remains an open research question (Scialom et al., 2022).\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "8 6 8 4 Ret. for exemplars ✓ ✗ ✗ ✗ Table 7: Dataset statistics and experimental settings of different tasks. wikipedia.org, wikiwand.com, wiki2.org, wikimedia.org Table 8: Wikipedia-related domains excluded from Bing’s search results. Dataset θ β Query formulation Combine single- & multi-time retrieval 2WikiMultihopQA 0.8 0.4 implicit ✗ StrategyQA 0.4 0.4 implicit ✗ ASQA & ASQA-hint 0.8 0.4 explicit ✓ WikiAsp 0.8 0.4 explicit ✓ Table 9: Hyperparameters of FLARE on different datasets. 7984Prompt D.3: retrieval instructions for 2WikiMultihopQA Skill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you want to look up. For example: Question: But what are the risks during production of nanomaterials? Answer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung damage. Question: The colors on the flag of Ghana have the following meanings. Answer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for mineral wealth. Question: Metformin is the first-line drug for what? Answer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity. Skill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu- sion. For example: Question: When did the director of film Hypocrite (Film) die? Answer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19 June 2013. Question: Are both Kurram Garhi and Trojkrsti located in the same country? Answer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus, they are not in the same country. So the answer is no. Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same nationality? Answer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the answer is no. Question: Who is Boraqchin (Wife Of Ögedei)’s father-in-law? Answer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is Genghis Khan. So the answer is Genghis Khan. Question: Who was born first out of Martin Hodge and Ivania Martinich? Answer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born first. So the answer is Martin Hodge. Question: When did the director of film Laughter In Hell die? Answer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is August 25, 1963. Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two? Answer (with step-by-step): The film Twenty Plus Two\n",
      "\n",
      "Chunk 2:\n",
      "LM recursion of Levine et al. (2022), though unlike their approach it does not require a large training set or updating any (prompt- tuning) weights. One such implementation is illustrated in openqa_predict below. 1 def openqa_predict (x): 2 preds = generate ( template_qa , n =20) (x). answers 3 x. choices = most_common ( preds , k =4) 4 5 queries = [f\"{x. question } {c}\" 6 for c in x. choices ] 7 8 x. passages = fused_retrieval ( queries ) 9 x. answer = generate ( TemplateMCQ )(x). answer 10 return x As an alternative comparison approach, we can invoke the RM via rank to ﬁnd the prediction that is most grounded in a retrieved contexts (i.e., most similar to the concatenation of the retrieved passages) or, given an RM that can score completions (Krishna et al., 2022), simply the prediction that has the highest score given the prompt. Aggregating Information When only a few demonstra- tions or passages are selected, we can simply concate- nate them all into the prompt. For instance, GPT-3.5 text-davinci-002 has a context window of 4097 tokens, which we ﬁnd to be reasonably large for accommodating several (e.g., 3–5) demonstrations, which individually in- clude their own passages and rationales. To deal with a larger number of demonstrations or passages, we can branch in parallel to process individual subsets of the passages or demonstrations and then aggregate the individual answers using one of the scoring methods pre- sented earlier. Indeed, Lewis et al. (2020) and Lazaridou et al. (2022) have explored marginalization as a way to com- bine scores across passages and Le et al. (2022) ensemble prompts across demonstrations, which can be expressed in this way. An alternative aggregation strategy is to accumulate informa- tion across passages sequentially, rather than independently. This is effectively how our multi-hop approach works (§2.4). Such a strategy has also been employed recently by Gao et al. (2022) for retroactively attributing text generated by LMs to citations. They generate many queries but instead of fusion (§2.4), they run their pipeline on each query and use its outputs to alter the input to subsequent queries.1 3. Evaluation We now consider how to implement DSP programs for three diverse knowledge-intensive NLP tasks: open-domain ques- tion answering (QA), multi-hop QA, and conversational QA. All of these tasks are “open-domain”, in the sense that sys- tems are given a short question or participate in a multi-turn conversation without being granted access to context that answers these questions. We build and evaluate intuitive compositions of the func- tions explored in §2 for each task. We show that, despite low development effort, the resulting DSP programs exhibit strong quality and deliver considerable empirical gains over vanilla in-context learning and a standard retrieve-then-read pipeline with in-context learning. 3.1. Evaluation Methodology In this report, we consider one development datasetfor each of the tasks we consider, namely, the open-domain version of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the multi-hop HotPotQA (Yang et al., 2018)\n",
      "\n",
      "Chunk 3:\n",
      "four key aspects for building a capable retrieval system, which are detailed as follows: —Architecture(Section3).Itisimportanttodesignsuitablenetworkarchitecturesforbuilding the relevance model. Currently, a number of general-purpose PLMs have been developed, and we need to adapt existing PLMs to text retrieval according to the task requirement. Training (Section 4). Different from traditional retrieval models, it is more difficult to train PLM-basedretrievers,consistingofahugenumberofparameters.Weneedtodevelopeffec- tive training approaches to achieving desirable retrieval performance. —Indexing(Section5).Traditionally,invertedindexcanefficientlysupportthesparseretrieval models. However, dense vectors do not explicitly correspond to lexical terms. We need to design suitable indexing mechanism for dense retrieval. —Integration (Section 6). As mentioned before, retrieval systems are usually implemented via a pipeline consisting of retrievers and rerankers. It is necessary to study how to integrate and optimize the dense retriever in a whole retrieval pipeline. 3 ARCHITECTURE This section describes the major network architectures for dense text retrieval. We start with the backgroundofTransformerandPLMs,thenintroducetwotypicalneuralarchitecturesforbuilding dense retrieval models, and finally compare sparse and dense retrieval models. 3.1 Background 3.1.1 Transformer and Pretrained Language Models.Transformer [268] has become the main- stream backbone for language model pretraining, which was originally proposed for efficiently modeling sequence data. Different from traditional sequence neural networks (i.e., RNN and its variants[40,88]),Transformernolongerprocessesthedatainorder,butinsteadintroducesanew self-attentionmechanism:a token attends to all thepositionsfrom the input.Suchan architecture isparticularlysuitedtobeparallelizedwiththehardwaresupport,makingitfeasibletotrainvery large neural networks. Based on Transformer and its variants, PLM are proposed [21, 53, 152]i n thefieldofnaturallanguageprocessing.PLMsarepretrainedoveralarge-scalegeneraltextcorpus with specially designed self-supervised loss functions, and can be further fine-tuned according to downstreamtasks,called pretrainingthenfine-tuningparadigm [53,148],whichisoneofthemost striking achievements on language intelligence in recent years. As one of the most representative PLMs, BERT [53] pretrains deep bidirectional architectures for learning general text representa- tions(withthetrickofwordmasking),whichhaslargelyraisedtheperformancebarinavarietyof naturallanguageprocessingtasks.Withexcellentrepresentationcapacities,PLMsalsoshedlights on developing more effective retrieval models. 3.1.2 PLMsforSparseRetrieval. BeforeintroducingtheuseofPLMsfordenseretrieval,thispart brieflyreviewshowtoutilizePLMsforimprovingsparseretrievalmodelsthatrelyonlexicalmatch. Basically speaking, this line of research work can be roughly divided into two major categories, namely, term weighting and term expansion. The first category of work aims to improve term weighting based on per-token contextualized representations. As a representative study, DeepCT [51] utilizes the learned BERT token repre- sentations for estimating context-specific importance of an occurring term in each passage. The basic idea is to regress the token representations into real-valued term weights. HDCT [50]e x - tendsittolongdocumentsbysplittingthedocumentsintopassages,andaggregatestheestimated term weights in each passage. Specifically, it leverages three kinds of data resources (i.e., titles, ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:7 Table 1. Detailed ListofDifferent Dense RetrievalMethodsinthe LiteraturewithDetailed Configurations Years Methods PLMs Arch. Training Other TricksLoss Negative Data Aug. Pretrain 2020 Poly-encoders [ 98]B E R T base MR CE TAP Context-candidate attention 2019 ICT [ 120]B E R T base SR IOC TAP Joint training 2020 ICT+BFS+WLP [ 28]B E R T base SR IOC TAP 2020 REALM [ 79]B E R T base SR IOC RAP Joint training + Async. index 2020 DPR [ 111]B E R T base SR NLL IB+SHN ALD 2020 ColBERT [ 113]B E R T base MR CE Multi-core pre-processing 2020 ME-BERT [ 161]B E R T large MR CE IB+DyHN Sparse-dense hybrid\n",
      "\n",
      "Chunk 4:\n",
      "F1 in Table 1. On HotPotQA, the task-aware DSP program outperforms the baselines and existing work by very wide margins, exceed- ing the vanilla LM, the retrieve-then-read baseline, and the self-ask pipeline by 82%, 39%, and 80%, respectively, in EM. This highlights the effectiveness of building up more sophisticated programs that coordinate the LM and RM for the SEARCH step. These results may be pegged against the evaluation on Hot- PotQA in a number of concurrent papers. We ﬁrst compare with non-retrieval approaches, though our comparisons must be tentative due to variation in evaluation methodologies. Si et al. (2022) achieve 25.2% EM with CoT prompting. With a “recite-and-answer” technique for PaLM-62B (Chowdh- ery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang et al. (2022b) achieve 33.8% EM and 44.6 F1 when apply- ing a self-consistency prompt for PaLM-540B. Next, we compare with a contemporaneous retrieval-based approach: Yao et al. (2022) achieve 35.1% EM using a system capable of searching using a Wikipedia API. All of these approaches trail our task-aware DSP program, which achieves 51.4% EM, by large margins. QReCC We use QReCC (Anantha et al., 2020) in an open- domain setting over Wikipedia 2018. QReCC does not have an ofﬁcial development set, so we sub-divide the training set into 90%/10% train/validation splits. For the ﬁrst ques- tion in every conversation, we use the rewritten question as the original question often assumes access to a ground- truth document. We also ﬁlter low-quality examples from QReCC.3 We conduct the QReCC conversations in an auto-regressive manner. At turn t > 1 of a particular conversation, the system sees its own responses (i.e., not the ground truth responses) to previous turns of the conversation. We report the novel-F1 metric (nF1; Paranjape et al. 2022), which computes the F1 overlap between the system response and the ground truth while discounting common stopwords and terms present in the question (or earlier questions). The results are shown in Table 1, and follow the same general pattern as SQuAD and HotPotQA. 3We remove conversations that have one or more empty ground- truth answers and conversations that have only one or two ques- tions. We also ﬁnd many conversations that include “what other interesting facts are in this article?”, which conﬂict with the open- domain formulation and have no well-deﬁned answer. Hence, we remove any conversation that includes the keywords “other inter- esting” or “else”, which we found to be markers of low quality.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models 4. Conclusion For a long time, the dominant paradigm for building models in AI has centered around multiplication of tensor repre- sentations, and in the deep learning era this has given rise to highly modular (layer-wise) designs that allow for fast development and wide exploration. However, these design paradigms require extensive domain expertise, and even experts face substantial challenges when it comes to com- bining different pretrained components into larger systems. The promise of in-context learning is that we can build com- plex\n",
      "\n",
      "Chunk 5:\n",
      "ranks the results for prompt construc- tion. CodeS [254] retrieves relevant information from table databases in a coarse-to-fine manner to generate SQL. 6) Others: There are several other code-related tasks that adopt query-based RAG paradigm, incorporating similar ex- amples to construct prompts. Jie et al. [255] used programs as the intermediate step in numerical reasoning. De-fine [256] uses programs to solve complex tasks. It refines the answer generated by query-based RAG, then adds the refined pro- grams back to the retrieval source. For program static analysis, E&V [257] leverages an LLM agent to form intermediate results with AST-based source code retrieval, pseudo-code ex- ecution, execution specifications verification, and other tools. Code4UIE [258] performs information extraction through code representation. StackSpotAI [259] builds an AI coding assis- tant with an RAG component. InputBlaster [260] generates unusual text input that could cause mobile app crash. C. RAG for Knowledge Structured knowledge, including KGs (Knowledge Graph) and tables, is widely used in language-related tasks. It usually serves as the retrieval source to augment generation. In addi- tion to regular sparse and dense retrieval, NER (Named-Entity Recognition) technique and graph-aware neighbor retrieval are applied to identify and extract relevant entities and relations. 1) Knowledge Base Question Answering : KBQA (knowl- edge base question answering) typically utilizes a knowledge base to determine the correct answer to a question. Many semantic parsing methods have been proposed, generating logical forms (e.g. SPARQL) based on the question. Query-based RAG is the mainstream approach. Unseen Entity Handling [53] uses FreeBase [261] to retrieve topic entities, which are combined with query to generate SPARQL output. CBR-KBQA [54] combines the query and the retrieved (query, logical form) pairs for generation. It also revises the final result to align with the relations present in the knowledge graph. GMT-KBQA [52] re-ranks the retrieved entities and relations, and conducts relation classification and entity disam- biguation before generation. RNG-KBQA [82], TIARA [83], BLLM augmentation [262], and Shu et al. [263] re-rank the candidate logical forms or entities from the knowledge graph for prompt construction. Uni-Parser [92] includes entities from mention detection, 2-hop paths extraction, and tables from databases into generator input. ECBRF [93] follows the case- based reasoning paradigm [264], retrieving similar triplet to build prompt input. FC-KBQA [265] extracts relevant classes, relations, and entities from BM25 or mention detection, Struct- GPT [266] extracts relevant triplets and nearest entities, and KAPING [267] extracts relevant facts through entity match- ing. Sen et al. [268] replaced the retrieval with a relation distribution generation model for weighted triplets. Retrieve- Rewrite-Answer [269] retrieves subgraphs into prompts using hop prediction, relation path prediction, and triplet sampling. Keqing [270] decomposes a complex question into simple sub-questions through LLM, then retrieves sub-question tem- plates and extract candidate entities from knowledge graph, and finally generates the answer through ChatGPT. Liu et al. [271] leveraged retrieved pairs to explore the capability of formal language understanding and generation. Interactive- KBQA [272] employs the LLM as an agent, which conducts entity-linking on KG and generates current thought\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "and metrics, as well as the current evaluation benchmarks and tools. Additionally, we anticipate future directions for RAG, emphasizing potential enhancements to tackle current challenges. The paper unfolds as follows: Section II introduces the main concept and current paradigms of RAG. The following three sections explore core components—“Retrieval”, “Gen- eration” and “Augmentation”, respectively. Section III focuses on optimization methods in retrieval,including indexing, query and embedding optimization. Section IV concentrates on post- retrieval process and LLM fine-tuning in generation. Section V analyzes the three augmentation processes. Section VI focuses on RAG’s downstream tasks and evaluation system. Sec- tion VII mainly discusses the challenges that RAG currently faces and its future development directions. At last, the paper concludes in Section VIII. II. O VERVIEW OF RAG A typical application of RAG is illustrated in Figure 2. Here, a user poses a question to ChatGPT about a recent, widely discussed news. Given ChatGPT’s reliance on pre- training data, it initially lacks the capacity to provide up- dates on recent developments. RAG bridges this information gap by sourcing and incorporating knowledge from external databases. In this case, it gathers relevant news articles related to the user’s query. These articles, combined with the original question, form a comprehensive prompt that empowers LLMs to generate a well-informed answer. The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3. Despite RAG method are cost-effective and surpass the performance of the native LLM, they also exhibit several limitations. The development of Advanced RAG and Modular RAG is a response to these specific shortcomings in Naive RAG. A. Naive RAG The Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the3 Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks, encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3) Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer. widespread adoption of ChatGPT. The Naive RAG follows a traditional process that includes indexing, retrieval, and generation, which is also characterized as a “Retrieve-Read” framework [7]. Indexing starts with the cleaning and extraction of raw data in diverse formats like PDF, HTML, Word, and Markdown, which is then converted into a uniform plain text format. To accommodate the context limitations of language models, text is segmented into smaller, digestible chunks. Chunks are then encoded into vector representations using an embedding model and stored in vector database. This step is crucial for enabling efficient similarity searches in the subsequent retrieval phase. Retrieval. Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector\n",
      "\n",
      "Chunk 2:\n",
      "with those presented in the local neighborhood of the query entity in knowledge graph. 5) RAG Pipeline Enhancement : RAG pipeline enhance- ment refers to optimizing the overall process of RAG in order to achieve better performance results. Adaptive Retrieval: Some studies on RAG suggest that re- trieval doesn’t always enhance the final results. Over-retrieval can lead to resource wastage and potential confusion when the model’s inherent parameterized knowledge suffices for answering relevant questions. Consequently, this chapter will delve into two methods for determining retrieval necessity: rule-based and model-based approaches. Rule-based: FLARE [178] actively decides whether and when to search through the probability in the generation pro- cess. Efficient-KNNLM [38] combines the generation proba- bility of KNN-LM [37] and NPM [119] with a hyperparameter λ to determine the proportion of generation and retrieval. Mallen et al. [179] used statistical analysis on questions to enable direct answers for high-frequency ones and applied RAG for low-frequency ones. Jiang et al. [180] evaluated model confidence based on Model Uncertainty, Input Uncer- tainty, and Input Statistics to guide retrieval decisions. Kandpal et al. [181] studied the correlation between the number of relevant documents and the model’s knowledge mastery to assess the need for retrieval. Model-based: Self-RAG [85] uses a trained generator to determine whether to perform a retrieval based on the retrieve token under different user querys. Ren et al. [182] used “Judg- ment Prompting” to determine whether LLMs can answer relevant questions and whether their answers are correct or not, thereby assisting in determining the necessity of a retrieval. SKR [183] uses the ability of LLMs themselves to judge in advance whether they can answer the question, and if they can answer, no retrieval is performed. Rowen [184] translates a question into multiple languages and checks for answer consistency across these languages, using the results to deter- mine the need for information retrieval. AdaptiveRAG [185] dynamically decides whether to retrieve based on the query complexity by a classifier, which is a smaller LM. Iterative RAG: Iterative RAG progressively refines results by repeatedly cycling through retrieval and generation phases, rather than a single round. RepoCoder [186] uses an iterative retrieval-generation ap- proach for code completion, refining queries with previously9 TABLE I: Taxonomy of RAG applications across various modalities. RAG for Text Question Answering Human-Machine ConversationNeural Machine Translation Summarization Others REALM‡§ TKEGEN§ RIAG‡ Fid‡§ RETRO§ NPM‡§ SKR§¶ Self-RAG§¶ TOG‡ ConceptFlow‡§ Skeleton-to-Response‡§ CREA-ICL†‡ Internet-Augmented-DG‡§ BlenderBot3‡§ CEG‡∥ NMT-with-Monolingual-TM†‡§ KNN-MT‡§ COG‡ TRIME‡§ RAMKG‡§ Unlimiformer§ RPRR‡ RIGHT‡§ CONCRETE‡§ Atlas‡§ KG-BART‡§ R-GQA‡§ RAG for Code Code GenerationCode Summary Code CompletionAutomatic Program RepairText-to-SQL andCode-based Semantic ParsingOthers SKCODER§ RRGCode‡ ARKS†¶ KNN-TRANX∥ RECODEToolcoder§∥ RACE† BASHEXPLAINER‡ READSUM∥ Rencos‡ CoRec‡ Tram§ EDITSUM‡ ReACC†‡ RepoCoder†§¶ De-Hallucinator¶ REPOFUSE§ RepoFusion§ EDITAS§ RING∥ CEDAR§ RAP-Gen‡§ InferFix§ SARGAM§ RTLFixer‡§ XRICL‡§ SYNCHROMESH‡§ RESDSQL§ REFSQL‡§ CodeICL§ MURRE∥¶ StackSpotAI‡§ E&V Code4UIE§ De-fine‡∥ ImputBlaster¶ RAG for Knowledge Knowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others CBR-KBQA ‡§∥ TIARA †‡§ Keqing †‡§ RNG-KBQA ‡∥ ReTraCk § SKP †‡§ UniK-QA †‡ KG-FiD ‡ GRAPE ‡ SKURG †‡ KnowledGPT ‡ EFSUM § EfficientQA ‡ CORE\n",
      "\n",
      "Chunk 3:\n",
      "and interpretability. RAG could be employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs. With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against misleading content and automated spam/phishing. Acknowledgments The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. Funding Disclosure EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program. This work was funded by Facebook. References [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http: //arxiv.org/abs/1611.09268. arXiv: 1611.09268. [2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222–228. Springer, 2015. URLhttps://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20 . [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. InProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URLhttp://www.aclweb.org/anthology/ D13-1160. [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod- ing&autoregressive language model for context-conditioned generation.ArXiv, abs/2004.07159, 2020. URLhttps://arxiv.org/abs/2004.07159. [5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171. [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-ﬁne question answering for long documents. InProceedings of the 1055th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020. [7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre- hension. arXiv:1710.10723 [cs], October 2017.\n",
      "\n",
      "Chunk 4:\n",
      "a selector to reduce redundant programs and obtain diverse retrieved programs. XRICL [153] uses a distillation-based exemplar reranker after retrieval. Rangan [154] employs the Quantized Influence Measure, as- sessing statistical biases between a query and a reference to evaluate the similarity of data subsets and rerank retrieval re- sults. UDAPDR [155] uses LLMs to cost-effectively generate synthetic queries that train domain-specific rerankers, which then apply multi-teacher knowledge distillation to develop a cohesive retriever. LLM-R [156] refines its retriever iteratively by employing a static LLM for document ranking and reward model training, complemented by knowledge distillation. Each training cycle incrementally improves the retriever, enabling progressive optimization. Finardi et al. [157] integrated recip- rocal rank into the retrieval process for enhanced text chunk relevance, and utilized monoT5 as a reranker to optimize the result quality. Li et al. [158] integrate a reranking module into their end-to-end RAG system, enhancing the retrieval quality and factual accuracy of LLMs. Retrieval Transformation: Retrieval Transformation involves rephrasing retrieved content to better activate the generator’s potential, resulting in improved output. FILCO [159] efficiently purges extraneous material from retrieved text, isolating only the pertinent supporting content to streamline the generator’s task and facilitate accurate answer prediction. FiD-Light [160] initially employs an encoder to convert the retrieved content into a vector, which it then compresses, resulting in a substantial reduction of latency time. RRR [161] integrates the current query with the top-k document in each round through a template, and subsequently restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.). Others: In addition to the above optimization methods, there are also some other optimization methods for the retrieve process. For example, meta-data filtering [162] is a method to help processing retrieved documents which uses metadata (such8 as time, purpose, etc.) to filter the retrieved documents for better results. GENREAD [163] and GRG [164] introduce a novel approach where the retrieval process is supplanted or improved by prompting a LLM to generate documents in response to a given question. Multi-Head-RAG [165] employs multiple embedding models to project the same text chunk into various vector spaces and utilizes a multi-head attention layer to capture different informational aspects, thereby increasing the accuracy of the retrieval process. 3) Generator Enhancement: In RAG systems, the quality of the generator often determines the quality of the final output results. Therefore, the ability of the generator determines the upper limit of the entire RAG system’s effectiveness. Prompt Engineering: Technologies in prompt engineer- ing [166] that focus on improving the quality of LLMs’ output, such as prompt compression, Stepback Prompt [167], Active Prompt [168], Chain of Thought Prompt [133], etc., are all applicable to LLM generators in RAG systems. LLMLingua [169] applies a small model to compresses the overall length of the query to accelerate model inference, relieving the negative impact of irrelevant information on the model and alleviating the phenomenon of “Lost in the Middle” [170]. ReMoDiffuse [51] decomposes complex descriptions into anatomical text scripts by using ChatGPT. ASAP [171] in- corporates exemplar tuples, consisting of input\n",
      "\n",
      "Chunk 5:\n",
      "probability of generating the gen- erated answer given the reference) and recall (esti- mated as the probability of generating the reference given the generated answer). 3 Evaluation Strategies We consider a standard RAG setting, where given a question q, the system first retrieves some context c(q) and then uses the retrieved context to generate an answer as(q). When building a RAG system, we usually do not have access to human-annotated 151datasets or reference answers. We therefore fo- cus on metrics that are fully self-contained and reference-free. We focus in particular three quality aspects, which we argue are of central importance. First, Faithfulness refers to the idea that the answer should be grounded in the given context. This is important to avoid hallucinations, and to ensure that the retrieved context can act as a jus- tification for the generated answer. Indeed, RAG systems are often used in applications where the factual consistency of the generated text w.r.t. the grounded sources is highly important, e.g. in do- mains such as law, where information is constantly evolving. Second, Answer Relevancerefers to the idea that the generated answer should address the actual question that was provided. Finally, Con- text Relevancerefers to the idea that the retrieved context should be focused, containing as little ir- relevant information as possible. This is important given the cost associated with feeding long context passages to LLMs. Moreover, when context pas- sages are too long, LLMs are often less effective in exploiting that context, especially for informa- tion that is provided in the middle of the context passage (Liu et al., 2023). We now explain how these three quality aspects can be measured in a fully automated way, by prompting an LLM. In our implementation and experiments, all prompts are evaluated using the gpt-3.5-turbo-16k model, which is available through the OpenAI API2. Faithfulness We say that the answer as(q) is faithful to the context c(q) if the claims that are made in the answer can be inferred from the con- text. To estimate faithfulness, we first use an LLM to extract a set of statements, S(as(q)). The aim of this step is to decompose longer sentences into shorter and more focused assertions. We use the following prompt for this step3: Given a question and answer, create one or more statements from each sentence in the given answer. question: [question] answer: [answer] where [question] and [answer] refer to the given question and answer. For each statementsi in 2https://platform.openai.com 3To help clarify the task, we include a demonstration as part of the prompt. This demonstration is not explicitly shown in the listing of the prompts throughout this paper. S(as(q)), the LLM determines if si can be inferred from c(q) using a verification function v(si, c(q)). This verification step is carried out using the fol- lowing prompt: Consider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explana- tion for each statement before arriving at the verdict (Yes/No). Provide\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "Published in Transactions on Machine Learning Research (06/2023) Augmented Language Models: a Survey Grégoire Mialon∗ gmialon@meta.com Roberto Dessì∗† rdessi@meta.com Maria Lomeli∗ marialomeli@meta.com Christoforos Nalmpantis∗ christoforos@meta.com Ram Pasunuru∗ rpasunuru@meta.com Roberta Raileanu∗ raileanu@meta.com Baptiste Rozière∗ broz@meta.com Timo Schick∗ schick@meta.com Jane Dwivedi-Yu∗ janeyu@meta.com Asli Celikyilmaz∗ aslic@meta.com Edouard Grave∗ egrave@meta.com Yann LeCun∗ yann@meta.com Thomas Scialom∗ tscialom@meta.com ∗Meta AI †Universitat Pompeu Fabra Reviewed on OpenReview:https://openreview.net/forum?id=jh7wH2AzKK Abstract This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues. 1 Introduction: motivation for the survey and definitions 1.1 Motivation and Definitions Large Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022) have fueled dramatic progress in Natural Language Processing (NLP) and are already core in several products with millions of users, such as the coding assistant Copilot (Chen et al., 2021), Google search engine1 or more recently ChatGPT2. Memorization (Tirumala et al., 2022) combined with compositionality (Zhou et al., 2022) capabilities made LLMs able to execute various tasks such as language understanding or conditional and 1See e.g. https://blog.google/products/search/search-language-understanding-bert/ 2https://openai.com/blog/chatgpt/ 1Published in Transactions on Machine Learning Research (06/2023) unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards higher-bandwidth human-computer interactions. However, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide non-factual but seemingly plausible predictions, often referred to as hallucinations (Welleck et al., 2020). This leads to many avoidable mistakes, for example in the context of arithmetics (Qian et al., 2022) or within a reasoning chain (Wei et al., 2022c). Moreover, many LLMs groundbreaking capabilities seem to emerge with size, measured by the number of trainable parameters: for example, Wei et al. (2022b) demonstrate that LLMs become able to perform some BIG-bench tasks3 via few-shot prompting once a certain scale is attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest counterpart (Hoffmann et al., 2022), the size and need for data of LLMs can be impractical for training but also maintenance: continual learning for large models remains an open research question (Scialom et al., 2022).\n",
      "\n",
      "Chunk 2:\n",
      "can lead to the correct predictions even though the intermediate reasoning doesn’t make any sense, indicating clear challenges for researchers exploring this direction. • Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example, an ALM connected to a python interpreter can run code and experiments on a user’s behalf, which a vanilla LM cannot do. In addition, a feedback loop can emerge between reasoning and acting, where each ability further improves the other (Yao et al., 2022b). Interacting with external tools, entities, and environments can improve reasoning since it allows the ALM to collect additional information and ground itself in the real-world. Similarly, reasoning can improve the ALM’s decision making abilities such as when and how to use a certain tool. Cost of using tools. To the best of our knowledge, the cost of using tools has not yet been taken into account comprehensively. Overall, using tools seem to be beneficial in terms of energy use. Retrieval-augmented language models are a good example of a more efficient way to handle new information since updating an external data store can be 21Published in Transactions on Machine Learning Research (06/2023) orders of magnitude cheaper than re-training a LLM for scratch every time we get new data (as argued for example in Borgeaud et al. (2022)). This stays true for using a calculator or browsing the web for example. One can therefore assume that relying on tools rather than storing knowledge in parameters is generally more energy-efficient than training, frequently updating a LLM, and scaling it if some ability remains out of reach, if one assumes an optimal tool usage (i.e., not using each tool at each token). Then, how to assess the cost of using a tool versus another? We believe that the most natural way of estimating such cost is to consider two factors: (i) the price per tool request, and (ii) the time required to fulfill the request. These factors make sense from the perspective of the ALM, and are a reasonable way to take into account the cost of creating and maintaining the tool since this should typically be reflected in the pricing. Interestingly, including such cost at training and inference time under the form of a loss term for example could help ALMs to develop optimal tool use, and lead to the emergence of models that know how to balance between tool use or internal chain of thoughts. Ethical concerns. ALMs raise new ethical concerns. LM predictions based on tools may look more trustworthy and authoritative at a first glance, when in fact many of them will still be incorrect. Moreover, we can expect this phenomenon to be amplified as LMs reason in quite a similar manner to humans (Dasgupta et al., 2022), making it even harder to detect mistakes. Hence, ALMs may be leveraged to generate more convincing fake news and conspiracy theories. Conversely, conditioning ALMs on “safe” and trusted\n",
      "\n",
      "Chunk 3:\n",
      "Context With the deepening of related research, the context of LLMs is continuously expanding [170]–[172]. Presently, LLMs can effortlessly manage contexts exceeding 200,000 tokens 9. This capability signifies that long-document question answering, previously reliant on RAG, can now incorporate the entire document directly into the prompt. This has also sparked discussions on whether RAG is still necessary when LLMs 8https://www.trulens.org/trulens eval/core concepts rag triad/ 9https://kimi.moonshot.cn are not constrained by context. In fact, RAG still plays an irreplaceable role. On one hand, providing LLMs with a large amount of context at once will significantly impact its inference speed, while chunked retrieval and on-demand input can significantly improve operational efficiency. On the other hand, RAG-based generation can quickly locate the original references for LLMs to help users verify the generated an- swers. The entire retrieval and reasoning process is observable, while generation solely relying on long context remains a black box. Conversely, the expansion of context provides new opportunities for the development of RAG, enabling it to address more complex problems and integrative or summary questions that require reading a large amount of material to answer [49]. Developing new RAG methods in the context of super-long contexts is one of the future research trends. B. RAG Robustness The presence of noise or contradictory information during retrieval can detrimentally affect RAG’s output quality. This situation is figuratively referred to as “Misinformation can be worse than no information at all”. Improving RAG’s resistance to such adversarial or counterfactual inputs is gain- ing research momentum and has become a key performance metric [48], [50], [82]. Cuconasu et al. [54] analyze which type of documents should be retrieved, evaluate the relevance of the documents to the prompt, their position, and the number included in the context. The research findings reveal that including irrelevant documents can unexpectedly increase accuracy by over 30%, contradicting the initial assumption of reduced quality. These results underscore the importance of developing specialized strategies to integrate retrieval with language generation models, highlighting the need for further research and exploration into the robustness of RAG. C. Hybrid Approaches Combining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to- end joint training—and how to harness both parameterized15 TABLE IV SUMMARY OF EVALUATION FRAMEWORKS Evaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics RGB† Retrieval Quality Generation Quality Noise Robustness Negative Rejection Information Integration Counterfactual Robustness Accuracy EM Accuracy Accuracy RECALL† Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate) RAGAS‡ Retrieval Quality Generation Quality Context Relevance Faithfulness Answer Relevance * * Cosine Similarity ARES‡ Retrieval Quality Generation Quality Context Relevance Faithfulness Answer Relevance Accuracy Accuracy Accuracy TruLens‡ Retrieval Quality Generation Quality Context Relevance Faithfulness Answer Relevance * * * CRUD† Retrieval Quality Generation Quality Creative Generation Knowledge-intensive QA Error Correction Summarization BLEU ROUGE-L BertScore RAGQuestEval † represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional metrics. Readers are encouraged to consult pertinent literature for the specific\n",
      "\n",
      "Chunk 4:\n",
      "within another tool call, the number of tool uses can become exponential in the number of tools. For example, within a tool call, an ALM could browse the web by reading the content of a web page, 19Published in Transactions on Machine Learning Research (06/2023) which is text, then apply another of the tools to the text it found to refine its query, etc. If for some reason we require a depth that is equal to the number of tools, we have an exponential complexity. For these reasons, we refer to Augmented Language Models (ALMs) in this survey, to distinguish from Language Modeling in the traditional sense. A tradeoff between memorizing and querying tools.Is it preferable to memorize information in the model weights, or to leverage external tools? Some situations arguably require external tools, for example computing 213443344. However, many information are well known facts such as “The Eiffel tower is located in Paris” or1 + 2 = 3, and should not be offloaded. And, when learning world representations, memorization is not only desirable, but also deeply connected to reasoning (Hayes et al., 2014). Can ALMs be calibrated enough to decide when and when not to use a tool? Could a computation budget for each tool be integrated into the loss to let the model learn to do so? Generalizing the non-parametric framework. A motivation behind information retrieval augmented LMs such asRETRO (Borgeaud et al., 2022) andAtlas (Izacard et al., 2022) is to develop a class of LM requiring less parameters through relying on an external non-parametric memory. The motivation for using other kind of tools such ascode interpreter or calculator has been slightly different so far: for instance, Cobbe et al. (2021) use a calculator to improve accuracy on tasks requiring arithmetic. Yet, the paradigm of tool-augmented LMs can be seen as a generalization of the non-parametric framework. Indeed, beyond information retrieval, LMs can delegate any kind of abilities such as calculus to the corresponding external tools. By avoiding to store rarely accessed knowledge in their weights, tool-augmented LMs may have better scaling laws and thus yield smaller models retaining the capabilities of their largest counterpart. Combined with the possibility to access recent information from the external world thus avoiding frequent updates, non-parametric generalization holds great benefits for ALMs. A path towards autonomous machine intelligence?A concept for an autonomous intelligent agent was proposed by LeCun (2022). We now discuss to what extent ALMs instantiate this idea. In LeCun (2022), the agent is composed of different modules starting from a world model and a short-term memory. Essentially, the agent takes actions via an actor module based on its world model, perception module, and short-term memory so as to minimize some cost. The agent is also equipped with a configurator module for modulating the world model, the perception, the actor and the cost given the task at hand. Translating into this framework, the ALM’s weights essentially contain the world model, perception and actor modules. The short-term memory can be identified with\n",
      "\n",
      "Chunk 5:\n",
      "in natural language and interact with external tools. Along these lines, Wang et al. (2023) uses a LM as a centralized planner to generate goal sequences for solving tasks in the game of Minecraft. Through a feedback loop and intermediate checks on subgoals execution, the LM can explain mistakes of the goal executor and refine its original plan. However, we note that a LM-based controller might not be the only viable approach for a generalist agent. Recent work on the game of Diplomacy (Bakhtin et al., 2022), a long-standing challenge for AI agents due to its complex planning and reasoning dynamics, employs an ad-hoc planning model trained via self-play and reinforcement learning. Here the LM is used to interact with other players, thus as an external communication module grounded in the current state of the game. This offers an alternative view of LMs as agents specialized to communicate with humans, albeit in the restricted setting of a Diplomacy game. We believe that (A)LMs will play a central role in the next generation of powerful interactive systems, whether as centralized controller of a modular system or as a language-only module that needs to interact with an orchestrator remains an open research question. Augmented Language Models benefits.Overall, ALMs offer many potential advantages over traditional LMs. • Truthfulness: As the current LM’s training objective is arguably responsible for inciting the generation of seemingly plausible but not factual information, grounding the predictions through some tools should lead to more trustworthy models. However, although this conclusion is straightforward when equipping a LM with a calculator, there is surprisingly little evidence of it for information retrieval augmented LMs (Krishna et al., 2021). One of the reasons is the presence of a lot of non-truthful information in the web. Investigating this direction will be critical for making LM reliable. • Estimating and reducing uncertainty: Extending the maximum-likelihood paradigm by letting the model reason and access additional information could help models to learn what they know and what they don’t. Some papers suggest that LMs are already well calibrated (Kadavath et al., 2022), i.e. there is a high correlation between the accuracy of their predictions and the corresponding likelihood. This uncertainty could be directly exploited by ALMs to know when to rely on their own weights, or when to query an external tool. • Interpretability: Deep learning models are often considered to be black boxes, and their predictions are difficult to interpret. Providing intermediate reasoning steps and relying on tools should help to make ALMs more interpretable. In particular, we can expect that being able to cite the sources used to compose the answer to be critical. However, some works Lewkowycz et al. (2022) pointed out that chain-of-thoughts can lead to the correct predictions even though the intermediate reasoning doesn’t make any sense, indicating clear challenges for researchers exploring this direction. • Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful assistants and solve a wider range of tasks than standard LMs. For example,\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "with Chain-of-Thought Reasoning Our proposed retriever method, IRCoT, can be instantiated from the following three ingredients: (i) a base retriever that can take a query and re- turn a given number of paragraphs from a corpus or knowledge source; (ii) a language model with zero/few-shot Chain-of-Thought (CoT) generation capabilities; and (iii) a small number of annotated questions with reasoning steps explaining how to arrive at the answer in natural language (chain of thoughts) and a set of paragraphs from the knowl- edge source that collectively support the reasoning chain and the answer. The overview of IRCoT is given in Fig. 2. We first gather a base set of paragraphs by retrievingK paragraphs using the questionQ as the query. Then, we interleave two steps ( reason and retrieve) iteratively until the termination criterion is met. The retrieval-guided reasoning step (“Rea- son”) generates the next CoT sentence using the question, the paragraphs collected thus far, and the CoT sentences generated thus far. The prompt template for the task looks as follows: Wikipedia Title: <Page Title> <Paragraph Text> ... Wikipedia Title: <Page Title> <Paragraph Text> Q: <Question> A: <CoT-Sent-1> ... <CoT-Sent-n> For in-context demonstrations, we use the com- plete CoT in the above format. For a test instance, 10016Who wrote the 1970 international hit song that Murray Head is most recognized for? The 1970 international hit song that Murray Head is most recognized for is \"Super Star\" \"Super Star\" was written by Andrew Lloyd W ebber and T im Rice. So the answer is: Andrew Lloyd W ebber and T im Rice. Retrieve (Q) → Retrieve (T1) → Retrieve (T2) → T1 ← Reason (Q, , ) T2 ← Reason (Q, + , T1) T3 ← Reason (Q, + + , T1+T2) T1 Q T2 T3 Stop + + IRCoT Interleaved Retrieval guided by Chain-of-Thought Reasoning Retrieve( ) Wikipedia T itle: Mack Rides Mack Rides GmbH & Co KG, also ... Q: In what country was Lost Gravity manufactured? A: The Lost Gravity was manufactured by Mack Rides. Mack Rides is a company from Germany . The answer is Germany . ... Wikipedia T itle: Murray Head Murray Seafield St George Head .. ... Wikipedia T itle: Most Beautifullest Hits The Most Beautifullest Hits is ... Q: Who wrote the 1970 international hit .. A: The 1970 international hit song that Murray Head is most recognized for is \"Super Star\". \"Super Star\" was written by . Andrew Lloyd W ebber and T im Rice. q Reason( , , ) Q T1 Q T1 q Figure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and vice-versa. We start by retrieving K documents using the question as they query and repeat two steps alternatingly until termination. (i) reason-step generates next CoT sentence based on the question, so far retrieved paragraphs, and CoT sentences. (ii) retrieve-step retrieves K more paragraphs based on the last CoT sentence. The process terminates when the generated CoT has “answer is” or the number\n",
      "\n",
      "Chunk 2:\n",
      "encode all candidate documents and store representations for each document. These two operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022). § Unless otherwise speciﬁed, we use the text-davinci-002 version of InstructGPT in our experiments. * Work done during internship at Microsoft Cognitive Service Research group. 1Published as a conference paper at ICLR 2023 In this paper, we propose to leverage large language models, such as InstructGPT (Ouyang et al., 2022), to directly generate contextual documents for a given question, instead of retrieving relevant documents from an external corpus, such as Wikipedia. Our approach has two main advantages. First, we show that generated contextual documents contain the correct answer more often than the top retrieved documents. We believe this is because large language models generate contextual documents by performing deep token-level cross-attention between all the question and document contents, resulting in generated documents that are more speciﬁc to the question than retrieved documents. Second, we show that our approach signiﬁcantly outperforms directly generating answers from large language models despite not incorporating any new external information. This is mainly because the task of generating document-level contexts is close to the objective of causal language modeling pre-training, so the world knowledge stored in the model parameters can be better utilized. We show, on multiple datasets, that generated documents are more likely to contain correct answers than the top retrieved documents. Notably, in dense retrieval methods, as more documents are retrieved, the recall of documents containing the correct answer increases (Karpukhin et al., 2020). However, the recall performance does not scale as well with generated documents because even with sampling methods, generated documents tend to contain duplicate information. In order to improve the recall performance of generated documents, we propose a novel clustering-based prompt method. We synthesize a prompt with in-context demonstrations of question-document pairs sampled from diverse clusters. These prompts result in generated documents that cover different perspectives of the question and improve the scaling of performance as more documents are generated per question. In contrast to the retrieve-then-read pipeline, our method is essentially a generate-then-read pipeline. Speciﬁcally, it ﬁrst prompts a large language model to generate contextual documents based on a given question, and then reads the generated document to produce the ﬁnal answer. The reader can still be a large model (e.g., InstructGPT (Ouyang et al., 2022)) used under a zero-shot setting, or a small one (e.g., FiD (Izacard & Grave, 2021)) ﬁne-tuned with generated documents on the training split of the target dataset. We evaluate our proposed method on three different knowledge-intensive tasks and demonstrate its effectiveness on both zero-shot and supervised settings. Overall, our main contributions can be summarized as follows: 1. We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks, i.e., replacing the process of retrieving documents from Wikipedia or searching for related documents on Google, by prompting a large language model to generate\n",
      "\n",
      "Chunk 3:\n",
      "Published as a conference paper at ICLR 2023 GENERATE RATHER THAN RETRIEVE : L ARGE LANGU - AGE MODELS ARE STRONG CONTEXT GENERATORS Wenhao Yu1∗, Dan Iter2, Shuohang Wang2, Yichong Xu2, Mingxuan Ju1, Soumya Sanyal3∗, Chenguang Zhu2, Michael Zeng2, Meng Jiang1 1University of Notre Dame 2Microsoft Cognitive Service Research 3University of Southern California 1wyu1@nd.edu; 2iterdan@microsoft.com ABSTRACT Knowledge-intensive tasks, such as open-domain question answering (QA), require access to a large amount of world or domain knowledge. A common approach for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that ﬁrst retrieves a handful of relevant contextual documents from an external corpus such as Wikipedia and then predicts an answer conditioned on the retrieved documents. In this paper, we present a novel perspective for solving knowledge-intensive tasks by replacing document retrievers with large language model generators. We call our method generate-then-read (GENREAD ), which ﬁrst prompts a large language model to generate contextual documents based on a given question, and then reads the generated documents to produce the ﬁnal answer. Furthermore, we propose a novel clustering-based prompting method that selects distinct prompts, in order to generate diverse documents that cover different perspectives, leading to better recall over acceptable answers. We conduct extensive experiments on three different knowledge-intensive tasks, including open-domain QA, fact checking, and dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on TriviaQA and WebQ, signiﬁcantly outperforming the state-of-the-art retrieve-then- read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from any external knowledge source. Lastly, we demonstrate the model performance can be further improved by combining retrieval and generation. Our code and generated documents can be found at https://github.com/wyu97/GenRead. 1 I NTRODUCTION Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require access to a large amount of world or domain knowledge (Petroni et al., 2021). These tasks are even challenging for humans without access to an external knowledge source such as Wikipedia. A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read pipeline that ﬁrst retrieves a handful of relevant contextual documents from Wikipedia and then conditions the prediction of the answer on these documents along with the question (Karpukhin et al., 2020; Lewis et al., 2020; Izacard & Grave, 2021). Nevertheless, these methods mainly suffer from three drawbacks. First, candidate documents for retrieval are chunked (e.g., 100 words) and ﬁxed, so the retrieved documents might contain noisy information that is irrelevant to the question. Second, the representations of questions and documents are typically obtained independently in modern two-tower dense retrieval models (Karpukhin et al., 2020), leading to only shallow interactions captured between them (Khattab et al., 2021). Third, document retrieval over a large corpus requires the retriever model to ﬁrst encode all candidate documents and store representations for each document. These two operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022). § Unless otherwise speciﬁed, we use the\n",
      "\n",
      "Chunk 4:\n",
      "is more costly since it requires a separate forward pass for each retrieved premise. Similar to DPR, we train the retriever by minimizing a contrastive loss between positive premises and in-batch negative premises. Specifically, suppose we have a batch of b states. For each state, we sample a positive premise from the ground truth and n negative premises from P.7 They are called “in-batch” negatives because they are shared by all states in the batch—Every state is associated with all b · (n + 1) premises; at least 1 of them is positive. Let lij ∈ {0, 1} denote whether a state-premise pair (si, pj) is positive. We minimize the mean squared loss: L(θ) = bX i=1 b·(n+1)X j=1 lij − f(si, θ)T f(pj, θ) ∥f(si, θ)∥2∥f(pj, θ)∥2 2 . (1) 6Another common type of proofs is “term-style proofs”. Any term-style proof “X” can always be converted into an equivalent tactic-style proof “exact X”, though such conversion may lead to unidiomatic proofs. 7When training the retriever, we ignore proof states followed by tactics without using any premise. 7Retrieving from Accessible Premises. We incorporate into DPR two insights tailored to premise selection. First, instead of retrieving from all premises in the math library, we restrict to premises accessible to the current theorem. They include premises defined in the same file before the theorem, as well as those imported from other files. We compute accessible premises for each theorem, relying on LeanDojo’s capability in program analysis (Sec. 4). Focusing on accessible premises makes P much smaller. LeanDojo Benchmark contains 130,262 premises in total, but the average number of accessible premises is only 33,160. In-file Negative Examples. DPR’s performance depends critically on the quality of negative examples [91, 92]. In early experiments, we sampled all n negative premises randomly, and the model often mistakenly retrieved other premises from the same file as the positive one. Therefore, we propose a scheme that samples k in-file negatives and n − k random negatives for training. Tactic Generation. As in Fig. 1 ( Bottom), retrieved premises are concatenated with the state. 8 Then an encoder-decoder Transformer, ByT5 [44], takes them as input and generates the tactic. The model is trained to minimize the cross entropy loss w.r.t. human-written tactics. Training ReProver takes substantially less compute than prior methods (120 GPU hours vs. more than 1000 hours [16, 17]). All existing LLM-based provers pretrain on datasets specific to math and coding [14–20]. The pretraining is computationally expensive, and the datasets are kept private. In contrast, we choose to avoid domain-specific pretraining and build upon google/byt5-small—a model checkpoint that is generic, publicly available, and relatively small (299M parameters vs. 837M [16] or 600M [ 17]). We could see further benefits from domain-specific pretraining, as in Minerva [57], or stronger LLMs like LLaMA [93] or StarCoder [94], but that is beyond our scope. In addition, our model is finetuned on human-written tactics only, without auxiliary data [16] or data collected through online interaction with Lean [17, 19]. These orthogonal\n",
      "\n",
      "Chunk 5:\n",
      "language modeling, where it is used for pre- training (Guu et al., 2020), and as a memory (Yo- gatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017). 3 Model Architectures We extend neural-retriever-in-the-loop generative- based architectures, which have performed well in open-domain QA, to knowledge-grounded tasks, where model responses must not only be knowl-3786 edgeable but also consistent and engaging both across long-form generation and throughout multi- ple turns of conversation. To keep notation consistent, we let xi = {x1 i, ..., xn i}represent the tokens for dialogue con- text i, and deﬁne yi similarly for the ground truth response; Zi = {zi,1, ...,zi,k}is the set of k documents retrieved. q(xi) and d(zj) are rep- resentations of the dialogue context and candi- date document respectively in the retrieval mecha- nism, where pη(zj|xi) is the probability of select- ing a document zj given a context xi. Finally, pθ(ym i |xi, zi,j, y1 i...ym−1 i ) is the full generator probability of outputting a token ym i given xi, zi,j, and the prior output tokens, where pθ(yi|xi, zi,j) is the full sequence score. In some cases subscripts i and j are omitted for clarity. 3.1 RAG and FiD Neural retrievers have been shown to outperform word-similarity-based architectures such as BM25, and, with the help of GPU-based similarity search libraries such as FAISS (Johnson et al., 2019), can scale to knowledge sources of millions of docu- ments. We ﬁrst discuss these new architectures. Lewis et al. (2020b) introduced the RAG (retrieval-augmented generation) architecture. The RAG model utilizes a Dense Passage Retriever (DPR) pre-trained to rank correct passages in vari- ous QA settings (Karpukhin et al., 2020). A large FAISS index stores d(zj), with q(xi) as the query for relevant documents. RAG-Sequence consid- ers documents independently, generating an out- put sequence for each concatenated context sepa- rately and marginalizing over the output genera- tions. RAG-Tokenmarginalizes the output distri- bution over all documents, allowing the generator to attend over a different document for each token. Though d(zj) remains ﬁxed during training, token losses are propagated to the retriever itself, and the context representations q(xi) are updated in order to better ﬁt the retriever for the task. Izacard and Grave (2021b) introduce the FiD (Fusion-in-Decoder) method. Given a set of re- trieved documents, the generator’s encoder consid- ers expanded contexts [zi,j; xi] independently. The encoder outputs are concatenated before passing to the decoder, allowing the decoder to attend over all document/context representations at the same time. Despite ﬁxing the retriever throughout train- ing, FiD demonstrates superior performance on a number of QA tasks, demonstrating its efﬁcacy in attending over several documents. 3.2 Improving Neural Retrieval The introduction of neural retrieval is a major driver of the performance gains achieved in QA tasks by the RAG and FiD models; when substituting a non- neural retriever, performance in open-domain QA tasks suffers dramatically (Lewis et al., 2020b). It follows that further improving retrieval should\n",
      "\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 1:\n",
      "case of RAG. 4 Self-Route 4.1 Motivation As demonstrated in Sec. 3, RAG lags behind long- context LLMs in terms of performance. However, despite this performance gap, we surprisingly ﬁnd a high degree of overlap in their predictions, as illustrated in Fig. 2. 𝑆!\"# − 𝑆$% Figure 2: Distribution of the difference of predic- tion scores between RAG and LC (computed w.r.t. groundtruth labels). RAG and LC predictions are highly identical, for both correct and incorrect ones. Fig. 2 displays the distribution of the differences between RAG prediction scores SRAG and LC pre- diction scores SLC, speciﬁcally SRAG −SLC (the scores are multiplied by 100 to be scaled to 1-100). These scores S represent the evaluation of model predictions against the groundtruth. Notably, for 883Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC 1-1 LC 49.70 32.76 47.83 52.33 61.85 62.9640.22 20.73 43.08 85.57 Gemini-1.5-Pro 1-2 RAG 37.33 22.54 44.68 49.53 48.36 54.24 26.56 19.51 19.46 51.09 1-3 S ELF-ROUTE 46.41 28.32 45.23 51.47 55.18 62.68 40.66 19.77 37.51 76.86 1-4 answerable %76.78 73.00 85.00 96.67 84.50 81.00 58.50 93.50 56.41 62.45 1-5 token % 38.39 23.07 49.93 36.88 32.97 53.49 56.14 17.96 42.25 32.84 GPT-4O 2-1 LC 48.67 32.78 44.54 55.28 62.42 70.69 41.65 21.92 32.36 76.42 2-2 RAG 32.60 18.05 46.02 50.74 36.86 50.21 16.09 19.97 14.43 41.05 2-3 S ELF-ROUTE 48.89 31.36 47.99 53.17 62.14 70.14 41.69 21.31 34.95 77.29 2-4 answerable %57.36 44.00 67.50 94.00 52.50 62.00 30.00 92.00 27.07 47.16 2-5 token % 61.40 66.40 72.25 39.65 65.79 77.05 85.00 20.26 73.01 53.21 GPT-3.5-Turbo 3-1 LC 32.07 23.34 42.96 49.19 45.33 41.04 17.92 19.61 14.73 34.50 3-2 RAG 30.33 18.22 38.15 49.21 37.84 35.16 16.41 18.94 15.39 43.67 3-3 S ELF-ROUTE 35.32 24.06 38.65 52.07 47.28 44.62 34.44 19.88 22.03 44.54 3-4 answerable %74.10 71.50 80.00 91.33 68.50 69.00 47.00 93.50 50.43 95.63 3-5 token % 38.85 20.56 55.08 35.29 48.70 65.91 65.08 16.40 38.17 4.50 Table 1: Results of Gemini-1.5-Pro, GPT-3.5-Turbo, and GPT-4O using the Contriever retriever. LC consistently outperforms RAG, while SELF -ROUTE achieves performance comparable to LC using much less tokens. most queries, RAG scores and LC scores are highly similar. In fact, for 63% queries, the model pre- dictions are exactly identical; and for 70% queries, the score difference is less than 10 (absolute value). Interestingly, the identical predictions are not nec- essarily correct, as shown by the varying colors rep- resenting the average score, i.e., (SRAG + SLC)/2. This observation suggests that RAG and LC tend to make not only the same correct predictions but also similar errors. This ﬁnding motivates us to leverage RAG for the majority of queries, reserving computationally more expensive LC for a small subset of queries where it truly excels. By doing so, RAG can signif- icantly reduce computational costs without sacriﬁc- ing overall performance. 4.2 Self-Route Based on the above motivation, we propose SELF - ROUTE , a simple yet effective method combining RAG and LC to reduce cost while maintaining a performance\n",
      "\n",
      "Chunk 2:\n",
      "reduced (rows *- 5). For example, GPT-4O uses only 61% tokens while achieving comparable performance (46.83) with LC (47.04), Gemini-1.5-Pro uses 38.6% of the tokens. Since the computation cost of the transformer-based LLMs is quadratic to token count, and most LLM APIs charge based on token count (OpenAI, 2024b; Google, 2024), this lower token count translates to substantial cost savings. On longer datasets, the advantage of our method is more pronounced for OpenAI models, but less signiﬁcant for Gemini. For instance, for GPT-4O, SELF -ROUTE outperforms LC by 2.3% and 7.4% respectively on EN.QA and EN.MC, which contain longer contexts. For GPT-3.5-Turbo, the advantage margins are even larger. However, for Gemini- 1.5-Pro, the performance is lower than LC. These different behaviors are possibly due to the differ- ence in LLM alignments, i.e., OpenAI models are more likely to reject answering using RAG, leading to a lower answerable percentage but higher accu- racy, which results in a different performance-cost trade-off compared with Gemini-1.5-Pro. 5 Analysis 5.1 Ablations of k Both RAG and SELF -ROUTE relies on the top- k retrieved text chunks. The larger k is, the longer context are fed into LLMs for RAG prediction as well as routing, resulting in different costs versus performances. To study the inﬂuence ofk, in Fig. 3, we plot the performance and cost (i.e. input token percentage) curves when different ks are used. In terms of performance, for both RAG and SELF -ROUTE , a larger k leads to better perfor- mance. While k increases, more and more chunks are fed into the LLMs, thus the performance grad- ually improves to approach LC. As can be seen in from the curves, the advantage of SELF -ROUTE is the most signiﬁcant for smaller k. For example, when k = 1, RAG gets from 20.24% while SELF - ROUTE gets 37.9%, while when k is larger than 50, all three methods get similar performance. However, the trend of cost is not monotonous for SELF -ROUTE . As seen, the cost reaches its performance top-k token percentage (a) (b) top-k Figure 3: Trade-off curves between (a) model perfor- mance and (b) token percentage as a function of k. minimum at k = 5. This is because when k in- creases, the cost of RAG (and routing) increases, but more queries are routed to RAG from LC, thus the overall cost may decrease. The sweet point ofk might be different for each dataset, e.g. on average, k = 5has the lowest cost as shown in the curves, but on some datasets, especially ones that contain extractive questions which does not need multi-hop reasoning (like NarrativeQA and QMSum), k = 1 leads to the lowest cost. This indicates that the opti- mal k depends on the nature of the task, as well as the performance requirement. We encourage future researchers to look for different ks when applying our method to various applications. 5.2 Why does RAG fail? To gain a better understanding of why RAG lags behind LC, we\n",
      "\n",
      "Chunk 3:\n",
      "of queries where it truly excels. By doing so, RAG can signif- icantly reduce computational costs without sacriﬁc- ing overall performance. 4.2 Self-Route Based on the above motivation, we propose SELF - ROUTE , a simple yet effective method combining RAG and LC to reduce cost while maintaining a performance comparable to LC. SELF -ROUTE uti- lizes LLM itself to route queries based on self- reﬂection, under the assumption that LLMs are well-calibrated in predicting whether a query is answerable given provided context. Concretely, our method consists of two steps: a RAG-and-Route step and a long-context prediction step. In the ﬁrst step, we provide the query and the retrieved chunks to the LLM, and prompt it to predict whether the query is answerable and, if so, generate the answer. This is similar to standard RAG, with one key difference: the LLM is given the option to decline answering with the prompt “Write unanswerable if the query can not be answered based on the provided text” . For the queries deemed answerable, we accept the RAG prediction as the ﬁnal answer. For the queries deemed unanswerable, we proceed to the second step, providing the full context to the long-context LLMs to obtain the ﬁnal prediction (i.e., LC). As our results will demonstrate, most queries can be solved by the ﬁrst RAG-and-Route step ( e.g., 82% for Gemini-1.5-Pro), with only a small por- tion requiring the following long-context prediction step. Since the RAG-and-Route step only needs the retrieved chunks ( e.g., 1.5k tokens) as input, which is signiﬁcantly shorter than the full contexts (e.g., 10k - 100k tokens), the overall computation cost is substantially reduced. Detailed token count analysis will be provided in the results. 4.3 Results Rows *-3 to *-5 in Tab. 1 present the results of our method, utilizing the three LLMs. Rows *-3 report the performance. Rows *-4 show the percentage of answerable queries, as predicted in the RAG- and-Route step. Rows *-5 display the percentage of tokens used by our method, compared to that of LC. In terms of performance (rows *-3), SELF - ROUTE signiﬁcantly outperforms RAG, achieving results comparable to LC. Across all three models, SELF -ROUTE surpasses RAG (rows *-2) by over 5%. Compared to LC (rows *-1), there is a slight performance drop for GPT-4O (-0.2%) and Gemini- 1.5-Pro (-2.2%), but an improvement for GPT-3.5- Turbo (+1.7%). All three LLMs consistently route more than half 884of queries towards RAG, as shown in rows *-4. For Gemini-1.5-Pro, the answerable percentage even reaches 81.74% (row 1-4). This indicates that RAG may answer most queries without the need for LC, conﬁrming our initial motivation. Due to the high answerable rate, the number of tokens required is signiﬁcantly reduced (rows *- 5). For example, GPT-4O uses only 61% tokens while achieving comparable performance (46.83) with LC (47.04), Gemini-1.5-Pro uses 38.6% of the tokens. Since the computation cost of the transformer-based LLMs is quadratic to token count, and most LLM APIs charge based on token count (OpenAI, 2024b; Google,\n",
      "\n",
      "Chunk 4:\n",
      "pros and cons, and ultimately combining them to get the best of both worlds. Different from ﬁndings in previous work (Xu et al., 2023), we ﬁnd that LC consistently outperform RAG in almost all settings (when re- sourced sufﬁciently). This demonstrates the su- perior progress of recent LLMs in long-context understanding. Despite the suboptimal performance, RAG re- mains relevant due to its signiﬁcantly lower compu- tational cost. In contrast to LC, RAG signiﬁcantly decreases the input length to LLMs, leading to re- 881duced costs, as LLM API pricing is typically based on the number of input tokens. (Google, 2024; Ope- nAI, 2024b)1. Moreover, our analysis reveals that the predictions from LC and RAG are identical for over 60% of queries. For these queries, RAG can reduce cost without sacriﬁcing performance. Based on this observation, we propose SELF - ROUTE , a simple yet effective method that routes various queries to RAG or LC based on model self- reﬂection. With SELF -ROUTE , we signiﬁcantly re- duce the cost while achieving overall performance comparable to LC. For example, the cost is reduced by 65% for Gemini-1.5-Pro and 39% for GPT-4O. Fig. 1 shows the comparisons of LC, RAG and SELF -ROUTE using three recent LLMs: GPT-4O, GPT-3.5-Turbo and Gemini-1.5-Pro. In addition to quantitative evaluation, we provide a comprehen- sive analysis comparing RAG and LC, including common failure patterns of RAG, the trade-offs between cost and performance, and the results on additional synthetic datasets. Our analysis serves as a starting point, inspiring future improvements of RAG, and as a empirical guide for building long- context applications using RAG and LC. 2 Related Work Long-context LLMs. There has long been ef- forts for enabling LLMs to handle long contexts (Guo et al., 2022; Beltagy et al., 2020; Chen et al., 2023b). While recent LLMs like Gemini-1.5 (Reid et al., 2024), GPT-4 (Achiam et al., 2023), Claude- 3 (Anthropic, 2024) achieve signiﬁcantly larger context window size, long-context prompting is still expensive due to the quadratic computation cost of transformers regarding to the input token numbers. Recent work proposes methods to reduce cost by prompt compression (Jiang et al., 2023), model distillation (Hsieh et al., 2023), or LLM cas- cading (Chen et al., 2023a). Retrieval-augmented generation. Augmenting LLMs with relevant information retrieved from var- ious sources (Lewis et al., 2020) has been success- ful in complementing LLMs with external knowl- edge. RAG achieves good performance on tasks like language modeling (Khandelwal et al., 2019; Shi et al., 2023) and QA (Guu et al., 2020; Izacard and Grave, 2020), with a signiﬁcantly lower compu- tation cost (Borgeaud et al., 2022). Related to but different from our work, recently works augment 1While retrieval may introduce extra cost, retrieval system is much easier to set up and can be hosted on customer side. RAG with correction (Yan et al., 2024), critique (Asai et al., 2023), veriﬁcation (Li et al., 2023), or adaptive search (Wang et al., 2023; Cheng et al., 2024; Jeong et al., 2024) to improve retrieval\n",
      "\n",
      "Chunk 5:\n",
      "We discuss the limitations of our framework as follows: (1) Although Selfmem greatly improves the generation quality compared with other retrieval- augmented generation models, it requires more computational resources with respect to the memory selection process. For large dataset with long context (e.g., BigPatent), it would become a more crucial problem considering the quadratic time complexity of transformer architecture. (2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment with transformer-based architecture for both generator and memory selector and the architecture of generator and memory selector keeps the same across all text generation tasks. We believe the task-specific design for the model architecture, training objective and generation methods in different text generation scenarios would further improve the performance. Acknowledgement This work was supported by the National Key Research and Development Program of China (No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan Zhao and Rui Yan are the corresponding authors. References [1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proc. of EMNLP, 2021. [2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-context examples selection for machine translation. CoRR, 2022. [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of ICLR, 2015. [4] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue generation model with discrete latent variable. In Proc. of ACL, 2020. [5] Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum learning. In Proc. of ACL Findings, 2021. [6] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. Skeleton-to-response: Dialogue generation guided by retrieval memory. In Proc. of NAACL, 2019. [7] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided dialogue response generation via a matching-to-generation framework. In Proc. of EMNLP, 2019. 10[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with monolingual translation memory. In Proc. of ACL, 2021. [9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine translation research. In Proc. of EACL, 2006. [10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation. In Proc. of EMNLP, 2018. [11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. of ACL, 2005. [12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, 2017. [13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu, Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained\n",
      "\n",
      "Generated answers for test queries have been saved to generated-test-answers.json.\n"
     ]
    }
   ],
   "source": [
    "# Load test queries\n",
    "with open(\"test-queries.json\", \"r\") as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Generate answers for test queries\n",
    "for q in test_queries:\n",
    "    q[\"generated_answer\"] = generate_answer(q[\"query\"])\n",
    "\n",
    "# Save test results\n",
    "with open(\"generated-test-answers.json\", \"w\") as f:\n",
    "    json.dump(test_queries, f, indent=4)\n",
    "\n",
    "print(\"Generated answers for test queries have been saved to generated-test-answers.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87510a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provided Queries - Average Precision: 0.8460\n",
      "Provided Queries - Average Recall: 0.8412\n",
      "Provided Queries - Average F1 Score: 0.8432\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Extract ground truth and generated answers for provided queries\n",
    "provided_ground_truths = [q[\"answer\"] for q in provided_queries]\n",
    "provided_generated_answers = [q[\"generated_answer\"] for q in provided_queries]\n",
    "\n",
    "# Compute BertScore\n",
    "P_provided, R_provided, F1_provided = score(provided_generated_answers, provided_ground_truths, lang=\"en\")\n",
    "\n",
    "# Compute average scores\n",
    "avg_precision_provided = P_provided.mean().item()\n",
    "avg_recall_provided = R_provided.mean().item()\n",
    "avg_f1_provided = F1_provided.mean().item()\n",
    "\n",
    "# Print results\n",
    "print(f\"Provided Queries - Average Precision: {avg_precision_provided:.4f}\")\n",
    "print(f\"Provided Queries - Average Recall: {avg_recall_provided:.4f}\")\n",
    "print(f\"Provided Queries - Average F1 Score: {avg_f1_provided:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b854c9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_provided: tensor([0.7919, 0.7656, 0.9180, 0.8554, 0.9791, 0.8427, 0.8028, 0.7598, 0.8983])\n",
      "R_provided: tensor([0.7648, 0.8025, 0.8675, 0.8738, 0.9687, 0.8378, 0.7876, 0.8135, 0.8549])\n",
      "F1_provided: tensor([0.7781, 0.7836, 0.8921, 0.8645, 0.9739, 0.8402, 0.7951, 0.7858, 0.8760])\n"
     ]
    }
   ],
   "source": [
    "print(\"P_provided:\", P_provided)\n",
    "print(\"R_provided:\", R_provided)\n",
    "print(\"F1_provided:\", F1_provided)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "253b1eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Queries - Average Precision: 0.9207\n",
      "Test Queries - Average Recall: 0.9315\n",
      "Test Queries - Average F1 Score: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Extract ground truth and generated answers for test queries\n",
    "test_ground_truths = [q[\"answer\"] for q in test_queries]\n",
    "test_generated_answers = [q[\"generated_answer\"] for q in test_queries]\n",
    "\n",
    "# Compute BertScore\n",
    "P_test, R_test, F1_test = score(test_generated_answers, test_ground_truths, lang=\"en\")\n",
    "\n",
    "# Compute average scores\n",
    "avg_precision_test = P_test.mean().item()\n",
    "avg_recall_test = R_test.mean().item()\n",
    "avg_f1_test = F1_test.mean().item()\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Queries - Average Precision: {avg_precision_test:.4f}\")\n",
    "print(f\"Test Queries - Average Recall: {avg_recall_test:.4f}\")\n",
    "print(f\"Test Queries - Average F1 Score: {avg_f1_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "87b07cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_test: tensor([0.9355, 0.8604, 0.9953, 0.8723, 0.7944, 0.8286, 1.0000, 1.0000, 1.0000])\n",
      "R_test: tensor([0.9760, 0.8580, 0.9889, 0.9005, 0.7768, 0.8836, 1.0000, 1.0000, 1.0000])\n",
      "F1_test: tensor([0.9553, 0.8592, 0.9921, 0.8861, 0.7855, 0.8552, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "print(\"P_test:\", P_test)\n",
    "print(\"R_test:\", R_test)\n",
    "print(\"F1_test:\", F1_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crystal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
